"""
æ•°æ®åº“è‡ªåŠ¨è¿ç§»å·¥å…· - ä»æ—§ç‰ˆæœ¬è¿ç§»åˆ° SQLAlchemy ORM ç»“æ„

ä¸»è¦åŠŸèƒ½:
1. è‡ªåŠ¨æ£€æµ‹æ—§è¡¨æ˜¯å¦å­˜åœ¨
2. å¤‡ä»½æ—§æ•°æ®åº“æ–‡ä»¶ (SQLite) æˆ–è¡¨ç»“æ„ (MySQL)
3. åˆ›å»ºæ–°è¡¨ç»“æ„
4. æ™ºèƒ½è¿ç§»å…¼å®¹çš„æ•°æ®
5. éªŒè¯æ•°æ®å®Œæ•´æ€§

æ”¯æŒçš„æ•°æ®åº“:
- SQLite
- MySQL
"""
import asyncio
import time
import os
import shutil
from datetime import datetime
from typing import Dict, List, Any, Optional, Set
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker
from sqlalchemy import text, inspect
from astrbot.api import logger

from ..models.orm import (
    Base,
    UserAffection,
    AffectionInteraction,
    UserConversationHistory,
    UserDiversity,
    Memory,
    MemoryEmbedding,
    MemorySummary,
    CompositePsychologicalState,
    PsychologicalStateComponent,
    PsychologicalStateHistory,
    UserSocialProfile,
    UserSocialRelationComponent,
    SocialRelationHistory,
    PersonaLearningReview,
    StyleLearningReview,
    ExpressionPattern,
)


class DatabaseMigrationTool:
    """
    æ•°æ®åº“è‡ªåŠ¨è¿ç§»å·¥å…·

    ç­–ç•¥:
    - ä»…è¿ç§»æ—§ç‰ˆæœ¬ä¸­å­˜åœ¨ä¸”æ–°ç‰ˆæœ¬ä¹Ÿéœ€è¦çš„è¡¨
    - æ–°ç‰ˆæœ¬æ–°å¢çš„è¡¨ç›´æ¥åˆ›å»º,ä¸å°è¯•è¿ç§»
    - æ—§ç‰ˆæœ¬åºŸå¼ƒçš„è¡¨ä¿ç•™ä¸ºå¤‡ä»½,ä¸åˆ é™¤
    """

    # å®šä¹‰éœ€è¦è¿ç§»çš„è¡¨æ˜ å°„ (æ—§è¡¨å -> æ–°è¡¨å)
    MIGRATION_TABLE_MAP = {
        # åªæœ‰è¿™äº›è¡¨éœ€è¦ä»æ—§ç‰ˆæœ¬è¿ç§»æ•°æ®
        'persona_update_reviews': 'persona_update_reviews',  # äººæ ¼å­¦ä¹ å®¡æ ¸è¡¨
        'style_learning_reviews': 'style_learning_reviews',  # é£æ ¼å­¦ä¹ å®¡æ ¸è¡¨
        'expression_patterns': 'expression_patterns',        # è¡¨è¾¾æ¨¡å¼è¡¨
        'social_relations': 'user_social_relation_components',  # ç¤¾äº¤å…³ç³»è¡¨ï¼ˆé‡æ„ï¼‰
    }

    # æ–°ç‰ˆæœ¬æ–°å¢çš„è¡¨ (ä¸å°è¯•è¿ç§»,ç›´æ¥åˆ›å»º)
    NEW_TABLES = {
        'user_affections',                      # å¥½æ„Ÿåº¦ç³»ç»Ÿé‡æ„
        'affection_interactions',
        'user_conversation_history',
        'user_diversity',
        'memories',                             # è®°å¿†ç³»ç»Ÿ (å…¨æ–°)
        'memory_embeddings',
        'memory_summaries',
        'composite_psychological_states',       # å¿ƒç†çŠ¶æ€ç³»ç»Ÿ (å…¨æ–°)
        'psychological_state_components',
        'psychological_state_history',
        'user_social_profiles',                 # ç¤¾äº¤å…³ç³»ç³»ç»Ÿé‡æ„
        'user_social_relation_components',
        'social_relation_history',
        'social_relation_analysis_results',
        'social_network_nodes',
        'social_network_edges',
        'style_learning_patterns',              # å­¦ä¹ ç³»ç»Ÿæ–°å¢
        'interaction_records',
    }

    def __init__(self, db_url: str, db_type: str = 'sqlite'):
        """
        åˆå§‹åŒ–è¿ç§»å·¥å…·

        Args:
            db_url: æ•°æ®åº“è¿æ¥URL
            db_type: æ•°æ®åº“ç±»å‹ ('sqlite' æˆ– 'mysql')
        """
        self.db_url = db_url
        self.db_type = db_type.lower()

        # åˆ›å»ºå¼‚æ­¥å¼•æ“
        if self.db_type == 'sqlite':
            # è§„èŒƒåŒ– SQLite URL
            if db_url.startswith('sqlite:///'):
                db_path = db_url.replace('sqlite:///', '')
            else:
                db_path = db_url
            self.db_path = db_path
            self.engine = create_async_engine(
                f"sqlite+aiosqlite:///{db_path}",
                echo=False
            )
        elif self.db_type == 'mysql':
            self.db_path = None
            # MySQL URL åº”è¯¥å·²ç»åŒ…å«äº†å®Œæ•´çš„è¿æ¥ä¿¡æ¯
            if not db_url.startswith('mysql+aiomysql://'):
                db_url = db_url.replace('mysql://', 'mysql+aiomysql://')
            self.engine = create_async_engine(db_url, echo=False)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®åº“ç±»å‹: {db_type}")

        self.session_factory = async_sessionmaker(self.engine, class_=AsyncSession)
        logger.info(f"[æ•°æ®è¿ç§»] è¿ç§»å·¥å…·åˆå§‹åŒ–å®Œæˆ (æ•°æ®åº“ç±»å‹: {self.db_type})")

    async def migrate_all(self, backup: bool = True) -> bool:
        """
        æ‰§è¡Œå®Œæ•´çš„æ•°æ®è¿ç§»æµç¨‹

        Args:
            backup: æ˜¯å¦å¤‡ä»½æ—§æ•°æ®åº“ (å¼ºåˆ¶è¦æ±‚,å¦‚æœä¸ºFalseä¼šè‡ªåŠ¨æ”¹ä¸ºTrue)

        Returns:
            bool: è¿ç§»æ˜¯å¦æˆåŠŸ
        """
        logger.info("=" * 70)
        logger.info("ğŸ”„ å¼€å§‹æ•°æ®åº“è¿ç§»æµç¨‹")
        logger.info("=" * 70)

        # å¤‡ä»½æ˜¯å¼ºåˆ¶æ€§çš„,ç¡®ä¿æ•°æ®å®‰å…¨
        if not backup:
            logger.warning("âš ï¸  å¤‡ä»½å‚æ•°ä¸ºFalse,ä½†ä¸ºäº†æ•°æ®å®‰å…¨,å¼ºåˆ¶å¯ç”¨å¤‡ä»½")
            backup = True

        start_time = time.time()

        try:
            # 1. å¤‡ä»½æ—§æ•°æ®åº“ (å¼ºåˆ¶æ‰§è¡Œ)
            logger.info("[æ­¥éª¤ 1/5] å¤‡ä»½æ•°æ®åº“ (å¼ºåˆ¶æ‰§è¡Œ)...")
            backup_path = await self._backup_database()

            if not backup_path:
                logger.error("âŒ æ•°æ®åº“å¤‡ä»½å¤±è´¥,ä¸ºäº†æ•°æ®å®‰å…¨,ä¸­æ­¢è¿ç§»!")
                logger.error("ğŸ’¡ æç¤º: è¯·ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´å’Œæ–‡ä»¶æƒé™")
                return False

            logger.info(f"âœ… æ•°æ®åº“å·²å¤‡ä»½åˆ°: {backup_path}")

            # 2. æ£€æŸ¥æ—§è¡¨æ˜¯å¦å­˜åœ¨
            old_tables = await self._check_old_tables()
            logger.info(f"ğŸ“Š æ£€æµ‹åˆ° {len(old_tables)} ä¸ªç°æœ‰è¡¨")

            # 3. åˆ›å»ºæ–°è¡¨ç»“æ„
            await self._create_new_tables()

            # 4. è¿ç§»å¯å…¼å®¹çš„æ•°æ®
            migration_results = await self._migrate_compatible_data(old_tables)

            # 5. éªŒè¯è¿ç§»ç»“æœ
            await self._verify_migration(migration_results)

            elapsed = time.time() - start_time
            logger.info("=" * 70)
            logger.info(f"âœ… æ•°æ®è¿ç§»å®Œæˆ! è€—æ—¶: {elapsed:.2f} ç§’")
            logger.info("=" * 70)

            return True

        except Exception as e:
            logger.error(f"âŒ æ•°æ®è¿ç§»å¤±è´¥: {e}", exc_info=True)
            logger.error(f"ğŸ’¡ å¦‚æœéœ€è¦æ¢å¤æ•°æ®,è¯·ä½¿ç”¨å¤‡ä»½æ–‡ä»¶: {backup_path if 'backup_path' in locals() else 'æœªåˆ›å»º'}")
            return False

    async def _backup_database(self) -> Optional[str]:
        """
        å¤‡ä»½æ•°æ®åº“

        Returns:
            str: å¤‡ä»½æ–‡ä»¶è·¯å¾„ (SQLite) æˆ–å¤‡ä»½æ ‡è¯† (MySQL)
        """
        logger.info("[æ­¥éª¤ 1/5] å¤‡ä»½æ•°æ®åº“...")

        if self.db_type == 'sqlite':
            return await self._backup_sqlite()
        elif self.db_type == 'mysql':
            return await self._backup_mysql()

        return None

    async def _backup_sqlite(self) -> Optional[str]:
        """å¤‡ä»½ SQLite æ•°æ®åº“æ–‡ä»¶"""
        if not os.path.exists(self.db_path):
            logger.info(f"  â„¹ï¸  æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨,è¿™æ˜¯å…¨æ–°å®‰è£…,æ— éœ€å¤‡ä»½")
            return "NEW_INSTALLATION"  # è¿”å›ç‰¹æ®Šæ ‡è¯†è¡¨ç¤ºå…¨æ–°å®‰è£…

        try:
            # åˆ›å»ºå¤‡ä»½ç›®å½•
            db_dir = os.path.dirname(self.db_path)
            if not db_dir:
                db_dir = "."
            backup_dir = os.path.join(db_dir, "backups")
            os.makedirs(backup_dir, exist_ok=True)

            # ç”Ÿæˆå¤‡ä»½æ–‡ä»¶å
            db_filename = os.path.basename(self.db_path)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_filename = f"{db_filename}.backup_{timestamp}"
            backup_path = os.path.join(backup_dir, backup_filename)

            # å¤åˆ¶æ•°æ®åº“æ–‡ä»¶
            shutil.copy2(self.db_path, backup_path)

            # åŒæ—¶å¤‡ä»½ WAL å’Œ SHM æ–‡ä»¶ (å¦‚æœå­˜åœ¨)
            for ext in ['-wal', '-shm']:
                wal_path = self.db_path + ext
                if os.path.exists(wal_path):
                    shutil.copy2(wal_path, backup_path + ext)
                    logger.info(f"  âœ… å·²å¤‡ä»½: {os.path.basename(wal_path)}")

            # éªŒè¯å¤‡ä»½æ–‡ä»¶
            if not os.path.exists(backup_path):
                raise Exception("å¤‡ä»½æ–‡ä»¶åˆ›å»ºå¤±è´¥")

            backup_size = os.path.getsize(backup_path)
            original_size = os.path.getsize(self.db_path)

            if backup_size != original_size:
                raise Exception(f"å¤‡ä»½æ–‡ä»¶å¤§å°ä¸åŒ¹é… (åŸå§‹: {original_size}, å¤‡ä»½: {backup_size})")

            logger.info(f"  âœ… SQLite æ•°æ®åº“å·²å¤‡ä»½ ({backup_size / 1024:.2f} KB)")
            return backup_path

        except Exception as e:
            logger.error(f"  âŒ SQLite å¤‡ä»½å¤±è´¥: {e}", exc_info=True)
            return None

    async def _backup_mysql(self) -> Optional[str]:
        """å¤‡ä»½ MySQL æ•°æ®åº“ (åˆ›å»ºè¡¨ç»“æ„å¿«ç…§)"""
        try:
            async with self.session_factory() as session:
                # è·å–æ‰€æœ‰è¡¨å
                result = await session.execute(text("SHOW TABLES"))
                tables = [row[0] for row in result.fetchall()]

                if not tables:
                    logger.warning("  âš ï¸ MySQL æ•°æ®åº“ä¸ºç©º,æ— éœ€å¤‡ä»½")
                    return None

                # è®°å½•å¤‡ä»½æ—¶é—´æˆ³
                timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                logger.info(f"  âœ… MySQL æ•°æ®åº“å¿«ç…§å·²è®°å½• (æ—¶é—´: {timestamp}, è¡¨æ•°é‡: {len(tables)})")
                logger.info(f"  ğŸ’¡ æç¤º: MySQLæ•°æ®åº“å»ºè®®ä½¿ç”¨ mysqldump è¿›è¡Œç‰©ç†å¤‡ä»½")

                return f"mysql_snapshot_{timestamp}"

        except Exception as e:
            logger.error(f"  âŒ MySQL å¤‡ä»½å¤±è´¥: {e}")
            return None

    async def _check_old_tables(self) -> Set[str]:
        """
        æ£€æŸ¥æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„è¡¨

        Returns:
            Set[str]: è¡¨åé›†åˆ
        """
        logger.info("[æ­¥éª¤ 2/5] æ£€æŸ¥ç°æœ‰è¡¨...")

        async with self.session_factory() as session:
            if self.db_type == 'sqlite':
                result = await session.execute(
                    text("SELECT name FROM sqlite_master WHERE type='table'")
                )
            elif self.db_type == 'mysql':
                result = await session.execute(text("SHOW TABLES"))
            else:
                return set()

            tables = {row[0] for row in result.fetchall()}

            # è¿‡æ»¤æ‰ç³»ç»Ÿè¡¨
            tables = {t for t in tables if not t.startswith('sqlite_')}

            if tables:
                logger.info(f"  ğŸ“‹ å·²å­˜åœ¨çš„è¡¨: {', '.join(sorted(tables))}")
            else:
                logger.info("  â„¹ï¸ æ•°æ®åº“ä¸ºç©º,è¿™æ˜¯å…¨æ–°å®‰è£…")

            return tables

    async def _create_new_tables(self):
        """åˆ›å»ºæ–°è¡¨ç»“æ„"""
        logger.info("[æ­¥éª¤ 3/5] åˆ›å»ºæ–°è¡¨ç»“æ„...")

        try:
            async with self.engine.begin() as conn:
                # åˆ›å»ºæ‰€æœ‰æ–°è¡¨
                await conn.run_sync(Base.metadata.create_all)

            logger.info("  âœ… æ‰€æœ‰æ–°è¡¨ç»“æ„å·²åˆ›å»º")

        except Exception as e:
            logger.error(f"  âŒ åˆ›å»ºè¡¨ç»“æ„å¤±è´¥: {e}")
            raise

    async def _migrate_compatible_data(self, old_tables: Set[str]) -> Dict[str, int]:
        """
        è¿ç§»å…¼å®¹çš„æ•°æ®

        Args:
            old_tables: æ—§æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„è¡¨

        Returns:
            Dict[str, int]: {è¡¨å: è¿ç§»è®°å½•æ•°}
        """
        logger.info("[æ­¥éª¤ 4/5] è¿ç§»å…¼å®¹æ•°æ®...")

        results = {}

        # åªè¿ç§»åœ¨æ—§è¡¨ä¸­å­˜åœ¨ä¸”åœ¨æ˜ å°„è¡¨ä¸­å®šä¹‰çš„è¡¨
        for old_table, new_table in self.MIGRATION_TABLE_MAP.items():
            if old_table in old_tables:
                count = await self._migrate_table(old_table, new_table)
                results[new_table] = count
            else:
                logger.info(f"  â­ï¸ {old_table} ä¸å­˜åœ¨äºæ—§æ•°æ®åº“,è·³è¿‡è¿ç§»")

        # è¾“å‡ºæ–°å¢è¡¨çš„è¯´æ˜
        new_tables_in_db = self.NEW_TABLES & old_tables
        if new_tables_in_db:
            logger.info(f"  â„¹ï¸ æ£€æµ‹åˆ°éƒ¨åˆ†æ–°è¡¨å·²å­˜åœ¨: {', '.join(new_tables_in_db)}")
            logger.info(f"  ğŸ’¡ è¿™äº›è¡¨å°†ä¿ç•™ç°æœ‰æ•°æ®")

        return results

    async def _migrate_table(self, old_table: str, new_table: str) -> int:
        """
        è¿ç§»å•ä¸ªè¡¨çš„æ•°æ®

        Args:
            old_table: æºè¡¨å
            new_table: ç›®æ ‡è¡¨å

        Returns:
            int: è¿ç§»çš„è®°å½•æ•°
        """
        logger.info(f"  ğŸ”„ è¿ç§»è¡¨: {old_table} -> {new_table}")

        try:
            async with self.session_factory() as session:
                # è¯»å–æ—§è¡¨æ•°æ®
                result = await session.execute(text(f"SELECT * FROM {old_table}"))
                rows = result.fetchall()
                columns = result.keys()

                if not rows:
                    logger.info(f"    - è¡¨ä¸ºç©º,è·³è¿‡")
                    return 0

                # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
                data = [dict(zip(columns, row)) for row in rows]
                logger.info(f"    - æ‰¾åˆ° {len(data)} æ¡è®°å½•")

                # æ£€æŸ¥ç›®æ ‡è¡¨æ˜¯å¦å·²æœ‰æ•°æ®
                count_result = await session.execute(
                    text(f"SELECT COUNT(*) FROM {new_table}")
                )
                existing_count = count_result.scalar()

                if existing_count > 0:
                    logger.warning(f"    âš ï¸ ç›®æ ‡è¡¨å·²æœ‰ {existing_count} æ¡è®°å½•,è·³è¿‡è¿ç§»ä»¥é¿å…é‡å¤")
                    return 0

                # è¿ç§»æ•°æ®
                migrated = await self._insert_migrated_data(session, new_table, data)
                await session.commit()

                logger.info(f"    âœ… æˆåŠŸè¿ç§» {migrated} æ¡è®°å½•")
                return migrated

        except Exception as e:
            logger.error(f"    âŒ è¿ç§»å¤±è´¥: {e}")
            return 0

    async def _insert_migrated_data(
        self,
        session: AsyncSession,
        table_name: str,
        data: List[Dict[str, Any]]
    ) -> int:
        """
        æ’å…¥è¿ç§»çš„æ•°æ® (æ™ºèƒ½å¤„ç†å­—æ®µä¸ä¸€è‡´é—®é¢˜)

        Args:
            session: æ•°æ®åº“ä¼šè¯
            table_name: è¡¨å
            data: æ•°æ®åˆ—è¡¨

        Returns:
            int: æˆåŠŸæ’å…¥çš„è®°å½•æ•°
        """
        if not data:
            return 0

        # ç‰¹æ®Šå¤„ç†ï¼šsocial_relations è¿ç§»åˆ° user_social_relation_components
        if table_name == 'user_social_relation_components':
            return await self._migrate_social_relations(session, data)

        # æ ¹æ®è¡¨åé€‰æ‹©åˆé€‚çš„ORMæ¨¡å‹
        model_map = {
            'persona_update_reviews': PersonaLearningReview,
            'style_learning_reviews': StyleLearningReview,
            'expression_patterns': ExpressionPattern,
            'user_social_relation_components': None,  # ç‰¹æ®Šå¤„ç†
        }

        model_class = model_map.get(table_name)
        if not model_class:
            logger.warning(f"æœªæ‰¾åˆ°è¡¨ {table_name} çš„ORMæ¨¡å‹,ä½¿ç”¨åŸå§‹SQLæ’å…¥")
            return await self._insert_raw_sql(session, table_name, data)

        # è·å–ç›®æ ‡æ¨¡å‹çš„å­—æ®µåˆ—è¡¨
        model_fields = {c.name for c in model_class.__table__.columns}

        # åˆ†æå­—æ®µå·®å¼‚
        source_fields = set(data[0].keys()) if data else set()
        missing_in_source = model_fields - source_fields - {'id'}  # æ’é™¤è‡ªå¢ID
        extra_in_source = source_fields - model_fields

        if missing_in_source:
            logger.info(f"    â„¹ï¸ æ–°ç‰ˆæœ¬æ–°å¢å­—æ®µ: {', '.join(missing_in_source)}")
        if extra_in_source:
            logger.info(f"    â„¹ï¸ æ—§ç‰ˆæœ¬æœ‰ä½†æ–°ç‰ˆæœ¬å·²ç§»é™¤çš„å­—æ®µ: {', '.join(extra_in_source)}")

        # ä½¿ç”¨ORMæ’å…¥ - æ™ºèƒ½å¤„ç†å­—æ®µæ˜ å°„
        count = 0
        for item in data:
            try:
                # è¿‡æ»¤æ‰æ¨¡å‹ä¸­ä¸å­˜åœ¨çš„å­—æ®µ
                filtered_data = {k: v for k, v in item.items() if k in model_fields}

                # ä¸ºç¼ºå¤±çš„å¿…å¡«å­—æ®µæä¾›é»˜è®¤å€¼
                for field_name in missing_in_source:
                    column = model_class.__table__.columns.get(field_name)
                    if column is not None and not column.nullable and column.default is None:
                        # æ ¹æ®å­—æ®µç±»å‹æä¾›åˆç†çš„é»˜è®¤å€¼
                        if 'int' in str(column.type).lower():
                            filtered_data[field_name] = 0
                        elif 'float' in str(column.type).lower() or 'real' in str(column.type).lower():
                            filtered_data[field_name] = 0.0
                        elif 'text' in str(column.type).lower() or 'string' in str(column.type).lower():
                            filtered_data[field_name] = ''
                        elif 'datetime' in str(column.type).lower():
                            filtered_data[field_name] = datetime.now()
                        elif 'bigint' in str(column.type).lower():
                            filtered_data[field_name] = int(time.time())

                # åˆ›å»ºæ¨¡å‹å®ä¾‹
                obj = model_class(**filtered_data)
                session.add(obj)
                count += 1

            except Exception as e:
                logger.warning(f"æ’å…¥è®°å½•å¤±è´¥,è·³è¿‡: {e}")
                continue

        return count

    async def _migrate_social_relations(
        self,
        session: AsyncSession,
        data: List[Dict[str, Any]]
    ) -> int:
        """
        ç‰¹æ®Šå¤„ç†ï¼šä»æ—§ social_relations è¡¨è¿ç§»åˆ°æ–° user_social_relation_components è¡¨

        æ—§è¡¨å­—æ®µ: from_user, to_user, relation_type, strength, frequency, last_interaction
        æ–°è¡¨å­—æ®µ: from_user_id, to_user_id, relation_type, value, frequency, last_interaction, ...

        Args:
            session: æ•°æ®åº“ä¼šè¯
            data: æ—§è¡¨æ•°æ®åˆ—è¡¨

        Returns:
            int: æˆåŠŸæ’å…¥çš„è®°å½•æ•°
        """
        from ..models.orm.social_relation import UserSocialRelationComponent

        count = 0
        for item in data:
            try:
                # è§£ææ—§æ ¼å¼çš„ç”¨æˆ·IDï¼ˆå¯èƒ½æ˜¯ "group_id:user_id" æˆ– "user_id"ï¼‰
                from_user = item.get('from_user', '')
                to_user = item.get('to_user', '')

                # æå– group_id å’Œ user_id
                if ':' in from_user:
                    from_group, from_user_id = from_user.split(':', 1)
                else:
                    # å¦‚æœæ²¡æœ‰group_idï¼Œå°è¯•ä»å…¶ä»–å­—æ®µæ¨æ–­
                    from_group = item.get('group_id', 'unknown')
                    from_user_id = from_user

                if ':' in to_user:
                    to_group, to_user_id = to_user.split(':', 1)
                else:
                    to_group = item.get('group_id', from_group)
                    to_user_id = to_user

                # ç»Ÿä¸€ä½¿ç”¨ from_user çš„ group_id
                group_id = from_group

                # åˆ›å»ºæ–°çš„ç¤¾äº¤å…³ç³»ç»„ä»¶
                component = UserSocialRelationComponent(
                    profile_id=0,  # ä¸´æ—¶å€¼ï¼Œç¨åå¯ä»¥å…³è” profile
                    from_user_id=from_user_id,
                    to_user_id=to_user_id,
                    group_id=group_id,
                    relation_type=item.get('relation_type', 'unknown'),
                    value=float(item.get('strength', 0.0)),  # strength -> value
                    frequency=int(item.get('frequency', 0)),
                    last_interaction=int(item.get('last_interaction', time.time())),
                    description=None,
                    tags=None,
                    created_at=int(time.time())
                )

                session.add(component)
                count += 1

            except Exception as e:
                logger.warning(f"    âš ï¸ è¿ç§»ç¤¾äº¤å…³ç³»è®°å½•å¤±è´¥,è·³è¿‡: {e}, æ•°æ®: {item}")
                continue

        logger.info(f"    â„¹ï¸ ç¤¾äº¤å…³ç³»è¿ç§»: æˆåŠŸè½¬æ¢ {count}/{len(data)} æ¡è®°å½•")
        return count

    async def _insert_raw_sql(
        self,
        session: AsyncSession,
        table_name: str,
        data: List[Dict[str, Any]]
    ) -> int:
        """ä½¿ç”¨åŸå§‹SQLæ’å…¥æ•°æ®"""
        count = 0
        for item in data:
            try:
                columns = ', '.join(item.keys())
                placeholders = ', '.join([f":{k}" for k in item.keys()])
                sql = f"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})"
                await session.execute(text(sql), item)
                count += 1
            except Exception as e:
                logger.warning(f"æ’å…¥è®°å½•å¤±è´¥,è·³è¿‡: {e}")
                continue

        return count

    async def _verify_migration(self, results: Dict[str, int]):
        """éªŒè¯è¿ç§»ç»“æœ"""
        logger.info("[æ­¥éª¤ 5/5] éªŒè¯è¿ç§»ç»“æœ...")

        total_migrated = sum(results.values())

        if results:
            logger.info(f"  ğŸ“Š è¿ç§»ç»Ÿè®¡:")
            for table, count in results.items():
                logger.info(f"    - {table}: {count} æ¡è®°å½•")
            logger.info(f"  âœ… æ€»è®¡è¿ç§»: {total_migrated} æ¡è®°å½•")
        else:
            logger.info(f"  â„¹ï¸ æœªè¿ç§»ä»»ä½•æ•°æ® (å¯èƒ½æ˜¯å…¨æ–°å®‰è£…æˆ–æ•°æ®å·²å­˜åœ¨)")

    async def check_need_migration(self) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰§è¡Œè¿ç§»

        Returns:
            bool: True è¡¨ç¤ºéœ€è¦è¿ç§», False è¡¨ç¤ºä¸éœ€è¦è¿ç§»(å…¨æ–°å®‰è£…æˆ–å·²è¿ç§»)
        """
        try:
            # 1. æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶æ˜¯å¦å­˜åœ¨ (SQLite)
            if self.db_type == 'sqlite':
                if not os.path.exists(self.db_path):
                    logger.info("âœ… æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨,è¿™æ˜¯å…¨æ–°å®‰è£…,æ— éœ€è¿ç§»")
                    return False

            # 2. æ£€æŸ¥æ•°æ®åº“ä¸­çš„è¡¨
            old_tables = await self._check_old_tables()

            # 3. å¦‚æœæ•°æ®åº“å®Œå…¨ä¸ºç©º,ä¸éœ€è¦è¿ç§»
            if not old_tables:
                logger.info("âœ… æ•°æ®åº“ä¸ºç©º,è¿™æ˜¯å…¨æ–°å®‰è£…,æ— éœ€è¿ç§»")
                return False

            # 4. æ£€æŸ¥æ˜¯å¦æœ‰éœ€è¦è¿ç§»çš„æ—§è¡¨
            tables_to_migrate = set(self.MIGRATION_TABLE_MAP.keys()) & old_tables

            if not tables_to_migrate:
                logger.info("âœ… æ²¡æœ‰å‘ç°éœ€è¦è¿ç§»çš„æ—§è¡¨æ•°æ®")
                return False

            # 5. æ£€æŸ¥è¿™äº›è¡¨æ˜¯å¦å·²ç»è¿ç§»è¿‡äº†
            async with self.session_factory() as session:
                for new_table in self.MIGRATION_TABLE_MAP.values():
                    try:
                        result = await session.execute(
                            text(f"SELECT COUNT(*) FROM {new_table}")
                        )
                        count = result.scalar()
                        if count > 0:
                            # å·²æœ‰æ•°æ®,å¯èƒ½å·²ç»è¿ç§»è¿‡äº†
                            logger.info(f"âœ… è¡¨ {new_table} å·²æœ‰æ•°æ®,å¯èƒ½å·²è¿ç§»,è·³è¿‡è¿ç§»")
                            return False
                    except Exception:
                        # è¡¨ä¸å­˜åœ¨,éœ€è¦åˆ›å»ºå’Œè¿ç§»
                        logger.info(f"ğŸ” æ£€æµ‹åˆ°éœ€è¦è¿ç§»çš„æ•°æ®: {', '.join(tables_to_migrate)}")
                        return True

            return True

        except Exception as e:
            logger.error(f"æ£€æŸ¥è¿ç§»éœ€æ±‚æ—¶å‡ºé”™: {e}")
            return False

    async def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        await self.engine.dispose()


# ============================================================
# ä¾¿æ·å‡½æ•°
# ============================================================

async def migrate_database(
    db_url: str,
    db_type: str = 'sqlite',
    backup: bool = True
) -> bool:
    """
    æ‰§è¡Œæ•°æ®åº“è¿ç§»

    Args:
        db_url: æ•°æ®åº“è¿æ¥URL
        db_type: æ•°æ®åº“ç±»å‹ ('sqlite' æˆ– 'mysql')
        backup: æ˜¯å¦å¤‡ä»½

    Returns:
        bool: æ˜¯å¦æˆåŠŸ

    Examples:
        # SQLite
        success = await migrate_database(
            'sqlite:///./data/database.db',
            db_type='sqlite'
        )

        # MySQL
        success = await migrate_database(
            'mysql://user:pass@localhost/dbname',
            db_type='mysql'
        )
    """
    migrator = DatabaseMigrationTool(db_url, db_type)

    try:
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿ç§»
        if not await migrator.check_need_migration():
            logger.info("âœ… æ•°æ®åº“å·²æ˜¯æœ€æ–°ç‰ˆæœ¬,æ— éœ€è¿ç§»")
            return True

        # æ‰§è¡Œè¿ç§»
        success = await migrator.migrate_all(backup=backup)
        return success

    finally:
        await migrator.close()


async def check_and_migrate_if_needed(
    db_url: str,
    db_type: str = 'sqlite',
    backup: bool = True
) -> bool:
    """
    æ£€æŸ¥å¹¶åœ¨éœ€è¦æ—¶è‡ªåŠ¨æ‰§è¡Œè¿ç§»

    è¿™æ˜¯æ¨èçš„å¯åŠ¨æ—¶è°ƒç”¨å‡½æ•°

    Args:
        db_url: æ•°æ®åº“è¿æ¥URL
        db_type: æ•°æ®åº“ç±»å‹
        backup: æ˜¯å¦å¤‡ä»½

    Returns:
        bool: æ˜¯å¦æˆåŠŸ (å¦‚æœä¸éœ€è¦è¿ç§»ä¹Ÿè¿”å›True)
    """
    migrator = DatabaseMigrationTool(db_url, db_type)

    try:
        if await migrator.check_need_migration():
            logger.info("ğŸ” æ£€æµ‹åˆ°éœ€è¦æ•°æ®åº“è¿ç§»,å¼€å§‹æ‰§è¡Œ...")
            return await migrator.migrate_all(backup=backup)
        else:
            logger.info("âœ… æ•°æ®åº“ç»“æ„å·²æ˜¯æœ€æ–°,æ— éœ€è¿ç§»")
            return True

    except Exception as e:
        logger.error(f"æ•°æ®åº“è¿ç§»æ£€æŸ¥å¤±è´¥: {e}", exc_info=True)
        return False

    finally:
        await migrator.close()


if __name__ == "__main__":
    # æµ‹è¯•è¿ç§»
    import sys

    if len(sys.argv) < 2:
        print("ç”¨æ³•: python migration_tool.py <database_url> [db_type]")
        print("ç¤ºä¾‹: python migration_tool.py sqlite:///./data/database.db sqlite")
        print("ç¤ºä¾‹: python migration_tool.py mysql://user:pass@localhost/db mysql")
        sys.exit(1)

    db_url = sys.argv[1]
    db_type = sys.argv[2] if len(sys.argv) > 2 else 'sqlite'

    asyncio.run(check_and_migrate_if_needed(db_url, db_type))