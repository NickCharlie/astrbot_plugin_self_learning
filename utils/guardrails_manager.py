"""
Guardrails AI ç®¡ç†å™¨
ç”¨äºç®¡ç† LLM çš„ç»“æ„åŒ–è¾“å‡º,ç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®ä¸”ç¬¦åˆçº¦æŸ
"""
from typing import Dict, List, Optional, Any, Type
from pydantic import BaseModel, Field, field_validator
from guardrails import Guard
from astrbot.api import logger


# ============================================================
# Pydantic æ¨¡å‹å®šä¹‰ - ç”¨äºå¿ƒç†çŠ¶æ€åˆ†æ
# ============================================================

class PsychologicalStateTransition(BaseModel):
    """
    å¿ƒç†çŠ¶æ€è½¬æ¢ç»“æœæ¨¡å‹
    """
    new_state: str = Field(
        description="æ–°çš„å¿ƒç†çŠ¶æ€åç§°(ä¸­æ–‡),ä¾‹å¦‚: æ„‰æ‚¦ã€ç–²æƒ«ã€ä¸“æ³¨ç­‰"
    )
    confidence: Optional[float] = Field(
        default=0.8,
        ge=0.0,
        le=1.0,
        description="ç½®ä¿¡åº¦(0-1)",
    )
    reason: Optional[str] = Field(
        default="",
        description="çŠ¶æ€è½¬æ¢çš„åŸå› è¯´æ˜"
    )

    @field_validator('new_state')
    @classmethod
    def validate_state_name(cls, v: str) -> str:
        """éªŒè¯çŠ¶æ€åç§°"""
        if not v or len(v) > 20:
            raise ValueError("çŠ¶æ€åç§°å¿…é¡»æ˜¯1-20ä¸ªå­—ç¬¦")
        return v.strip()


# ============================================================
# Pydantic æ¨¡å‹å®šä¹‰ - ç”¨äºå¯¹è¯ç›®æ ‡åˆ†æ
# ============================================================

class GoalAnalysisResult(BaseModel):
    """
    å¯¹è¯ç›®æ ‡åˆ†æç»“æœæ¨¡å‹
    """
    goal_type: str = Field(
        description="å¯¹è¯ç›®æ ‡ç±»å‹,ä¾‹å¦‚: emotional_support, casual_chatç­‰"
    )
    topic: str = Field(
        description="å¯¹è¯è¯é¢˜,ç®€çŸ­æè¿°(1-20å­—)"
    )
    confidence: float = Field(
        default=0.7,
        ge=0.0,
        le=1.0,
        description="ç½®ä¿¡åº¦(0-1)"
    )
    reasoning: Optional[str] = Field(
        default="",
        description="åˆ†æç†ç”±"
    )

    @field_validator('goal_type')
    @classmethod
    def validate_goal_type(cls, v: str) -> str:
        """éªŒè¯ç›®æ ‡ç±»å‹"""
        if not v or len(v) > 50:
            raise ValueError("ç›®æ ‡ç±»å‹å¿…é¡»æ˜¯1-50ä¸ªå­—ç¬¦")
        return v.strip()

    @field_validator('topic')
    @classmethod
    def validate_topic(cls, v: str) -> str:
        """éªŒè¯è¯é¢˜"""
        if not v or len(v) > 100:
            raise ValueError("è¯é¢˜å¿…é¡»æ˜¯1-100ä¸ªå­—ç¬¦")
        return v.strip()


class ConversationIntentAnalysis(BaseModel):
    """
    å¯¹è¯æ„å›¾åˆ†æç»“æœæ¨¡å‹
    """
    goal_switch_needed: bool = Field(
        default=False,
        description="æ˜¯å¦éœ€è¦åˆ‡æ¢ç›®æ ‡ç±»å‹"
    )
    new_goal_type: Optional[str] = Field(
        default=None,
        description="æ–°çš„ç›®æ ‡ç±»å‹(å¦‚æœéœ€è¦åˆ‡æ¢)"
    )
    new_topic: Optional[str] = Field(
        default=None,
        description="æ–°çš„è¯é¢˜(å¦‚æœéœ€è¦åˆ‡æ¢)"
    )
    topic_completed: bool = Field(
        default=False,
        description="å½“å‰è¯é¢˜æ˜¯å¦å·²å®Œæˆ"
    )
    stage_completed: bool = Field(
        default=False,
        description="å½“å‰é˜¶æ®µæ˜¯å¦å·²å®Œæˆ"
    )
    stage_adjustment_needed: bool = Field(
        default=False,
        description="æ˜¯å¦éœ€è¦è°ƒæ•´å½“å‰é˜¶æ®µ"
    )
    suggested_stage: Optional[str] = Field(
        default=None,
        description="å»ºè®®çš„ä¸‹ä¸€é˜¶æ®µä»»åŠ¡"
    )
    completion_signals: int = Field(
        default=0,
        ge=0,
        description="æ£€æµ‹åˆ°çš„å®Œæˆä¿¡å·æ•°é‡"
    )
    user_engagement: float = Field(
        default=0.5,
        ge=0.0,
        le=1.0,
        description="ç”¨æˆ·å‚ä¸åº¦(0-1)"
    )
    reasoning: Optional[str] = Field(
        default="",
        description="åˆ†æç†ç”±"
    )


# ============================================================
# Pydantic æ¨¡å‹å®šä¹‰ - ç”¨äºç¤¾äº¤å…³ç³»åˆ†æ
# ============================================================

class RelationChange(BaseModel):
    """
    å•ä¸ªå…³ç³»ç±»å‹çš„å˜åŒ–
    """
    relation_type: str = Field(
        description="å…³ç³»ç±»å‹åç§°,ä¾‹å¦‚: æŒšå‹ã€åŒäº‹ã€é™Œç”Ÿå…³ç³»ç­‰"
    )
    value_delta: float = Field(
        ge=-1.0,
        le=1.0,
        description="å…³ç³»å¼ºåº¦å˜åŒ–é‡,èŒƒå›´[-1.0, 1.0]"
    )
    reason: Optional[str] = Field(
        default="",
        description="å˜åŒ–åŸå› "
    )

    @field_validator('relation_type')
    @classmethod
    def validate_relation_type(cls, v: str) -> str:
        """éªŒè¯å…³ç³»ç±»å‹åç§°"""
        if not v or len(v) > 30:
            raise ValueError("å…³ç³»ç±»å‹åç§°å¿…é¡»æ˜¯1-30ä¸ªå­—ç¬¦")
        return v.strip()


class SocialRelationAnalysis(BaseModel):
    """
    ç¤¾äº¤å…³ç³»åˆ†æç»“æœæ¨¡å‹
    """
    relations: List[RelationChange] = Field(
        description="å—å½±å“çš„å…³ç³»ç±»å‹åŠå˜åŒ–é‡åˆ—è¡¨",
        min_length=0,
        max_length=5
    )
    overall_sentiment: Optional[str] = Field(
        default="neutral",
        description="æ•´ä½“æƒ…æ„Ÿå€¾å‘: positive/neutral/negative"
    )

    @field_validator('relations')
    @classmethod
    def validate_relations_count(cls, v: List[RelationChange]) -> List[RelationChange]:
        """é™åˆ¶å…³ç³»æ•°é‡"""
        if len(v) > 5:
            logger.warning(f"å…³ç³»æ•°é‡è¿‡å¤š({len(v)}),æˆªå–å‰5ä¸ª")
            return v[:5]
        return v


# ============================================================
# Guardrails ç®¡ç†å™¨
# ============================================================

class GuardrailsManager:
    """
    Guardrails AI ç®¡ç†å™¨

    åŠŸèƒ½:
    1. ç®¡ç†ä¸åŒæ•°æ®æ¨¡å‹çš„ Guard å®ä¾‹
    2. æä¾›é«˜æ€§èƒ½çš„ LLM è°ƒç”¨æ¥å£
    3. è‡ªåŠ¨éªŒè¯å’Œä¿®å¤ LLM è¾“å‡º
    4. æ”¯æŒé‡è¯•å’Œé”™è¯¯å¤„ç†
    """

    def __init__(self, max_reasks: int = 1):
        """
        åˆå§‹åŒ– Guardrails ç®¡ç†å™¨

        Args:
            max_reasks: æœ€å¤§é‡è¯•æ¬¡æ•°(é»˜è®¤1æ¬¡,ä¿æŒé«˜æ€§èƒ½)
        """
        self.max_reasks = max_reasks

        # åˆ›å»ºä¸åŒç”¨é€”çš„ Guard å®ä¾‹
        self._state_guard: Optional[Guard] = None
        self._relation_guard: Optional[Guard] = None
        self._goal_analysis_guard: Optional[Guard] = None
        self._intent_analysis_guard: Optional[Guard] = None

        logger.info(f"[Guardrails] ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ (max_reasks={max_reasks})")

    def get_state_transition_guard(self) -> Guard:
        """
        è·å–å¿ƒç†çŠ¶æ€è½¬æ¢çš„ Guard å®ä¾‹

        Returns:
            Guard å®ä¾‹
        """
        if self._state_guard is None:
            self._state_guard = Guard.for_pydantic(
                output_class=PsychologicalStateTransition,
                # ä¸ä½¿ç”¨é¢å¤–çš„éªŒè¯å™¨,ä¿æŒé«˜æ€§èƒ½
            )
            logger.debug("[Guardrails] å¿ƒç†çŠ¶æ€è½¬æ¢ Guard å·²åˆ›å»º")

        return self._state_guard

    def get_relation_analysis_guard(self) -> Guard:
        """
        è·å–ç¤¾äº¤å…³ç³»åˆ†æçš„ Guard å®ä¾‹

        Returns:
            Guard å®ä¾‹
        """
        if self._relation_guard is None:
            self._relation_guard = Guard.for_pydantic(
                output_class=SocialRelationAnalysis,
            )
            logger.debug("[Guardrails] ç¤¾äº¤å…³ç³»åˆ†æ Guard å·²åˆ›å»º")

        return self._relation_guard

    async def parse_state_transition(
        self,
        llm_callable,
        prompt: str,
        model: str = "gpt-4o",
        **kwargs
    ) -> Optional[PsychologicalStateTransition]:
        """
        è§£æå¿ƒç†çŠ¶æ€è½¬æ¢ç»“æœ

        Args:
            llm_callable: LLM è°ƒç”¨å‡½æ•°(åº”è¯¥è¿”å›æ–‡æœ¬)
            prompt: æç¤ºè¯
            model: æ¨¡å‹åç§°
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            PsychologicalStateTransition å¯¹è±¡,å¤±è´¥è¿”å› None
        """
        try:
            guard = self.get_state_transition_guard()

            # ä½¿ç”¨ JSON æ¨¡å¼è·å–ç»“æ„åŒ–è¾“å‡º
            # ä¸ºæç¤ºè¯æ·»åŠ  JSON è¾“å‡ºè¦æ±‚
            enhanced_prompt = f"""{prompt}

è¯·ä»¥ JSON æ ¼å¼è¿”å›ç»“æœ,æ ¼å¼å¦‚ä¸‹:
{{
    "new_state": "æ–°çŠ¶æ€åç§°",
    "confidence": 0.8,
    "reason": "è½¬æ¢åŸå› "
}}
"""

            # è°ƒç”¨ LLM(é€šè¿‡ç”¨æˆ·æä¾›çš„ callable)
            response_text = await llm_callable(enhanced_prompt, model=model, **kwargs)

            # ä½¿ç”¨ Guard éªŒè¯
            result = guard.parse(response_text)

            if result.validation_passed:
                logger.debug(f"âœ… [Guardrails] å¿ƒç†çŠ¶æ€è§£ææˆåŠŸ: {result.validated_output.new_state}")
                return result.validated_output
            else:
                logger.warning(f"âš ï¸ [Guardrails] å¿ƒç†çŠ¶æ€éªŒè¯å¤±è´¥: {result.validation_summaries}")
                return None

        except Exception as e:
            logger.error(f"âŒ [Guardrails] å¿ƒç†çŠ¶æ€è§£æå¤±è´¥: {e}", exc_info=True)
            return None

    async def parse_relation_analysis(
        self,
        llm_callable,
        prompt: str,
        model: str = "gpt-4o",
        **kwargs
    ) -> Optional[SocialRelationAnalysis]:
        """
        è§£æç¤¾äº¤å…³ç³»åˆ†æç»“æœ

        Args:
            llm_callable: LLM è°ƒç”¨å‡½æ•°
            prompt: æç¤ºè¯
            model: æ¨¡å‹åç§°
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            SocialRelationAnalysis å¯¹è±¡,å¤±è´¥è¿”å› None
        """
        try:
            guard = self.get_relation_analysis_guard()

            # å¢å¼ºæç¤ºè¯
            enhanced_prompt = f"""{prompt}

è¯·ä»¥ JSON æ ¼å¼è¿”å›ç»“æœ,æ ¼å¼å¦‚ä¸‹:
{{
    "relations": [
        {{"relation_type": "å…³ç³»ç±»å‹1", "value_delta": 0.05, "reason": "åŸå› "}},
        {{"relation_type": "å…³ç³»ç±»å‹2", "value_delta": 0.03, "reason": "åŸå› "}}
    ],
    "overall_sentiment": "positive"
}}

æ³¨æ„:
- relations æœ€å¤šè¿”å›5ä¸ª
- value_delta èŒƒå›´ [-1.0, 1.0]
- overall_sentiment å¯é€‰å€¼: positive/neutral/negative
"""

            # è°ƒç”¨ LLM
            response_text = await llm_callable(enhanced_prompt, model=model, **kwargs)

            # ä½¿ç”¨ Guard éªŒè¯
            result = guard.parse(response_text)

            if result.validation_passed:
                relation_count = len(result.validated_output.relations)
                logger.debug(f"âœ… [Guardrails] ç¤¾äº¤å…³ç³»è§£ææˆåŠŸ: {relation_count}ä¸ªå…³ç³»")
                return result.validated_output
            else:
                logger.warning(f"âš ï¸ [Guardrails] ç¤¾äº¤å…³ç³»éªŒè¯å¤±è´¥: {result.validation_summaries}")
                return None

        except Exception as e:
            logger.error(f"âŒ [Guardrails] ç¤¾äº¤å…³ç³»è§£æå¤±è´¥: {e}", exc_info=True)
            return None

    def get_goal_analysis_guard(self) -> Guard:
        """
        è·å–å¯¹è¯ç›®æ ‡åˆ†æçš„ Guard å®ä¾‹

        Returns:
            Guard å®ä¾‹
        """
        if self._goal_analysis_guard is None:
            self._goal_analysis_guard = Guard.for_pydantic(
                output_class=GoalAnalysisResult,
            )
            logger.debug("[Guardrails] å¯¹è¯ç›®æ ‡åˆ†æ Guard å·²åˆ›å»º")

        return self._goal_analysis_guard

    def get_intent_analysis_guard(self) -> Guard:
        """
        è·å–å¯¹è¯æ„å›¾åˆ†æçš„ Guard å®ä¾‹

        Returns:
            Guard å®ä¾‹
        """
        if self._intent_analysis_guard is None:
            self._intent_analysis_guard = Guard.for_pydantic(
                output_class=ConversationIntentAnalysis,
            )
            logger.debug("[Guardrails] å¯¹è¯æ„å›¾åˆ†æ Guard å·²åˆ›å»º")

        return self._intent_analysis_guard

    async def parse_goal_analysis(
        self,
        llm_callable,
        prompt: str,
        model: str = "gpt-4o",
        **kwargs
    ) -> Optional[GoalAnalysisResult]:
        """
        è§£æå¯¹è¯ç›®æ ‡åˆ†æç»“æœ

        Args:
            llm_callable: LLM è°ƒç”¨å‡½æ•°
            prompt: æç¤ºè¯
            model: æ¨¡å‹åç§°
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            GoalAnalysisResult å¯¹è±¡,å¤±è´¥è¿”å› None
        """
        try:
            guard = self.get_goal_analysis_guard()

            # å¢å¼ºæç¤ºè¯
            enhanced_prompt = f"""{prompt}

è¯·ä»¥ JSON æ ¼å¼è¿”å›ç»“æœ,æ ¼å¼å¦‚ä¸‹:
{{
    "goal_type": "emotional_support",
    "topic": "å·¥ä½œå‹åŠ›",
    "confidence": 0.85,
    "reasoning": "ç®€çŸ­ç†ç”±"
}}

æ³¨æ„:
- goal_type å¿…é¡»æ˜¯è‹±æ–‡è›‡å½¢å‘½å(å¦‚ emotional_support, casual_chat)
- topic ç®€çŸ­æè¿°(1-20å­—)
- confidence èŒƒå›´ [0.0, 1.0]
"""

            # è°ƒç”¨ LLM
            response_text = await llm_callable(enhanced_prompt, model=model, **kwargs)

            # ä½¿ç”¨ Guard éªŒè¯
            result = guard.parse(response_text)

            if result.validation_passed:
                logger.debug(f"âœ… [Guardrails] å¯¹è¯ç›®æ ‡è§£ææˆåŠŸ: {result.validated_output.goal_type}")
                return result.validated_output
            else:
                logger.warning(f"âš ï¸ [Guardrails] å¯¹è¯ç›®æ ‡éªŒè¯å¤±è´¥: {result.validation_summaries}")
                return None

        except Exception as e:
            logger.error(f"âŒ [Guardrails] å¯¹è¯ç›®æ ‡è§£æå¤±è´¥: {e}", exc_info=True)
            return None

    async def parse_intent_analysis(
        self,
        llm_callable,
        prompt: str,
        model: str = "gpt-4o",
        **kwargs
    ) -> Optional[ConversationIntentAnalysis]:
        """
        è§£æå¯¹è¯æ„å›¾åˆ†æç»“æœ

        Args:
            llm_callable: LLM è°ƒç”¨å‡½æ•°
            prompt: æç¤ºè¯
            model: æ¨¡å‹åç§°
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            ConversationIntentAnalysis å¯¹è±¡,å¤±è´¥è¿”å› None
        """
        try:
            guard = self.get_intent_analysis_guard()

            # å¢å¼ºæç¤ºè¯
            enhanced_prompt = f"""{prompt}

è¯·ä»¥ JSON æ ¼å¼è¿”å›ç»“æœ,æ ¼å¼å¦‚ä¸‹:
{{
    "goal_switch_needed": false,
    "new_goal_type": null,
    "new_topic": null,
    "topic_completed": false,
    "stage_completed": true,
    "stage_adjustment_needed": false,
    "suggested_stage": "ä¸‹ä¸€é˜¶æ®µä»»åŠ¡",
    "completion_signals": 1,
    "user_engagement": 0.8,
    "reasoning": "ç®€çŸ­ç†ç”±(20å­—å†…)"
}}

æ³¨æ„:
- goal_switch_needed/topic_completed/stage_completed/stage_adjustment_needed ä¸º boolean ç±»å‹
- new_goal_type/new_topic/suggested_stage ä¸ºå­—ç¬¦ä¸²æˆ– null
- completion_signals ä¸ºéè´Ÿæ•´æ•°
- user_engagement èŒƒå›´ [0.0, 1.0]
"""

            # è°ƒç”¨ LLM
            response_text = await llm_callable(enhanced_prompt, model=model, **kwargs)

            # ä½¿ç”¨ Guard éªŒè¯
            result = guard.parse(response_text)

            if result.validation_passed:
                logger.debug(f"âœ… [Guardrails] å¯¹è¯æ„å›¾è§£ææˆåŠŸ")
                return result.validated_output
            else:
                logger.warning(f"âš ï¸ [Guardrails] å¯¹è¯æ„å›¾éªŒè¯å¤±è´¥: {result.validation_summaries}")
                return None

        except Exception as e:
            logger.error(f"âŒ [Guardrails] å¯¹è¯æ„å›¾è§£æå¤±è´¥: {e}", exc_info=True)
            return None

    def parse_json_direct(
        self,
        response_text: str,
        model_class: Type[BaseModel]
    ) -> Optional[BaseModel]:
        """
        ç›´æ¥è§£æ JSON æ–‡æœ¬(ä¸è°ƒç”¨ LLM)

        Args:
            response_text: JSON æ–‡æœ¬
            model_class: Pydantic æ¨¡å‹ç±»

        Returns:
            æ¨¡å‹å®ä¾‹,å¤±è´¥è¿”å› None
        """
        try:
            guard = Guard.for_pydantic(output_class=model_class)
            result = guard.parse(response_text)

            if result.validation_passed:
                return result.validated_output
            else:
                logger.warning(f"âš ï¸ [Guardrails] JSON éªŒè¯å¤±è´¥: {result.validation_summaries}")
                return None

        except Exception as e:
            logger.error(f"âŒ [Guardrails] JSON è§£æå¤±è´¥: {e}")
            return None

    def validate_and_clean_json(
        self,
        response_text: str,
        expected_type: str = "auto"
    ) -> Optional[Any]:
        """
        é€šç”¨ JSON éªŒè¯å’Œæ¸…æ´— - é€‚ç”¨äºæ‰€æœ‰ LLM è¿”å›

        Args:
            response_text: LLM è¿”å›çš„æ–‡æœ¬ï¼ˆå¯èƒ½åŒ…å« Markdownã€ä»£ç å—ç­‰ï¼‰
            expected_type: æœŸæœ›çš„ç±»å‹ ("object", "array", "auto")

        Returns:
            æ¸…æ´—åçš„ JSON å¯¹è±¡/æ•°ç»„ï¼Œå¤±è´¥è¿”å› None
        """
        import json
        import re

        try:
            # æ£€æŸ¥è¾“å…¥æ˜¯å¦ä¸ºç©º
            if not response_text:
                logger.error(f"âŒ [Guardrails] è¾“å…¥ä¸ºç©ºï¼Œæ— æ³•è§£æ JSON")
                return None

            # 1. ç§»é™¤ Markdown ä»£ç å—æ ‡è®°
            cleaned_text = response_text.strip()

            # è®°å½•åŸå§‹å“åº”é•¿åº¦ç”¨äºè°ƒè¯•
            logger.debug(f"ğŸ” [Guardrails] åŸå§‹å“åº”é•¿åº¦: {len(response_text)}, æ¸…ç†åé•¿åº¦: {len(cleaned_text)}")

            # ç§»é™¤ ```json å’Œ ``` æ ‡è®°
            if cleaned_text.startswith("```json"):
                cleaned_text = cleaned_text[7:]
            elif cleaned_text.startswith("```"):
                cleaned_text = cleaned_text[3:]

            if cleaned_text.endswith("```"):
                cleaned_text = cleaned_text[:-3]

            cleaned_text = cleaned_text.strip()

            # 2. å°è¯•æå– JSON éƒ¨åˆ†ï¼ˆå¤„ç† LLM å¯èƒ½åœ¨ JSON å‰ååŠ è¯´æ˜çš„æƒ…å†µï¼‰
            # åŒ¹é…æœ€å¤–å±‚çš„ { } æˆ– [ ]
            json_pattern = r'(\{(?:[^{}]|(?:\{(?:[^{}]|(?:\{[^{}]*\}))*\}))*\})'
            array_pattern = r'(\[(?:[^\[\]]|(?:\[(?:[^\[\]]|(?:\[[^\[\]]*\]))*\]))*\])'

            json_match = re.search(json_pattern, cleaned_text, re.DOTALL)
            array_match = re.search(array_pattern, cleaned_text, re.DOTALL)

            if expected_type == "object" or (expected_type == "auto" and json_match):
                if json_match:
                    cleaned_text = json_match.group(1)
            elif expected_type == "array" or (expected_type == "auto" and array_match):
                if array_match:
                    cleaned_text = array_match.group(1)

            # 3. å°è¯•è§£æ JSON
            parsed = json.loads(cleaned_text)

            logger.debug(f"âœ… [Guardrails] JSON éªŒè¯æˆåŠŸï¼Œç±»å‹: {type(parsed).__name__}")
            return parsed

        except json.JSONDecodeError as e:
            # æ˜¾ç¤ºå“åº”é¢„è§ˆç”¨äºè°ƒè¯•
            preview = cleaned_text[:200] if len(cleaned_text) > 200 else cleaned_text
            logger.warning(f"âš ï¸ [Guardrails] JSON è§£æå¤±è´¥: {e}ï¼Œå°è¯•ä¿®å¤...")
            logger.debug(f"ğŸ” [Guardrails] å“åº”é¢„è§ˆ: {preview}")

            # å°è¯•ä¿®å¤å¸¸è§çš„ JSON é”™è¯¯
            try:
                # æ›¿æ¢å•å¼•å·ä¸ºåŒå¼•å·ï¼ˆPython dict é£æ ¼ï¼‰
                fixed_text = cleaned_text.replace("'", '"')

                # ç§»é™¤å°¾éšé€—å·
                fixed_text = re.sub(r',\s*}', '}', fixed_text)
                fixed_text = re.sub(r',\s*]', ']', fixed_text)

                parsed = json.loads(fixed_text)
                logger.info(f"âœ… [Guardrails] JSON ä¿®å¤æˆåŠŸ")
                return parsed

            except Exception as fix_error:
                logger.error(f"âŒ [Guardrails] JSON ä¿®å¤å¤±è´¥: {fix_error}")
                return None

        except Exception as e:
            logger.error(f"âŒ [Guardrails] JSON éªŒè¯å¼‚å¸¸: {e}")
            return None

    async def validate_llm_response(
        self,
        llm_callable,
        prompt: str,
        expected_format: str = "json",
        model: str = "gpt-4o",
        **kwargs
    ) -> Optional[Any]:
        """
        é€šç”¨ LLM å“åº”éªŒè¯å™¨ - åŒ…è£…æ‰€æœ‰ LLM è°ƒç”¨

        Args:
            llm_callable: LLM è°ƒç”¨å‡½æ•°
            prompt: æç¤ºè¯
            expected_format: æœŸæœ›çš„æ ¼å¼ ("json", "text", "list", "object")
            model: æ¨¡å‹åç§°
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            éªŒè¯åçš„å“åº”å†…å®¹ï¼Œå¤±è´¥è¿”å› None
        """
        try:
            # å¢å¼ºæç¤ºè¯ - æ˜ç¡®è¦æ±‚è¾“å‡ºæ ¼å¼
            if expected_format == "json":
                enhanced_prompt = f"""{prompt}

è¯·ä»¥ JSON æ ¼å¼è¿”å›ç»“æœï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–è¯´æ˜ã€‚"""
            elif expected_format in ["list", "array"]:
                enhanced_prompt = f"""{prompt}

è¯·ä»¥ JSON æ•°ç»„æ ¼å¼è¿”å›ç»“æœï¼Œä¾‹å¦‚: ["item1", "item2"]"""
            elif expected_format == "object":
                enhanced_prompt = f"""{prompt}

è¯·ä»¥ JSON å¯¹è±¡æ ¼å¼è¿”å›ç»“æœï¼Œä¾‹å¦‚: {{"key": "value"}}"""
            else:
                enhanced_prompt = prompt

            # è°ƒç”¨ LLM
            response_text = await llm_callable(enhanced_prompt, model=model, **kwargs)

            if not response_text:
                logger.warning("âš ï¸ [Guardrails] LLM è¿”å›ä¸ºç©º")
                return None

            # æ ¹æ®æœŸæœ›æ ¼å¼éªŒè¯
            if expected_format in ["json", "list", "array", "object"]:
                result = self.validate_and_clean_json(
                    response_text,
                    expected_type="array" if expected_format in ["list", "array"] else "object"
                )
                return result
            else:
                # çº¯æ–‡æœ¬ï¼Œç›´æ¥è¿”å›
                return response_text.strip()

        except Exception as e:
            logger.error(f"âŒ [Guardrails] LLM å“åº”éªŒè¯å¤±è´¥: {e}", exc_info=True)
            return None


# ============================================================
# å…¨å±€å•ä¾‹
# ============================================================

# ä½¿ç”¨ max_reasks=1 ä¿æŒé«˜æ€§èƒ½
_guardrails_manager: Optional[GuardrailsManager] = None


def get_guardrails_manager(max_reasks: int = 1) -> GuardrailsManager:
    """
    è·å–å…¨å±€ Guardrails ç®¡ç†å™¨å•ä¾‹

    Args:
        max_reasks: æœ€å¤§é‡è¯•æ¬¡æ•°

    Returns:
        GuardrailsManager å®ä¾‹
    """
    global _guardrails_manager

    if _guardrails_manager is None:
        _guardrails_manager = GuardrailsManager(max_reasks=max_reasks)

    return _guardrails_manager
