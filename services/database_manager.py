"""
æ•°æ®åº“ç®¡ç†å™¨ - ç®¡ç†åˆ†ç¾¤æ•°æ®åº“å’Œæ•°æ®æŒä¹…åŒ– å³å°†å¼ƒç”¨
"""
import os
import json
import aiosqlite
import time
import asyncio
from typing import Dict, List, Optional, Any, Callable
from datetime import datetime

from astrbot.api import logger

from ..config import PluginConfig
from ..constants import UPDATE_TYPE_EXPRESSION_LEARNING
from ..exceptions import DataStorageError

from ..core.patterns import AsyncServiceBase

# å¯¼å…¥æ•°æ®åº“åç«¯
from ..core.database import (
    DatabaseFactory,
    DatabaseConfig,
    DatabaseType,
    IDatabaseBackend
)

# âœ¨ å¯¼å…¥ORMæ”¯æŒ
from ..core.database.engine import DatabaseEngine
from ..repositories.reinforcement_repository import (
    ReinforcementLearningRepository,
    PersonaFusionRepository,
    StrategyOptimizationRepository
)
from ..repositories.learning_repository import (
    LearningBatchRepository,
    LearningSessionRepository,
    StyleLearningReviewRepository,
    PersonaLearningReviewRepository
)
from ..repositories.message_repository import (
    ConversationContextRepository,
    ConversationTopicClusteringRepository,
    ConversationQualityMetricsRepository,
    ContextSimilarityCacheRepository
)
from ..repositories.jargon_repository import (
    JargonRepository
)


class DatabaseConnectionPool:
    """æ•°æ®åº“è¿æ¥æ± """
    
    def __init__(self, db_path: str, max_connections: int = 10, min_connections: int = 2):
        self.db_path = db_path
        self.max_connections = max_connections
        self.min_connections = min_connections
        self.pool: asyncio.Queue = asyncio.Queue(maxsize=max_connections)
        self.active_connections = 0
        self.total_connections = 0
        self._lock = asyncio.Lock()
        self._logger = logger

    async def initialize(self):
        """åˆå§‹åŒ–è¿æ¥æ± """
        async with self._lock:
            # åˆ›å»ºæœ€å°æ•°é‡çš„è¿æ¥
            for _ in range(self.min_connections):
                conn = await self._create_connection()
                await self.pool.put(conn)

    async def _create_connection(self) -> aiosqlite.Connection:
        """åˆ›å»ºæ–°çš„æ•°æ®åº“è¿æ¥"""
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        db_dir = os.path.dirname(self.db_path)
        os.makedirs(db_dir, exist_ok=True)
        
        # æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶æƒé™
        if os.path.exists(self.db_path):
            try:
                import stat
                os.chmod(self.db_path, stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IWGRP)
            except OSError as e:
                self._logger.warning(f"æ— æ³•ä¿®æ”¹æ•°æ®åº“æ–‡ä»¶æƒé™: {e}")
        
        conn = await aiosqlite.connect(self.db_path)
        
        # è®¾ç½®è¿æ¥å‚æ•°
        await conn.execute('PRAGMA foreign_keys = ON')
        await conn.execute('PRAGMA journal_mode = WAL')
        await conn.execute('PRAGMA synchronous = NORMAL')
        await conn.execute('PRAGMA cache_size = 10000')
        await conn.execute('PRAGMA temp_store = memory')
        await conn.commit()
        
        self.total_connections += 1
        self._logger.debug(f"åˆ›å»ºæ–°æ•°æ®åº“è¿æ¥ï¼Œæ€»è¿æ¥æ•°: {self.total_connections}")
        return conn

    async def get_connection(self) -> aiosqlite.Connection:
        """è·å–æ•°æ®åº“è¿æ¥"""
        try:
            # å°è¯•ä»æ± ä¸­è·å–è¿æ¥ï¼ˆéé˜»å¡ï¼‰
            conn = self.pool.get_nowait()
            self.active_connections += 1
            return conn
        except asyncio.QueueEmpty:
            # æ± ä¸­æ— å¯ç”¨è¿æ¥
            async with self._lock:
                if self.total_connections < self.max_connections:
                    # å¯ä»¥åˆ›å»ºæ–°è¿æ¥
                    conn = await self._create_connection()
                    self.active_connections += 1
                    return conn
                else:
                    # è¾¾åˆ°æœ€å¤§è¿æ¥æ•°ï¼Œç­‰å¾…è¿æ¥å½’è¿˜
                    self._logger.debug("è¿æ¥æ± å·²æ»¡ï¼Œç­‰å¾…è¿æ¥å½’è¿˜...")
                    conn = await self.pool.get()
                    self.active_connections += 1
                    return conn

    async def return_connection(self, conn: aiosqlite.Connection):
        """å½’è¿˜æ•°æ®åº“è¿æ¥"""
        if conn:
            try:
                # æ£€æŸ¥è¿æ¥æ˜¯å¦ä»ç„¶æœ‰æ•ˆ
                await conn.execute('SELECT 1')
                await self.pool.put(conn)
                self.active_connections -= 1
            except Exception as e:
                # è¿æ¥å·²æŸåï¼Œå…³é—­å¹¶å‡å°‘è®¡æ•°
                self._logger.warning(f"è¿æ¥å·²æŸåï¼Œå…³é—­è¿æ¥: {e}")
                try:
                    await conn.close()
                except:
                    pass
                self.total_connections -= 1
                self.active_connections -= 1

    async def close_all(self):
        """å…³é—­æ‰€æœ‰è¿æ¥"""
        self._logger.info("å¼€å§‹å…³é—­æ•°æ®åº“è¿æ¥æ± ...")
        
        # å…³é—­æ± ä¸­çš„æ‰€æœ‰è¿æ¥
        while not self.pool.empty():
            try:
                conn = self.pool.get_nowait()
                await conn.close()
                self.total_connections -= 1
            except asyncio.QueueEmpty:
                break
            except Exception as e:
                self._logger.error(f"å…³é—­è¿æ¥æ—¶å‡ºé”™: {e}")
        
        self._logger.info(f"æ•°æ®åº“è¿æ¥æ± å·²å…³é—­ï¼Œå‰©ä½™è¿æ¥æ•°: {self.total_connections}")

    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return await self.get_connection()

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡º"""
        # æ³¨æ„ï¼šè¿™é‡Œä¸èƒ½ç›´æ¥å½’è¿˜è¿æ¥ï¼Œå› ä¸ºæˆ‘ä»¬ä¸çŸ¥é“è¿æ¥å¯¹è±¡
        # å®é™…ä½¿ç”¨æ—¶éœ€è¦åœ¨è°ƒç”¨æ–¹æ‰‹åŠ¨å½’è¿˜
        pass


class DatabaseManager(AsyncServiceBase):
    """æ•°æ®åº“ç®¡ç†å™¨ - ä½¿ç”¨è¿æ¥æ± ç®¡ç†æ•°æ®åº“è¿æ¥ï¼Œæ”¯æŒSQLiteå’ŒMySQL"""

    def __init__(self, config: PluginConfig, context=None, skip_table_init: bool = False):
        super().__init__("database_manager")
        self.config = config
        self.context = context
        self.group_db_connections: Dict[str, aiosqlite.Connection] = {}
        self.skip_table_init = skip_table_init  # âœ¨ æ–°å¢ï¼šè·³è¿‡è¡¨åˆå§‹åŒ–æ ‡å¿—

        # å®‰å…¨åœ°æ„å»ºè·¯å¾„
        if not config.data_dir:
            raise ValueError("config.data_dir ä¸èƒ½ä¸ºç©º")

        self.group_data_dir = os.path.join(config.data_dir, "group_databases")
        self.messages_db_path = config.messages_db_path

        # æ–°å¢: æ•°æ®åº“åç«¯ï¼ˆæ”¯æŒSQLiteå’ŒMySQLï¼‰
        self.db_backend: Optional[IDatabaseBackend] = None

        # âœ¨ æ–°å¢: DatabaseEngine for ORMæ”¯æŒ
        self.db_engine: Optional[DatabaseEngine] = None

        # åˆå§‹åŒ–è¿æ¥æ± ï¼ˆä¿ç•™æ—§çš„SQLiteè¿æ¥æ± ï¼Œç”¨äºgroupæ•°æ®åº“ï¼‰
        self.connection_pool = DatabaseConnectionPool(
            db_path=self.messages_db_path,
            max_connections=config.max_connections,
            min_connections=config.min_connections
        )

        # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
        os.makedirs(self.group_data_dir, exist_ok=True)

        self._logger.info(f"æ•°æ®åº“ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ (ç±»å‹: {config.db_type}, è·³è¿‡è¡¨åˆå§‹åŒ–: {skip_table_init})")

    async def _do_start(self) -> bool:
        """å¯åŠ¨æœåŠ¡æ—¶åˆå§‹åŒ–è¿æ¥æ± å’Œæ•°æ®åº“"""
        try:
            self._logger.info(f"ğŸš€ [DatabaseManager] å¼€å§‹å¯åŠ¨ (db_type={self.config.db_type}, skip_table_init={self.skip_table_init})")

            # 1. åˆ›å»ºæ•°æ®åº“åç«¯ï¼ˆæ— è®º skip_table_init æ˜¯å¦ä¸º True éƒ½éœ€è¦åˆå§‹åŒ–åç«¯ï¼‰
            # skip_table_init åªå½±å“è¡¨çš„åˆ›å»ºï¼Œä¸å½±å“åç«¯è¿æ¥çš„åˆå§‹åŒ–
            self._logger.info(f"ğŸ“¡ [DatabaseManager] æ­£åœ¨åˆå§‹åŒ– {self.config.db_type} æ•°æ®åº“åç«¯...")
            backend_success = await self._initialize_database_backend()

            # 2. å¦‚æœæ•°æ®åº“åç«¯åˆå§‹åŒ–å¤±è´¥ï¼Œç›´æ¥æŠ¥é”™ï¼Œä¸å›é€€
            if not backend_success or not self.db_backend:
                error_msg = f"âŒ {self.config.db_type} æ•°æ®åº“åç«¯åˆå§‹åŒ–å¤±è´¥"
                self._logger.error(error_msg)
                raise RuntimeError(error_msg)

            self._logger.info(f"âœ… [DatabaseManager] {self.config.db_type} åç«¯åˆå§‹åŒ–æˆåŠŸ")

            # 3. åˆå§‹åŒ–æ—§çš„è¿æ¥æ± ï¼ˆä»…ç”¨äºgroupæ•°æ®åº“ï¼Œæš‚æ—¶ä¿ç•™ï¼‰
            await self.connection_pool.initialize()
            self._logger.info("âœ… [DatabaseManager] æ•°æ®åº“è¿æ¥æ± åˆå§‹åŒ–æˆåŠŸ")

            # 4. åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„ï¼ˆå¦‚æœè¡¨ä¸å­˜åœ¨åˆ™è‡ªåŠ¨åˆ›å»ºï¼‰
            # å¦‚æœ skip_table_init=Trueï¼ˆç”± ORM ç®¡ç†è¡¨ï¼‰ï¼Œåˆ™è·³è¿‡è¡¨åˆ›å»º
            if not self.skip_table_init:
                await self._init_messages_database()
                self._logger.info("âœ… [DatabaseManager] å…¨å±€æ¶ˆæ¯æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ")
            else:
                self._logger.info("â­ï¸ [DatabaseManager] è·³è¿‡ä¼ ç»Ÿæ•°æ®åº“è¡¨åˆ›å»ºï¼ˆç”± SQLAlchemy ORM ç®¡ç†ï¼‰")

            self._logger.info(f"ğŸ‰ [DatabaseManager] æ•°æ®åº“ç®¡ç†å™¨å¯åŠ¨å®Œæˆ (ä½¿ç”¨åç«¯: {self.config.db_type})")
            return True
        except Exception as e:
            self._logger.error(f"âŒ [DatabaseManager] å¯åŠ¨æ•°æ®åº“ç®¡ç†å™¨å¤±è´¥: {e}", exc_info=True)
            return False

    async def _initialize_database_backend(self) -> bool:
        """åˆå§‹åŒ–æ•°æ®åº“åç«¯"""
        try:
            # æ„å»ºæ•°æ®åº“é…ç½®
            db_type = DatabaseType(self.config.db_type.lower())

            if db_type == DatabaseType.SQLITE:
                db_config = DatabaseConfig(
                    db_type=DatabaseType.SQLITE,
                    sqlite_path=self.messages_db_path,
                    max_connections=self.config.max_connections,
                    min_connections=self.config.min_connections
                )
            elif db_type == DatabaseType.MYSQL:
                db_config = DatabaseConfig(
                    db_type=DatabaseType.MYSQL,
                    mysql_host=self.config.mysql_host,
                    mysql_port=self.config.mysql_port,
                    mysql_user=self.config.mysql_user,
                    mysql_password=self.config.mysql_password,
                    mysql_database=self.config.mysql_database,
                    max_connections=self.config.max_connections,
                    min_connections=self.config.min_connections
                )
            elif db_type == DatabaseType.POSTGRESQL:
                db_config = DatabaseConfig(
                    db_type=DatabaseType.POSTGRESQL,
                    postgresql_host=self.config.postgresql_host,
                    postgresql_port=self.config.postgresql_port,
                    postgresql_user=self.config.postgresql_user,
                    postgresql_password=self.config.postgresql_password,
                    postgresql_database=self.config.postgresql_database,
                    postgresql_schema=self.config.postgresql_schema,
                    max_connections=self.config.max_connections,
                    min_connections=self.config.min_connections
                )
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®åº“ç±»å‹: {self.config.db_type}")

            # ä½¿ç”¨å·¥å‚åˆ›å»ºåç«¯
            self.db_backend = DatabaseFactory.create_backend(db_config)
            if not self.db_backend:
                raise Exception("åˆ›å»ºæ•°æ®åº“åç«¯å¤±è´¥")

            # åˆå§‹åŒ–åç«¯
            success = await self.db_backend.initialize()
            if not success:
                raise Exception("æ•°æ®åº“åç«¯åˆå§‹åŒ–å¤±è´¥")

            self._logger.info(f"æ•°æ®åº“åç«¯åˆå§‹åŒ–æˆåŠŸ: {self.config.db_type}")
            return True

        except Exception as e:
            self._logger.error(f"åˆå§‹åŒ–æ•°æ®åº“åç«¯å¤±è´¥: {e}", exc_info=True)
            return False

    async def _do_stop(self) -> bool:
        """åœæ­¢æœåŠ¡æ—¶å…³é—­æ‰€æœ‰æ•°æ®åº“è¿æ¥"""
        try:
            # å…³é—­æ•°æ®åº“åç«¯
            if self.db_backend:
                await self.db_backend.close()

            # å…³é—­æ—§çš„è¿æ¥æ± 
            await self.close_all_connections()
            await self.connection_pool.close_all()

            self._logger.info("æ‰€æœ‰æ•°æ®åº“è¿æ¥å·²å…³é—­")
            return True
        except Exception as e:
            self._logger.error(f"å…³é—­æ•°æ®åº“ç®¡ç†å™¨å¤±è´¥: {e}", exc_info=True)
            return False

    def get_db_connection(self):
        """
        è·å–æ•°æ®åº“è¿æ¥çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        æ ¹æ®é…ç½®çš„æ•°æ®åº“ç±»å‹ï¼Œè‡ªåŠ¨é€‰æ‹©SQLiteã€MySQLæˆ–PostgreSQLåç«¯
        """
        db_type = self.config.db_type.lower()

        # ğŸ” è°ƒè¯•æ—¥å¿—ï¼šè¾“å‡ºæ•°æ®åº“ç±»å‹å’Œåç«¯çŠ¶æ€
        self._logger.debug(f"[get_db_connection] é…ç½®çš„æ•°æ®åº“ç±»å‹: {db_type}")
        self._logger.debug(f"[get_db_connection] db_backend çŠ¶æ€: {self.db_backend is not None}")

        # å¦‚æœä½¿ç”¨MySQLæˆ–PostgreSQLä¸”db_backendå¯ç”¨ï¼Œä½¿ç”¨é€šç”¨åç«¯è¿æ¥ç®¡ç†å™¨
        if db_type in ('mysql', 'postgresql') and self.db_backend:
            self._logger.debug(f"[get_db_connection] âœ… ä½¿ç”¨ {db_type.upper()} åç«¯")
            return self._get_backend_connection_manager()
        else:
            # ä½¿ç”¨æ—§çš„SQLiteè¿æ¥æ± 
            self._logger.warning(f"[get_db_connection] âš ï¸ å›é€€åˆ° SQLite è¿æ¥æ±  (db_type={db_type}, backend_exists={self.db_backend is not None})")
            return self._get_sqlite_connection_manager()

    def _get_sqlite_connection_manager(self):
        """è·å–SQLiteè¿æ¥ç®¡ç†å™¨"""
        class SQLiteConnectionManager:
            def __init__(self, pool: DatabaseConnectionPool):
                self.pool = pool
                self.connection = None

            async def __aenter__(self):
                self.connection = await self.pool.get_connection()
                return self.connection

            async def __aexit__(self, exc_type, exc_val, exc_tb):
                if self.connection:
                    await self.pool.return_connection(self.connection)

        return SQLiteConnectionManager(self.connection_pool)

    def _get_backend_connection_manager(self):
        """è·å–MySQL/PostgreSQLè¿æ¥ç®¡ç†å™¨ - é€‚é…aiosqliteæ¥å£"""
        db_backend = self.db_backend

        class BackendConnectionAdapter:
            """æ•°æ®åº“åç«¯è¿æ¥é€‚é…å™¨ - æ¨¡æ‹Ÿaiosqliteæ¥å£"""
            def __init__(self, backend):
                self.backend = backend
                self._cursor = None

            async def cursor(self):
                """è¿”å›æ¸¸æ ‡é€‚é…å™¨"""
                return BackendCursorAdapter(self.backend)

            async def commit(self):
                """æäº¤äº‹åŠ¡ - åç«¯åœ¨executeä¸­å·²è‡ªåŠ¨æäº¤"""
                pass

            async def rollback(self):
                """å›æ»šäº‹åŠ¡"""
                await self.backend.rollback()

            async def execute(self, sql, params=None):
                """æ‰§è¡ŒSQL"""
                return await self.backend.execute(sql, params)

            async def executemany(self, sql, params_list):
                """æ‰¹é‡æ‰§è¡ŒSQL"""
                return await self.backend.execute_many(sql, params_list)

            async def fetchone(self):
                """è·å–å•è¡Œ"""
                return await self._cursor.fetchone() if self._cursor else None

            async def fetchall(self):
                """è·å–æ‰€æœ‰è¡Œ"""
                return await self._cursor.fetchall() if self._cursor else []

        class BackendCursorAdapter:
            """æ•°æ®åº“åç«¯æ¸¸æ ‡é€‚é…å™¨"""
            def __init__(self, backend):
                self.backend = backend
                self._last_result = None
                self.lastrowid = None
                self.rowcount = 0

            async def execute(self, sql, params=None):
                """æ‰§è¡ŒSQLå¹¶å­˜å‚¨ç»“æœ"""
                import re

                # æ£€æµ‹æ˜¯SELECTæŸ¥è¯¢è¿˜æ˜¯å…¶ä»–æ“ä½œ
                sql_upper = sql.strip().upper()

                # è·å–æ•°æ®åº“ç±»å‹
                db_type = self.backend.db_type
                is_mysql = (db_type == DatabaseType.MYSQL)
                is_postgresql = (db_type == DatabaseType.POSTGRESQL)

                # å¯¹äº CREATE TABLE å’Œ ALTER TABLEï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
                if sql_upper.startswith('CREATE TABLE') or sql_upper.startswith('ALTER TABLE'):
                    # ä½¿ç”¨åç«¯çš„ convert_ddl è¿›è¡Œè½¬æ¢
                    converted_sql = self.backend.convert_ddl(sql)
                    await self.backend.execute(converted_sql, None)
                    self._last_result = []
                    self.rowcount = 0
                    return self

                # è½¬æ¢å‚æ•°å ä½ç¬¦
                if is_mysql:
                    # âœ… MySQL: è½¬æ¢ INSERT OR REPLACE ä¸º REPLACE INTO
                    converted_sql = sql.replace('INSERT OR REPLACE', 'REPLACE')
                    # è½¬æ¢å‚æ•°å ä½ç¬¦ ? -> %s
                    converted_sql = converted_sql.replace('?', '%s')
                elif is_postgresql:
                    # PostgreSQL ä½¿ç”¨ $1, $2, ...
                    # è°ƒç”¨åç«¯çš„å ä½ç¬¦è½¬æ¢æ–¹æ³•
                    converted_sql = self.backend._convert_placeholders(sql) if hasattr(self.backend, '_convert_placeholders') else sql
                else:
                    converted_sql = sql

                # ç¡®ä¿ params æ˜¯ tuple ç±»å‹
                if params is not None and not isinstance(params, tuple):
                    if isinstance(params, list):
                        params = tuple(params)
                    else:
                        params = (params,)

                # å¤„ç† sqlite_master æŸ¥è¯¢
                if 'SQLITE_MASTER' in sql_upper:
                    table_match = re.search(r"NAME\s*=\s*['\"]?(\w+)['\"]?", sql_upper)
                    if table_match:
                        table_name = table_match.group(1).lower()
                        if is_mysql:
                            check_sql = """
                                SELECT TABLE_NAME as name
                                FROM INFORMATION_SCHEMA.TABLES
                                WHERE TABLE_SCHEMA = DATABASE() AND LOWER(TABLE_NAME) = %s
                            """
                            self._last_result = await self.backend.fetch_all(check_sql, (table_name,))
                        elif is_postgresql:
                            check_sql = """
                                SELECT table_name as name
                                FROM information_schema.tables
                                WHERE table_schema = $1 AND LOWER(table_name) = $2
                            """
                            schema = getattr(self.backend.config, 'postgresql_schema', 'public')
                            self._last_result = await self.backend.fetch_all(check_sql, (schema, table_name))
                        self.rowcount = len(self._last_result) if self._last_result else 0
                        return self
                    else:
                        self._last_result = []
                        self.rowcount = 0
                        return self

                # å¤„ç† PRAGMA table_info æŸ¥è¯¢
                if sql_upper.startswith('PRAGMA'):
                    pragma_match = re.search(r'PRAGMA\s+TABLE_INFO\s*\(\s*(\w+)\s*\)', sql_upper)
                    if pragma_match:
                        table_name = pragma_match.group(1)
                        try:
                            if is_mysql:
                                describe_sql = f"DESCRIBE {table_name}"
                                mysql_result = await self.backend.fetch_all(describe_sql, None)
                                self._last_result = []
                                for idx, row in enumerate(mysql_result or []):
                                    field_name = row[0]
                                    field_type = row[1]
                                    is_nullable = 0 if row[2] == 'NO' else 1
                                    default_value = row[4]
                                    is_pk = 1 if row[3] == 'PRI' else 0
                                    self._last_result.append((idx, field_name, field_type, 1 - is_nullable, default_value, is_pk))
                            elif is_postgresql:
                                # PostgreSQL ä½¿ç”¨ information_schema.columns
                                schema = getattr(self.backend.config, 'postgresql_schema', 'public')
                                pg_sql = """
                                    SELECT
                                        ordinal_position - 1 as cid,
                                        column_name as name,
                                        data_type as type,
                                        CASE WHEN is_nullable = 'NO' THEN 1 ELSE 0 END as notnull,
                                        column_default as dflt_value,
                                        0 as pk
                                    FROM information_schema.columns
                                    WHERE table_schema = $1 AND table_name = $2
                                    ORDER BY ordinal_position
                                """
                                self._last_result = await self.backend.fetch_all(pg_sql, (schema, table_name))
                            self.rowcount = len(self._last_result)
                        except Exception:
                            self._last_result = []
                            self.rowcount = 0
                        return self
                    else:
                        self._last_result = []
                        self.rowcount = 0
                        return self

                if sql_upper.startswith('SELECT'):
                    self._last_result = await self.backend.fetch_all(converted_sql, params)
                    self.rowcount = len(self._last_result) if self._last_result else 0
                else:
                    # INSERT/UPDATE/DELETE
                    self.rowcount = await self.backend.execute(converted_sql, params)
                    # å°è¯•è·å–lastrowidï¼ˆå¯¹äºINSERTæ“ä½œï¼‰
                    if sql_upper.startswith('INSERT'):
                        try:
                            if is_mysql:
                                result = await self.backend.fetch_one("SELECT LAST_INSERT_ID()")
                            elif is_postgresql:
                                result = await self.backend.fetch_one("SELECT lastval()")
                            else:
                                result = None
                            self.lastrowid = result[0] if result else None
                        except Exception:
                            self.lastrowid = None
                return self

            async def executemany(self, sql, params_list):
                """æ‰¹é‡æ‰§è¡ŒSQL"""
                db_type = self.backend.db_type
                if db_type == DatabaseType.MYSQL:
                    converted_sql = sql.replace('?', '%s')
                elif db_type == DatabaseType.POSTGRESQL:
                    converted_sql = self.backend._convert_placeholders(sql) if hasattr(self.backend, '_convert_placeholders') else sql
                else:
                    converted_sql = sql
                self.rowcount = await self.backend.execute_many(converted_sql, params_list)
                return self

            async def fetchone(self):
                """è·å–å•è¡Œç»“æœ"""
                if self._last_result and len(self._last_result) > 0:
                    return self._last_result[0]
                return None

            async def fetchall(self):
                """è·å–æ‰€æœ‰ç»“æœ"""
                return self._last_result if self._last_result else []

            def __aiter__(self):
                """æ”¯æŒå¼‚æ­¥è¿­ä»£"""
                self._iter_index = 0
                return self

            async def __anext__(self):
                """å¼‚æ­¥è¿­ä»£"""
                if not self._last_result or self._iter_index >= len(self._last_result):
                    raise StopAsyncIteration
                result = self._last_result[self._iter_index]
                self._iter_index += 1
                return result

            async def close(self):
                """å…³é—­æ¸¸æ ‡ï¼ˆåç«¯ä½¿ç”¨è¿æ¥æ± ï¼Œæ— éœ€å®é™…å…³é—­ï¼‰"""
                self._last_result = None
                self.lastrowid = None
                self.rowcount = 0

        class BackendConnectionManager:
            def __init__(self, backend):
                self.backend = backend
                self.adapter = None

            async def __aenter__(self):
                self.adapter = BackendConnectionAdapter(self.backend)
                return self.adapter

            async def __aexit__(self, exc_type, exc_val, exc_tb):
                # åç«¯ä½¿ç”¨è¿æ¥æ± ï¼Œæ— éœ€æ‰‹åŠ¨å…³é—­
                pass

        return BackendConnectionManager(db_backend)

    def get_connection(self):
        """
        è·å–æ•°æ®åº“è¿æ¥çš„åŒæ­¥æ¥å£ï¼Œç”¨äºå…¼å®¹æ—§ä»£ç 
        æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªåŒæ­¥æ–¹æ³•ï¼Œç”¨äºå…¼å®¹ä½¿ç”¨ 'with' è¯­å¥çš„ä»£ç 
        """
        class SyncConnectionWrapper:
            def __init__(self, db_manager):
                self.db_manager = db_manager
                self.connection = None
                
            def __enter__(self):
                # åŒæ­¥è·å–è¿æ¥ï¼Œè¿™éœ€è¦åœ¨å¼‚æ­¥ä¸Šä¸‹æ–‡ä¸­ä½¿ç”¨
                import sqlite3
                # ç›´æ¥åˆ›å»ºåŒæ­¥è¿æ¥åˆ°åŒä¸€ä¸ªæ•°æ®åº“æ–‡ä»¶
                self.connection = sqlite3.connect(self.db_manager.messages_db_path)
                return self.connection
                
            def __exit__(self, exc_type, exc_val, exc_tb):
                if self.connection:
                    self.connection.close()
        
        return SyncConnectionWrapper(self)

    async def close_all_connections(self):
        """å…³é—­æ‰€æœ‰æ•°æ®åº“è¿æ¥"""
        try:
            # å…³é—­æ‰€æœ‰ç¾¤ç»„æ•°æ®åº“è¿æ¥
            for group_id, conn in list(self.group_db_connections.items()):
                try:
                    await conn.close()
                    self._logger.info(f"ç¾¤ç»„ {group_id} æ•°æ®åº“è¿æ¥å·²å…³é—­")
                except Exception as e:
                    self._logger.error(f"å…³é—­ç¾¤ç»„ {group_id} æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            
            self.group_db_connections.clear()
            self._logger.info("æ‰€æœ‰ç¾¤ç»„æ•°æ®åº“è¿æ¥å·²å…³é—­")
            
        except Exception as e:
            self._logger.error(f"å…³é—­æ•°æ®åº“è¿æ¥è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            raise

    async def _retry_on_connection_error(self, func, *args, **kwargs):
        """åœ¨è¿æ¥é”™è¯¯æ—¶é‡è¯•çš„é€šç”¨æ–¹æ³•ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰"""
        try:
            return await func(*args, **kwargs)
        except Exception as e:
            if "no active connection" in str(e).lower():
                self._logger.warning(f"æ£€æµ‹åˆ°è¿æ¥é—®é¢˜: {e}ï¼Œå°è¯•é‡æ–°æ‰§è¡Œ...")
                try:
                    # è¿æ¥æ± ä¼šè‡ªåŠ¨å¤„ç†è¿æ¥é—®é¢˜ï¼Œç›´æ¥é‡è¯•
                    return await func(*args, **kwargs)
                except Exception as retry_error:
                    self._logger.error(f"é‡è¯•ä¹Ÿå¤±è´¥: {retry_error}")
                    raise retry_error
            else:
                raise e

    async def _init_messages_database(self):
        """
        åˆå§‹åŒ–å…¨å±€æ¶ˆæ¯æ•°æ®åº“ï¼ˆæ ¹æ®æ•°æ®åº“ç±»å‹é€‰æ‹©åç«¯ï¼‰

        âš ï¸ å·²åºŸå¼ƒï¼šæ‰€æœ‰è¡¨ç»“æ„ç”± SQLAlchemy ORM ç»Ÿä¸€ç®¡ç†
        æ­¤æ–¹æ³•ä¿ç•™ä»…ç”¨äºå‘åå…¼å®¹ï¼Œä¸å†åˆ›å»ºè¡¨
        """
        self._logger.info("â­ï¸ [ä¼ ç»Ÿæ•°æ®åº“ç®¡ç†å™¨] è¡¨åˆ›å»ºå·²ç”± SQLAlchemy ORM æ¥ç®¡ï¼Œè·³è¿‡ä¼ ç»Ÿè¡¨åˆå§‹åŒ–")
        # å¦‚æœä½¿ç”¨MySQLåç«¯ï¼Œä½¿ç”¨db_backendåˆå§‹åŒ–è¡¨
        # if self.db_backend and self.config.db_type.lower() == 'mysql':
        #     await self._init_messages_database_mysql()
        #     self._logger.info("MySQLæ•°æ®åº“è¡¨åˆå§‹åŒ–å®Œæˆã€‚")
        # else:
        #     # ä½¿ç”¨æ—§çš„SQLiteè¿æ¥æ± 
        #     async with self.get_db_connection() as conn:
        #         await self._init_messages_database_tables(conn)
        #         self._logger.info("å…¨å±€æ¶ˆæ¯æ•°æ®åº“è¿æ¥æ± åˆå§‹åŒ–å®Œæˆå¹¶è¡¨å·²åˆå§‹åŒ–ã€‚")

    async def _init_messages_database_mysql(self):
        """
        ä½¿ç”¨MySQLåç«¯åˆå§‹åŒ–æ•°æ®åº“è¡¨

        âš ï¸ å·²åºŸå¼ƒï¼šæ‰€æœ‰è¡¨ç»“æ„ç”± SQLAlchemy ORM ç»Ÿä¸€ç®¡ç†
        æ­¤æ–¹æ³•ä¿ç•™ä»…ç”¨äºå‚è€ƒï¼Œä¸å†ä½¿ç”¨
        """
        self._logger.warning("âš ï¸ [ä¼ ç»Ÿæ•°æ®åº“ç®¡ç†å™¨] _init_messages_database_mysql å·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨ SQLAlchemy ORM")
        return

        # ä»¥ä¸‹ä»£ç å·²ç¦ç”¨ï¼Œä¿ç•™ä»…ä¾›å‚è€ƒ
        """
        try:
            # åˆ›å»ºåŸå§‹æ¶ˆæ¯è¡¨
            self._logger.info("å°è¯•åˆ›å»º raw_messages è¡¨ (MySQL)...")
            await self.db_backend.execute('''
                CREATE TABLE IF NOT EXISTS raw_messages (
                    id INT PRIMARY KEY AUTO_INCREMENT,
                    sender_id VARCHAR(255) NOT NULL,
                    sender_name VARCHAR(255),
                    message TEXT NOT NULL,
                    group_id VARCHAR(255),
                    platform VARCHAR(50),
                    timestamp DOUBLE NOT NULL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    processed TINYINT(1) DEFAULT 0,
                    INDEX idx_timestamp (timestamp),
                    INDEX idx_sender (sender_id),
                    INDEX idx_processed (processed),
                    INDEX idx_group (group_id)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
            ''')
            self._logger.info("raw_messages è¡¨åˆ›å»º/æ£€æŸ¥å®Œæˆã€‚")

            # åˆ›å»ºBotæ¶ˆæ¯è¡¨
            self._logger.info("å°è¯•åˆ›å»º bot_messages è¡¨ (MySQL)...")
            await self.db_backend.execute('''
                CREATE TABLE IF NOT EXISTS bot_messages (
                    id INT PRIMARY KEY AUTO_INCREMENT,
                    group_id VARCHAR(255) NOT NULL,
                    user_id VARCHAR(255),
                    message TEXT NOT NULL,
                    response_to_message_id INT,
                    context_type VARCHAR(100),
                    temperature DOUBLE,
                    language_style VARCHAR(100),
                    response_pattern VARCHAR(255),
                    timestamp DOUBLE NOT NULL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    INDEX idx_group (group_id),
                    INDEX idx_timestamp (timestamp)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
            ''')
            self._logger.info("bot_messages è¡¨åˆ›å»º/æ£€æŸ¥å®Œæˆã€‚")

            # åˆ›å»ºç­›é€‰åæ¶ˆæ¯è¡¨
            self._logger.info("å°è¯•åˆ›å»º filtered_messages è¡¨ (MySQL)...")
            await self.db_backend.execute('''
                CREATE TABLE IF NOT EXISTS filtered_messages (
                    id INT PRIMARY KEY AUTO_INCREMENT,
                    raw_message_id INT,
                    message TEXT NOT NULL,
                    sender_id VARCHAR(255),
                    group_id VARCHAR(255),
                    confidence DOUBLE,
                    filter_reason TEXT,
                    timestamp DOUBLE NOT NULL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    used_for_learning TINYINT(1) DEFAULT 0,
                    quality_scores TEXT,
                    refined TINYINT(1) DEFAULT 0,
                    INDEX idx_confidence (confidence),
                    INDEX idx_used (used_for_learning),
                    INDEX idx_group (group_id)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
            ''')
            self._logger.info("filtered_messages è¡¨åˆ›å»º/æ£€æŸ¥å®Œæˆã€‚")

            # åˆ›å»ºå­¦ä¹ æ‰¹æ¬¡è¡¨
            await self.db_backend.execute('''
                CREATE TABLE IF NOT EXISTS learning_batches (
                    id INT PRIMARY KEY AUTO_INCREMENT,
                    group_id VARCHAR(255) NOT NULL,
                    start_time DOUBLE NOT NULL,
                    end_time DOUBLE,
                    quality_score DOUBLE DEFAULT 0.5,
                    processed_messages INT DEFAULT 0,
                    batch_name VARCHAR(255) UNIQUE,
                    message_count INT,
                    filtered_count INT,
                    success TINYINT(1),
                    error_message TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    INDEX idx_group (group_id)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
            ''')

            # åˆ›å»ºäººæ ¼æ›´æ–°è®°å½•è¡¨
            await self.db_backend.execute('''
                CREATE TABLE IF NOT EXISTS persona_update_records (
                    id INT PRIMARY KEY AUTO_INCREMENT,
                    timestamp DOUBLE NOT NULL,
                    group_id VARCHAR(255) NOT NULL,
                    update_type VARCHAR(100) NOT NULL,
                    original_content TEXT,
                    new_content TEXT NOT NULL,
                    reason TEXT,
                    status VARCHAR(50) DEFAULT 'pending',
                    reviewer_comment TEXT,
                    review_time DOUBLE,
                    INDEX idx_status (status),
                    INDEX idx_group (group_id)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
            ''')

            # åˆ›å»ºå¼ºåŒ–å­¦ä¹ ç»“æœè¡¨
            await self.db_backend.execute('''
                CREATE TABLE IF NOT EXISTS reinforcement_learning_results (
                    id INT PRIMARY KEY AUTO_INCREMENT,
                    group_id VARCHAR(255) NOT NULL,
                    timestamp DOUBLE NOT NULL,
                    replay_analysis TEXT,
                    optimization_strategy TEXT,
                    reinforcement_feedback TEXT,
                    next_action TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    INDEX idx_group (group_id)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
            ''')

            # åˆ›å»ºç­–ç•¥ä¼˜åŒ–ç»“æœè¡¨
            await self.db_backend.execute('''
                CREATE TABLE IF NOT EXISTS strategy_optimization_results (
                    id INT PRIMARY KEY AUTO_INCREMENT,
                    group_id VARCHAR(255) NOT NULL,
                    timestamp DOUBLE NOT NULL,
                    exploration_type VARCHAR(100),
                    effectiveness_score DOUBLE,
                    new_strategy TEXT,
                    rollback_reason TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    INDEX idx_group (group_id)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
            ''')

            # åˆ›å»ºå­¦ä¹ æ€§èƒ½å†å²è¡¨
            await self.db_backend.execute('''
                CREATE TABLE IF NOT EXISTS learning_performance_history (
                    id INT PRIMARY KEY AUTO_INCREMENT,
                    group_id VARCHAR(255) NOT NULL,
                    session_id VARCHAR(255),
                    timestamp DOUBLE NOT NULL,
                    quality_score DOUBLE,
                    learning_time DOUBLE,
                    success TINYINT(1),
                    successful_pattern TEXT,
                    failed_pattern TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    INDEX idx_group (group_id),
                    INDEX idx_session (session_id),
                    INDEX idx_timestamp (timestamp)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
            ''')

            # åˆ›å»ºLLMè°ƒç”¨ç»Ÿè®¡è¡¨
            await self.db_backend.execute('''
                CREATE TABLE IF NOT EXISTS llm_call_statistics (
                    id INT PRIMARY KEY AUTO_INCREMENT,
                    call_type VARCHAR(100) NOT NULL,
                    provider VARCHAR(100),
                    model VARCHAR(100),
                    input_tokens INT DEFAULT 0,
                    output_tokens INT DEFAULT 0,
                    total_tokens INT DEFAULT 0,
                    latency_ms DOUBLE,
                    success TINYINT(1) DEFAULT 1,
                    error_message TEXT,
                    group_id VARCHAR(255),
                    timestamp DOUBLE NOT NULL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    INDEX idx_call_type (call_type),
                    INDEX idx_timestamp (timestamp)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
            ''')

            # åˆ›å»ºé»‘è¯è¡¨ï¼ˆä¸ SQLite ç‰ˆæœ¬ç»“æ„ä¸€è‡´ï¼‰
            await self.db_backend.execute('''
                CREATE TABLE IF NOT EXISTS jargon (
                    id INT PRIMARY KEY AUTO_INCREMENT,
                    content TEXT NOT NULL,
                    raw_content TEXT,
                    meaning TEXT,
                    is_jargon TINYINT(1),
                    count INT DEFAULT 1,
                    last_inference_count INT DEFAULT 0,
                    is_complete TINYINT(1) DEFAULT 0,
                    is_global TINYINT(1) DEFAULT 0,
                    chat_id VARCHAR(255) NOT NULL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
                    UNIQUE KEY uk_chat_content (chat_id, content(255)),
                    INDEX idx_content (content(255)),
                    INDEX idx_chat_id (chat_id),
                    INDEX idx_is_jargon (is_jargon)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
            ''')

            # åˆ›å»ºç¤¾äº¤å…³ç³»è¡¨
            await self.db_backend.execute('''
                CREATE TABLE IF NOT EXISTS social_relations (
                    id INT PRIMARY KEY AUTO_INCREMENT,
                    user_id VARCHAR(255) NOT NULL,
                    group_id VARCHAR(255) NOT NULL,
                    relation_type VARCHAR(100),
                    affection_score DOUBLE DEFAULT 0,
                    interaction_count INT DEFAULT 0,
                    last_interaction DOUBLE,
                    metadata TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
                    UNIQUE KEY uk_user_group (user_id, group_id),
                    INDEX idx_group (group_id),
                    INDEX idx_affection (affection_score)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
            ''')

            # åˆ›å»ºè¡¨è¾¾æ¨¡å¼è¡¨ï¼ˆä¸ expression_pattern_learner.py ä¸­çš„ SQLite ç»“æ„ä¸€è‡´ï¼‰
            await self.db_backend.execute('''
                CREATE TABLE IF NOT EXISTS expression_patterns (
                    id INT PRIMARY KEY AUTO_INCREMENT,
                    situation TEXT NOT NULL,
                    expression TEXT NOT NULL,
                    weight DOUBLE NOT NULL DEFAULT 1.0,
                    last_active_time DOUBLE NOT NULL,
                    create_time DOUBLE NOT NULL,
                    group_id VARCHAR(255) NOT NULL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
                    UNIQUE KEY uk_situation_expression_group (situation(255), expression(255), group_id),
                    INDEX idx_group (group_id),
                    INDEX idx_weight (weight)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
            ''')

            # åˆ›å»ºè¯­è¨€é£æ ¼æ¨¡å¼è¡¨
            await self.db_backend.execute('''
                CREATE TABLE IF NOT EXISTS language_style_patterns (
                    id INT PRIMARY KEY AUTO_INCREMENT,
                    style_name VARCHAR(100) NOT NULL,
                    style_description TEXT,
                    examples TEXT,
                    frequency INT DEFAULT 1,
                    source_group VARCHAR(255),
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
                    INDEX idx_style (style_name)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
            ''')

            # åˆ›å»ºè¯é¢˜æ‘˜è¦è¡¨
            await self.db_backend.execute('''
                CREATE TABLE IF NOT EXISTS topic_summaries (
                    id INT PRIMARY KEY AUTO_INCREMENT,
                    group_id VARCHAR(255) NOT NULL,
                    topic VARCHAR(255) NOT NULL,
                    summary TEXT,
                    message_count INT DEFAULT 0,
                    start_time DOUBLE,
                    end_time DOUBLE,
                    keywords TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    INDEX idx_group (group_id),
                    INDEX idx_topic (topic)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
            ''')

            # åˆ›å»ºé£æ ¼å­¦ä¹ è®°å½•è¡¨
            await self.db_backend.execute('''
                CREATE TABLE IF NOT EXISTS style_learning_records (
                    id INT PRIMARY KEY AUTO_INCREMENT,
                    group_id VARCHAR(255) NOT NULL,
                    timestamp DOUBLE NOT NULL,
                    style_type VARCHAR(100),
                    learned_content TEXT,
                    confidence DOUBLE DEFAULT 0.5,
                    source_messages TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    INDEX idx_group (group_id),
                    INDEX idx_style (style_type)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
            ''')

            # åˆ›å»ºé£æ ¼å­¦ä¹ å®¡æ ¸è¡¨ï¼ˆä¸ SQLite ç‰ˆæœ¬çš„ _ensure_style_review_table_exists ç»“æ„ä¸€è‡´ï¼‰
            await self.db_backend.execute('''
                CREATE TABLE IF NOT EXISTS style_learning_reviews (
                    id INT PRIMARY KEY AUTO_INCREMENT,
                    type VARCHAR(100) NOT NULL,
                    group_id VARCHAR(255) NOT NULL,
                    timestamp DOUBLE NOT NULL,
                    learned_patterns TEXT,
                    few_shots_content TEXT,
                    status VARCHAR(50) DEFAULT 'pending',
                    description TEXT,
                    reviewer_comment TEXT,
                    review_time DOUBLE,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
                    INDEX idx_status (status),
                    INDEX idx_group (group_id),
                    INDEX idx_timestamp (timestamp)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
            ''')

            # åˆ›å»ºäººæ ¼èåˆå†å²è¡¨
            await self.db_backend.execute('''
                CREATE TABLE IF NOT EXISTS persona_fusion_history (
                    id INT PRIMARY KEY AUTO_INCREMENT,
                    group_id VARCHAR(255) NOT NULL,
                    timestamp DOUBLE NOT NULL,
                    base_persona_hash BIGINT,
                    incremental_hash BIGINT,
                    fusion_result TEXT,
                    compatibility_score DOUBLE,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    INDEX idx_group_id (group_id),
                    INDEX idx_timestamp (timestamp)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
            ''')

            # åˆ›å»ºäººæ ¼æ›´æ–°å®¡æ ¸è¡¨ï¼ˆä¸ SQLite ç‰ˆæœ¬ç»“æ„ä¸€è‡´ï¼‰
            await self.db_backend.execute('''
                CREATE TABLE IF NOT EXISTS persona_update_reviews (
                    id INT PRIMARY KEY AUTO_INCREMENT,
                    timestamp DOUBLE NOT NULL,
                    group_id VARCHAR(255) NOT NULL,
                    update_type VARCHAR(100) NOT NULL,
                    original_content TEXT,
                    new_content TEXT,
                    proposed_content TEXT,
                    confidence_score DOUBLE,
                    reason TEXT,
                    status VARCHAR(50) NOT NULL DEFAULT 'pending',
                    reviewer_comment TEXT,
                    review_time DOUBLE,
                    metadata TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    INDEX idx_status (status),
                    INDEX idx_group (group_id),
                    INDEX idx_timestamp (timestamp)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
            ''')

            self._logger.info("æ‰€æœ‰MySQLè¡¨åˆ›å»ºå®Œæˆ")

        except Exception as e:
            self._logger.error(f"MySQLè¡¨åˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
            raise
        """

    async def _init_messages_database_tables(self, conn: aiosqlite.Connection):
        """
        åˆå§‹åŒ–å…¨å±€æ¶ˆæ¯SQLiteæ•°æ®åº“çš„è¡¨ç»“æ„

        âš ï¸ å·²åºŸå¼ƒï¼šæ‰€æœ‰è¡¨ç»“æ„ç”± SQLAlchemy ORM ç»Ÿä¸€ç®¡ç†
        æ­¤æ–¹æ³•ä¿ç•™ä»…ç”¨äºå‘åå…¼å®¹ï¼Œä¸å†åˆ›å»ºè¡¨
        """
        self._logger.warning("âš ï¸ [ä¼ ç»Ÿæ•°æ®åº“ç®¡ç†å™¨] _init_messages_database_tables å·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨ SQLAlchemy ORM")
        return

        # ä»¥ä¸‹ä»£ç å·²ç¦ç”¨ï¼Œä¿ç•™ä»…ä¾›å‚è€ƒ
        """
        cursor = await conn.cursor()

        try:
            # è®¾ç½®æ•°æ®åº“ä¸ºWALæ¨¡å¼ï¼Œæé«˜å¹¶å‘æ€§èƒ½å¹¶é¿å…é”å®šé—®é¢˜
            await cursor.execute('PRAGMA journal_mode=WAL')
            await cursor.execute('PRAGMA synchronous=NORMAL')
            await cursor.execute('PRAGMA cache_size=10000')
            await cursor.execute('PRAGMA temp_store=memory')
            
            # åˆ›å»ºåŸå§‹æ¶ˆæ¯è¡¨
            self._logger.info("å°è¯•åˆ›å»º raw_messages è¡¨...")
            await cursor.execute('''
                CREATE TABLE IF NOT EXISTS raw_messages (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    sender_id TEXT NOT NULL,
                    sender_name TEXT,
                    message TEXT NOT NULL,
                    group_id TEXT,
                    platform TEXT,
                    timestamp REAL NOT NULL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    processed BOOLEAN DEFAULT FALSE
                )
            ''')
            self._logger.info("raw_messages è¡¨åˆ›å»º/æ£€æŸ¥å®Œæˆã€‚")
            await conn.commit() # å¼ºåˆ¶æäº¤ï¼Œç¡®ä¿è¡¨ç»“æ„å†™å…¥ç£ç›˜

            # åˆ›å»ºBotæ¶ˆæ¯è¡¨ (ç”¨äºå­˜å‚¨Botå‘é€çš„æ¶ˆæ¯ï¼Œä¾›å¤šæ ·æ€§ç®¡ç†å™¨ä½¿ç”¨)
            self._logger.info("å°è¯•åˆ›å»º bot_messages è¡¨...")
            await cursor.execute('''
                CREATE TABLE IF NOT EXISTS bot_messages (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    group_id TEXT NOT NULL,
                    user_id TEXT,
                    message TEXT NOT NULL,
                    response_to_message_id INTEGER,
                    context_type TEXT,
                    temperature REAL,
                    language_style TEXT,
                    response_pattern TEXT,
                    timestamp REAL NOT NULL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (response_to_message_id) REFERENCES raw_messages (id)
                )
            ''')
            self._logger.info("bot_messages è¡¨åˆ›å»º/æ£€æŸ¥å®Œæˆã€‚")
            await conn.commit()

            # åˆ›å»ºç­›é€‰åæ¶ˆæ¯è¡¨
            self._logger.info("å°è¯•åˆ›å»º filtered_messages è¡¨...")
            await cursor.execute('''
                CREATE TABLE IF NOT EXISTS filtered_messages (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    raw_message_id INTEGER,
                    message TEXT NOT NULL,
                    sender_id TEXT,
                    group_id TEXT,
                    confidence REAL,
                    filter_reason TEXT,
                    timestamp REAL NOT NULL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    used_for_learning BOOLEAN DEFAULT FALSE,
                    quality_scores TEXT, -- æ–°å¢å­—æ®µï¼Œå­˜å‚¨JSONå­—ç¬¦ä¸²
                    FOREIGN KEY (raw_message_id) REFERENCES raw_messages (id)
                )
            ''')
            self._logger.info("filtered_messages è¡¨åˆ›å»º/æ£€æŸ¥å®Œæˆã€‚")

            # æ£€æŸ¥å¹¶æ·»åŠ  quality_scores åˆ—ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            await cursor.execute("PRAGMA table_info(filtered_messages)")
            columns = [col[1] for col in await cursor.fetchall()]
            if 'quality_scores' not in columns:
                await cursor.execute("ALTER TABLE filtered_messages ADD COLUMN quality_scores TEXT")
                await conn.commit()  # ç«‹å³æäº¤ï¼Œç¡®ä¿åˆ—æ·»åŠ æˆåŠŸ
                logger.info("å·²ä¸º filtered_messages è¡¨æ·»åŠ  quality_scores åˆ—ã€‚")

            # æ£€æŸ¥å¹¶æ·»åŠ  group_id åˆ—ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            # é‡æ–°è·å–åˆ—ä¿¡æ¯ï¼Œå› ä¸ºå‰é¢å¯èƒ½æ·»åŠ äº†æ–°åˆ—
            await cursor.execute("PRAGMA table_info(filtered_messages)")
            columns = [col[1] for col in await cursor.fetchall()]
            if 'group_id' not in columns:
                await cursor.execute("ALTER TABLE filtered_messages ADD COLUMN group_id TEXT")
                await conn.commit()
                logger.info("å·²ä¸º filtered_messages è¡¨æ·»åŠ  group_id åˆ—ã€‚")

            # æ£€æŸ¥å¹¶æ·»åŠ  refined åˆ—ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            await cursor.execute("PRAGMA table_info(filtered_messages)")
            columns = [col[1] for col in await cursor.fetchall()]
            if 'refined' not in columns:
                await cursor.execute("ALTER TABLE filtered_messages ADD COLUMN refined BOOLEAN DEFAULT 0")
                await conn.commit()
                logger.info("å·²ä¸º filtered_messages è¡¨æ·»åŠ  refined åˆ—ã€‚")

            # æ£€æŸ¥å¹¶æ·»åŠ  used_for_learning åˆ—ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            await cursor.execute("PRAGMA table_info(filtered_messages)")
            columns = [col[1] for col in await cursor.fetchall()]
            if 'used_for_learning' not in columns:
                await cursor.execute("ALTER TABLE filtered_messages ADD COLUMN used_for_learning BOOLEAN DEFAULT 0")
                await conn.commit()
                logger.info("å·²ä¸º filtered_messages è¡¨æ·»åŠ  used_for_learning åˆ—ã€‚")

            # åˆ›å»ºå­¦ä¹ æ‰¹æ¬¡è¡¨
            await cursor.execute('''
                CREATE TABLE IF NOT EXISTS learning_batches (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    group_id TEXT NOT NULL,
                    start_time REAL NOT NULL,
                    end_time REAL,
                    quality_score REAL DEFAULT 0.5,
                    processed_messages INTEGER DEFAULT 0,
                    batch_name TEXT UNIQUE,
                    message_count INTEGER,
                    filtered_count INTEGER,
                    success BOOLEAN,
                    error_message TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')

            # åˆ›å»ºäººæ ¼æ›´æ–°è®°å½•è¡¨
            await cursor.execute('''
                CREATE TABLE IF NOT EXISTS persona_update_records (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp REAL NOT NULL,
                    group_id TEXT NOT NULL,
                    update_type TEXT NOT NULL,
                    original_content TEXT,
                    new_content TEXT NOT NULL,
                    reason TEXT,
                    status TEXT DEFAULT 'pending',
                    reviewer_comment TEXT,
                    review_time REAL
                )
            ''')

            # åˆ›å»ºç´¢å¼•ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼Œé¿å…åˆ—ä¸å­˜åœ¨å¯¼è‡´å¤±è´¥ï¼‰
            indices = [
                ('idx_raw_messages_timestamp', 'raw_messages', 'timestamp'),
                ('idx_raw_messages_sender', 'raw_messages', 'sender_id'),
                ('idx_raw_messages_processed', 'raw_messages', 'processed'),
                ('idx_filtered_messages_confidence', 'filtered_messages', 'confidence'),
                ('idx_filtered_messages_used', 'filtered_messages', 'used_for_learning'),
                ('idx_persona_update_records_status', 'persona_update_records', 'status'),
                ('idx_persona_update_records_group_id', 'persona_update_records', 'group_id'),
            ]

            for index_name, table_name, column_name in indices:
                try:
                    await cursor.execute(f'CREATE INDEX IF NOT EXISTS {index_name} ON {table_name}({column_name})')
                except Exception as e:
                    logger.debug(f"åˆ›å»ºç´¢å¼• {index_name} å¤±è´¥ï¼ˆå¯èƒ½åˆ—ä¸å­˜åœ¨ï¼‰: {e}")

            # æ–°å¢å¼ºåŒ–å­¦ä¹ ç›¸å…³è¡¨
            await cursor.execute('''
                CREATE TABLE IF NOT EXISTS reinforcement_learning_results (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    group_id TEXT NOT NULL,
                    timestamp REAL NOT NULL,
                    replay_analysis TEXT,
                    optimization_strategy TEXT,
                    reinforcement_feedback TEXT,
                    next_action TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            await cursor.execute('''
                CREATE TABLE IF NOT EXISTS persona_fusion_history (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    group_id TEXT NOT NULL,
                    timestamp REAL NOT NULL,
                    base_persona_hash INTEGER,
                    incremental_hash INTEGER,
                    fusion_result TEXT,
                    compatibility_score REAL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            await cursor.execute('''
                CREATE TABLE IF NOT EXISTS strategy_optimization_results (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    group_id TEXT NOT NULL,
                    timestamp REAL NOT NULL,
                    original_strategy TEXT,
                    optimization_result TEXT,
                    expected_improvement TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            await cursor.execute('''
                CREATE TABLE IF NOT EXISTS learning_performance_history (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    group_id TEXT NOT NULL,
                    session_id TEXT,
                    timestamp REAL NOT NULL,
                    quality_score REAL,
                    learning_time REAL,
                    success BOOLEAN,
                    successful_pattern TEXT,
                    failed_pattern TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # ä¸ºå¼ºåŒ–å­¦ä¹ è¡¨åˆ›å»ºç´¢å¼•
            await cursor.execute('CREATE INDEX IF NOT EXISTS idx_reinforcement_learning_group ON reinforcement_learning_results(group_id)')
            await cursor.execute('CREATE INDEX IF NOT EXISTS idx_persona_fusion_group ON persona_fusion_history(group_id)')
            await cursor.execute('CREATE INDEX IF NOT EXISTS idx_strategy_optimization_group ON strategy_optimization_results(group_id)')
            await cursor.execute('CREATE INDEX IF NOT EXISTS idx_learning_performance_group ON learning_performance_history(group_id)')
            
            # åˆ›å»ºLLMè°ƒç”¨ç»Ÿè®¡è¡¨
            await cursor.execute('''
                CREATE TABLE IF NOT EXISTS llm_call_statistics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    provider_type TEXT NOT NULL, -- filter, refine, reinforce
                    model_name TEXT,
                    total_calls INTEGER DEFAULT 0,
                    success_calls INTEGER DEFAULT 0,
                    failed_calls INTEGER DEFAULT 0,
                    total_response_time_ms INTEGER DEFAULT 0,
                    avg_response_time_ms REAL DEFAULT 0,
                    success_rate REAL DEFAULT 0,
                    last_call_time REAL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(provider_type, model_name)
                )
            ''')
            
            # é£æ ¼å­¦ä¹ è®°å½•è¡¨ (ä»ç¾¤ç»„æ•°æ®åº“ç§»è‡³æ¶ˆæ¯æ•°æ®åº“)
            await cursor.execute(''' 
                CREATE TABLE IF NOT EXISTS style_learning_records (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    group_id TEXT NOT NULL,
                    style_type TEXT NOT NULL,
                    learned_patterns TEXT, -- JSONæ ¼å¼å­˜å‚¨å­¦ä¹ åˆ°çš„æ¨¡å¼
                    confidence_score REAL,
                    sample_count INTEGER,
                    learning_time REAL NOT NULL,
                    last_updated REAL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # è¯­è¨€é£æ ¼æ¨¡å¼è¡¨ (ä»ç¾¤ç»„æ•°æ®åº“ç§»è‡³æ¶ˆæ¯æ•°æ®åº“)
            await cursor.execute(''' 
                CREATE TABLE IF NOT EXISTS language_style_patterns (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    group_id TEXT NOT NULL,
                    language_style TEXT NOT NULL,
                    example_phrases TEXT, -- JSONæ ¼å¼å­˜å‚¨ç¤ºä¾‹çŸ­è¯­
                    usage_frequency INTEGER DEFAULT 0,
                    context_type TEXT DEFAULT 'general',
                    confidence_score REAL,
                    last_updated REAL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # ä¸ºæ–°è¡¨åˆ›å»ºç´¢å¼•
            await cursor.execute('CREATE INDEX IF NOT EXISTS idx_style_learning_group ON style_learning_records(group_id)')
            await cursor.execute('CREATE INDEX IF NOT EXISTS idx_style_learning_time ON style_learning_records(learning_time)')
            await cursor.execute('CREATE INDEX IF NOT EXISTS idx_language_style_group ON language_style_patterns(group_id)')
            await cursor.execute('CREATE INDEX IF NOT EXISTS idx_language_style_frequency ON language_style_patterns(usage_frequency)')

            # åˆ›å»ºè¯é¢˜æ€»ç»“è¡¨
            await cursor.execute('''
                CREATE TABLE IF NOT EXISTS topic_summaries (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    group_id TEXT NOT NULL,
                    topic TEXT NOT NULL,
                    summary TEXT,
                    participants TEXT,  -- JSONæ ¼å¼å­˜å‚¨å‚ä¸è€…åˆ—è¡¨
                    message_count INTEGER DEFAULT 0,
                    start_timestamp REAL,
                    end_timestamp REAL,
                    generated_at REAL NOT NULL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')

            # ä¸ºè¯é¢˜æ€»ç»“è¡¨åˆ›å»ºç´¢å¼•
            await cursor.execute('CREATE INDEX IF NOT EXISTS idx_topic_summaries_group ON topic_summaries(group_id)')
            await cursor.execute('CREATE INDEX IF NOT EXISTS idx_topic_summaries_time ON topic_summaries(generated_at)')

            # åˆ›å»ºé»‘è¯å­¦ä¹ è¡¨
            await cursor.execute('''
                CREATE TABLE IF NOT EXISTS jargon (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    content TEXT NOT NULL,
                    raw_content TEXT DEFAULT '[]',
                    meaning TEXT,
                    is_jargon BOOLEAN,
                    count INTEGER DEFAULT 1,
                    last_inference_count INTEGER DEFAULT 0,
                    is_complete BOOLEAN DEFAULT 0,
                    is_global BOOLEAN DEFAULT 0,
                    chat_id TEXT NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(chat_id, content)
                )
            ''')

            # ä¸ºé»‘è¯è¡¨åˆ›å»ºç´¢å¼•
            await cursor.execute('CREATE INDEX IF NOT EXISTS idx_jargon_content ON jargon(content)')
            await cursor.execute('CREATE INDEX IF NOT EXISTS idx_jargon_chat_id ON jargon(chat_id)')
            await cursor.execute('CREATE INDEX IF NOT EXISTS idx_jargon_is_jargon ON jargon(is_jargon)')
            await cursor.execute('CREATE INDEX IF NOT EXISTS idx_jargon_count ON jargon(count)')
            await cursor.execute('CREATE INDEX IF NOT EXISTS idx_jargon_updated_at ON jargon(updated_at)')

            await conn.commit()
            logger.info("å…¨å±€æ¶ˆæ¯æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")

        except aiosqlite.Error as e:
            logger.error(f"å…¨å±€æ¶ˆæ¯æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
            # å°è¯•åˆ é™¤å¯èƒ½æŸåçš„æ•°æ®åº“æ–‡ä»¶ï¼Œä»¥ä¾¿ä¸‹æ¬¡å¯åŠ¨æ—¶é‡æ–°åˆ›å»º
            if os.path.exists(self.messages_db_path):
                self._logger.warning(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥ï¼Œå°è¯•åˆ é™¤æŸåçš„æ•°æ®åº“æ–‡ä»¶: {self.messages_db_path}")
                try:
                    os.remove(self.messages_db_path)
                except OSError as ose:
                    self._logger.error(f"åˆ é™¤æ•°æ®åº“æ–‡ä»¶å¤±è´¥: {ose}")
            raise DataStorageError(f"å…¨å±€æ¶ˆæ¯æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        """

    def get_group_db_path(self, group_id: str) -> str:
        """è·å–ç¾¤æ•°æ®åº“æ–‡ä»¶è·¯å¾„"""
        if not group_id:
            raise ValueError("group_id ä¸èƒ½ä¸ºç©º")
        if not self.group_data_dir:
            raise ValueError("group_data_dir æœªåˆå§‹åŒ–")
        return os.path.join(self.group_data_dir, f"{group_id}_ID.db")

    async def get_group_connection(self, group_id: str) -> aiosqlite.Connection:
        """è·å–ç¾¤æ•°æ®åº“è¿æ¥"""
        if group_id not in self.group_db_connections:
            db_path = self.get_group_db_path(group_id)
            
            # ç¡®ä¿æ•°æ®åº“ç›®å½•å­˜åœ¨
            db_dir = os.path.dirname(db_path)
            os.makedirs(db_dir, exist_ok=True)
            
            # æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶æƒé™
            if os.path.exists(db_path):
                try:
                    # å°è¯•ä¿®æ”¹æ–‡ä»¶æƒé™ä¸ºå¯å†™
                    import stat
                    os.chmod(db_path, stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IWGRP)
                except OSError as e:
                    logger.warning(f"æ— æ³•ä¿®æ”¹ç¾¤æ•°æ®åº“æ–‡ä»¶æƒé™: {e}")
            
            conn = await aiosqlite.connect(db_path)
            
            # è®¾ç½®è¿æ¥å‚æ•°ï¼Œç¡®ä¿æ•°æ®åº“å¯å†™
            await conn.execute('PRAGMA foreign_keys = ON')
            await conn.execute('PRAGMA journal_mode = WAL')  
            await conn.execute('PRAGMA synchronous = NORMAL')
            await conn.commit()
            
            await self._init_group_database(conn)
            self.group_db_connections[group_id] = conn
            logger.info(f"å·²åˆ›å»ºç¾¤ {group_id} çš„æ•°æ®åº“è¿æ¥")
        
        return self.group_db_connections[group_id]

    async def _init_group_database(self, conn: aiosqlite.Connection):
        """åˆå§‹åŒ–ç¾¤æ•°æ®åº“è¡¨ç»“æ„"""
        cursor = await conn.cursor()
        
        try:
            # è®¾ç½®æ•°æ®åº“ä¸ºWALæ¨¡å¼ï¼Œæé«˜å¹¶å‘æ€§èƒ½å¹¶é¿å…é”å®šé—®é¢˜
            await cursor.execute('PRAGMA journal_mode=WAL')
            await cursor.execute('PRAGMA synchronous=NORMAL')
            await cursor.execute('PRAGMA cache_size=10000')
            await cursor.execute('PRAGMA temp_store=memory')
            
            # åŸå§‹æ¶ˆæ¯è¡¨ (ç¾¤æ•°æ®åº“ä¸­ä¸å†å­˜å‚¨åŸå§‹æ¶ˆæ¯ï¼Œç”±å…¨å±€æ¶ˆæ¯æ•°æ®åº“ç»Ÿä¸€ç®¡ç†)
            # ç­›é€‰æ¶ˆæ¯è¡¨ (ç¾¤æ•°æ®åº“ä¸­ä¸å†å­˜å‚¨ç­›é€‰æ¶ˆæ¯ï¼Œç”±å…¨å±€æ¶ˆæ¯æ•°æ®åº“ç»Ÿä¸€ç®¡ç†)
            
            # ç”¨æˆ·ç”»åƒè¡¨
            await cursor.execute(''' 
                CREATE TABLE IF NOT EXISTS user_profiles (
                    qq_id TEXT PRIMARY KEY,
                    qq_name TEXT,
                    nicknames TEXT, -- JSONæ ¼å¼å­˜å‚¨
                    activity_pattern TEXT, -- JSONæ ¼å¼å­˜å‚¨æ´»åŠ¨æ¨¡å¼
                    communication_style TEXT, -- JSONæ ¼å¼å­˜å‚¨æ²Ÿé€šé£æ ¼
                    topic_preferences TEXT, -- JSONæ ¼å¼å­˜å‚¨è¯é¢˜åå¥½
                    emotional_tendency TEXT, -- JSONæ ¼å¼å­˜å‚¨æƒ…æ„Ÿå€¾å‘
                    last_active REAL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # ç¤¾äº¤å…³ç³»è¡¨
            await cursor.execute(''' 
                CREATE TABLE IF NOT EXISTS social_relations (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    from_user TEXT NOT NULL,
                    to_user TEXT NOT NULL,
                    relation_type TEXT NOT NULL, -- mention, reply, frequent_interaction
                    strength REAL NOT NULL,
                    frequency INTEGER NOT NULL,
                    last_interaction REAL NOT NULL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(from_user, to_user, relation_type)
                )
            ''')
            
            # é£æ ¼æ¡£æ¡ˆè¡¨
            await cursor.execute(''' 
                CREATE TABLE IF NOT EXISTS style_profiles (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    profile_name TEXT NOT NULL,
                    vocabulary_richness REAL,
                    sentence_complexity REAL,
                    emotional_expression REAL,
                    interaction_tendency REAL,
                    topic_diversity REAL,
                    formality_level REAL,
                    creativity_score REAL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # äººæ ¼å¤‡ä»½è¡¨
            await cursor.execute(''' 
                CREATE TABLE IF NOT EXISTS persona_backups (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    backup_name TEXT NOT NULL,
                    timestamp REAL NOT NULL,
                    reason TEXT,
                    persona_config TEXT, -- JSONæ ¼å¼å­˜å‚¨äººæ ¼é…ç½®
                    original_persona TEXT, -- JSONæ ¼å¼å­˜å‚¨
                    imitation_dialogues TEXT, -- JSONæ ¼å¼å­˜å‚¨æ¨¡ä»¿å¯¹è¯
                    backup_reason TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # é£æ ¼å­¦ä¹ è®°å½•è¡¨
            await cursor.execute(''' 
                CREATE TABLE IF NOT EXISTS style_learning_records (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    style_type TEXT NOT NULL,
                    learned_patterns TEXT, -- JSONæ ¼å¼å­˜å‚¨å­¦ä¹ åˆ°çš„æ¨¡å¼
                    confidence_score REAL,
                    sample_count INTEGER,
                    last_updated REAL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # æƒ…æ„Ÿè¡¨è¾¾æ¨¡å¼è¡¨
            await cursor.execute(''' 
                CREATE TABLE IF NOT EXISTS emotion_patterns (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    emotional_pattern TEXT NOT NULL,
                    confidence_score REAL,
                    frequency INTEGER DEFAULT 0,
                    context_type TEXT,
                    last_updated REAL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # è¯­è¨€é£æ ¼æ¨¡å¼è¡¨
            await cursor.execute(''' 
                CREATE TABLE IF NOT EXISTS language_style_patterns (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    language_style TEXT NOT NULL,
                    example_phrases TEXT, -- JSONæ ¼å¼å­˜å‚¨ç¤ºä¾‹çŸ­è¯­
                    usage_frequency INTEGER DEFAULT 0,
                    context_type TEXT DEFAULT 'general',
                    confidence_score REAL,
                    last_updated REAL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # ä¸»é¢˜åå¥½è¡¨
            await cursor.execute(''' 
                CREATE TABLE IF NOT EXISTS topic_preferences (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    topic_category TEXT NOT NULL,
                    interest_level REAL,
                    response_style TEXT,
                    sample_count INTEGER DEFAULT 0,
                    confidence_score REAL,
                    last_updated REAL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # äººæ ¼æ›´æ–°å®¡æŸ¥è¡¨
            await cursor.execute(''' 
                CREATE TABLE IF NOT EXISTS persona_update_reviews (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    update_type TEXT NOT NULL, -- style_update, persona_update, learning_update
                    original_content TEXT, -- åŸå§‹äººæ ¼å†…å®¹
                    proposed_content TEXT, -- å»ºè®®çš„æ–°å†…å®¹
                    confidence_score REAL,
                    reason TEXT, -- æ›´æ–°åŸå› 
                    sample_messages TEXT, -- JSONæ ¼å¼å­˜å‚¨è§¦å‘æ›´æ–°çš„ç¤ºä¾‹æ¶ˆæ¯
                    review_status TEXT DEFAULT 'pending', -- pending, approved, rejected
                    reviewer_comment TEXT,
                    created_at REAL,
                    reviewed_at REAL,
                    auto_score REAL, -- è‡ªåŠ¨è¯„åˆ†
                    manual_override BOOLEAN DEFAULT FALSE
                )
            ''')
            
            # å­¦ä¹ æ‰¹æ¬¡è¡¨ (å¦‚æœä¸å­˜åœ¨)
            await cursor.execute(''' 
                CREATE TABLE IF NOT EXISTS learning_batches (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    batch_name TEXT,
                    start_time REAL,
                    end_time REAL,
                    processed_messages INTEGER DEFAULT 0,
                    success BOOLEAN DEFAULT FALSE,
                    error_message TEXT,
                    learning_type TEXT, -- style_learning, persona_update, etc.
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # å­¦ä¹ ä¼šè¯è¡¨
            await cursor.execute(''' 
                CREATE TABLE IF NOT EXISTS learning_sessions (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    session_id TEXT UNIQUE NOT NULL,
                    start_time REAL NOT NULL,
                    end_time REAL,
                    messages_processed INTEGER DEFAULT 0,
                    filtered_messages INTEGER DEFAULT 0,
                    style_updates INTEGER DEFAULT 0,
                    quality_score REAL DEFAULT 0.0,
                    success BOOLEAN DEFAULT FALSE,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # åˆ›å»ºç´¢å¼•
            await cursor.execute('CREATE INDEX IF NOT EXISTS idx_social_relations_from_user ON social_relations(from_user)') 
            await cursor.execute('CREATE INDEX IF NOT EXISTS idx_social_relations_to_user ON social_relations(to_user)') 
            await cursor.execute('CREATE INDEX IF NOT EXISTS idx_user_profiles_active ON user_profiles(last_active)') 
            await cursor.execute('CREATE INDEX IF NOT EXISTS idx_style_profiles_name ON style_profiles(profile_name)')
            
            # åˆ›å»ºå¥½æ„Ÿåº¦è¡¨
            await cursor.execute(''' 
                CREATE TABLE IF NOT EXISTS user_affection (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_id TEXT NOT NULL,
                    group_id TEXT NOT NULL,
                    affection_level INTEGER DEFAULT 0,
                    last_interaction REAL NOT NULL,
                    last_updated REAL NOT NULL,
                    interaction_count INTEGER DEFAULT 0,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(user_id, group_id)
                )
            ''')
            
            # åˆ›å»ºbotæƒ…ç»ªè¡¨
            await cursor.execute('''
                CREATE TABLE IF NOT EXISTS bot_mood (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    group_id TEXT NOT NULL,
                    mood_type TEXT NOT NULL,
                    mood_intensity REAL DEFAULT 0.5,
                    mood_description TEXT,
                    start_time REAL NOT NULL,
                    end_time REAL,
                    is_active BOOLEAN DEFAULT TRUE,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # åˆ›å»ºå¥½æ„Ÿåº¦å˜åŒ–è®°å½•è¡¨
            await cursor.execute('''
                CREATE TABLE IF NOT EXISTS affection_history (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_id TEXT NOT NULL,
                    group_id TEXT NOT NULL,
                    change_amount INTEGER NOT NULL,
                    previous_level INTEGER NOT NULL,
                    new_level INTEGER NOT NULL,
                    change_reason TEXT,
                    bot_mood TEXT,
                    timestamp REAL NOT NULL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            await conn.commit() 
            logger.debug("ç¾¤æ•°æ®åº“è¡¨ç»“æ„åˆå§‹åŒ–å®Œæˆ") 
            
        except aiosqlite.Error as e: 
            logger.error(f"åˆå§‹åŒ–ç¾¤æ•°æ®åº“å¤±è´¥: {e}", exc_info=True) 
            raise DataStorageError(f"åˆå§‹åŒ–ç¾¤æ•°æ®åº“å¤±è´¥: {str(e)}")

    async def save_style_profile(self, group_id: str, profile_data: Dict[str, Any]):
        """ä¿å­˜é£æ ¼æ¡£æ¡ˆåˆ°æ•°æ®åº“"""
        conn = await self.get_group_connection(group_id)
        cursor = await conn.cursor()

        try:
            await cursor.execute('''
                INSERT OR REPLACE INTO style_profiles
                (profile_name, vocabulary_richness, sentence_complexity, emotional_expression,
                 interaction_tendency, topic_diversity, formality_level, creativity_score)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                profile_data['profile_name'],
                profile_data.get('vocabulary_richness'),
                profile_data.get('sentence_complexity'),
                profile_data.get('emotional_expression'),
                profile_data.get('interaction_tendency'),
                profile_data.get('topic_diversity'),
                profile_data.get('formality_level'),
                profile_data.get('creativity_score')
            ))
            await conn.commit()
            logger.debug(f"é£æ ¼æ¡£æ¡ˆ '{profile_data['profile_name']}' å·²ä¿å­˜åˆ°ç¾¤ {group_id} æ•°æ®åº“ã€‚")
        except aiosqlite.Error as e:
            logger.error(f"ä¿å­˜é£æ ¼æ¡£æ¡ˆå¤±è´¥: {e}", exc_info=True)
            raise DataStorageError(f"ä¿å­˜é£æ ¼æ¡£æ¡ˆå¤±è´¥: {str(e)}")

    async def load_style_profile(self, group_id: str, profile_name: str) -> Optional[Dict[str, Any]]:
        """ä»æ•°æ®åº“åŠ è½½é£æ ¼æ¡£æ¡ˆ"""
        conn = await self.get_group_connection(group_id)
        cursor = await conn.cursor()

        try:
            await cursor.execute('''
                SELECT profile_name, vocabulary_richness, sentence_complexity, emotional_expression,
                       interaction_tendency, topic_diversity, formality_level, creativity_score
                FROM style_profiles WHERE profile_name = ?
            ''', (profile_name,))
            row = await cursor.fetchone()
            if not row:
                return None
            return {
                'profile_name': row[0],
                'vocabulary_richness': row[1],
                'sentence_complexity': row[2],
                'emotional_expression': row[3],
                'interaction_tendency': row[4],
                'topic_diversity': row[5],
                'formality_level': row[6],
                'creativity_score': row[7]
            }
        except aiosqlite.Error as e:
            logger.error(f"åŠ è½½é£æ ¼æ¡£æ¡ˆå¤±è´¥: {e}", exc_info=True)
            return None

    async def save_user_profile(self, group_id: str, profile_data: Dict[str, Any]):
        """ä¿å­˜ç”¨æˆ·ç”»åƒåˆ°æ•°æ®åº“"""
        conn = await self.get_group_connection(group_id)
        cursor = await conn.cursor()
        
        try:
            await cursor.execute('''
                INSERT OR REPLACE INTO user_profiles 
                (qq_id, qq_name, nicknames, activity_pattern, communication_style, 
                 topic_preferences, emotional_tendency, last_active, updated_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                profile_data['qq_id'],
                profile_data.get('qq_name', ''),
                json.dumps(profile_data.get('nicknames', []), ensure_ascii=False),
                json.dumps(profile_data.get('activity_pattern', {}), ensure_ascii=False),
                json.dumps(profile_data.get('communication_style', {}), ensure_ascii=False),
                json.dumps(profile_data.get('topic_preferences', {}), ensure_ascii=False),
                json.dumps(profile_data.get('emotional_tendency', {}), ensure_ascii=False),
                profile_data.get('last_active', time.time()),  # ä½¿ç”¨profileä¸­çš„å€¼æˆ–å½“å‰æ—¶é—´
                datetime.now().isoformat()
            ))
            
            await conn.commit()
            
        except aiosqlite.Error as e:
            logger.error(f"ä¿å­˜ç”¨æˆ·ç”»åƒå¤±è´¥: {e}", exc_info=True)
            raise DataStorageError(f"ä¿å­˜ç”¨æˆ·ç”»åƒå¤±è´¥: {str(e)}")

    async def load_user_profile(self, group_id: str, qq_id: str) -> Optional[Dict[str, Any]]:
        """ä»æ•°æ®åº“åŠ è½½ç”¨æˆ·ç”»åƒ"""
        conn = await self.get_group_connection(group_id)
        cursor = await conn.cursor()
        
        try:
            await cursor.execute('''
                SELECT qq_id, qq_name, nicknames, activity_pattern, communication_style,
                       topic_preferences, emotional_tendency, last_active
                FROM user_profiles WHERE qq_id = ?
            ''', (qq_id,))
            
            row = await cursor.fetchone()
            if not row:
                return None
            
            return {
                'qq_id': row[0],
                'qq_name': row[1],
                'nicknames': json.loads(row[2]) if row[2] else [],
                'activity_pattern': json.loads(row[3]) if row[3] else {},
                'communication_style': json.loads(row[4]) if row[4] else {},
                'topic_preferences': json.loads(row[5]) if row[5] else {},
                'emotional_tendency': json.loads(row[6]) if row[6] else {},
                'last_active': row[7]
            }
            
        except aiosqlite.Error as e:
            logger.error(f"åŠ è½½ç”¨æˆ·ç”»åƒå¤±è´¥: {e}", exc_info=True)
            return None

    async def save_social_relation(self, group_id: str, relation_data: Dict[str, Any]):
        """ä¿å­˜ç¤¾äº¤å…³ç³»åˆ°æ•°æ®åº“"""
        conn = await self.get_group_connection(group_id)
        cursor = await conn.cursor()

        try:
            await cursor.execute('''
                INSERT OR REPLACE INTO social_relations
                (from_user, to_user, relation_type, strength, frequency, last_interaction, updated_at)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (
                relation_data['from_user'],
                relation_data['to_user'],
                relation_data['relation_type'],
                relation_data['strength'],
                relation_data['frequency'],
                relation_data['last_interaction'],
                datetime.now().isoformat()
            ))

            await conn.commit()

        except aiosqlite.Error as e:
            logger.error(f"ä¿å­˜ç¤¾äº¤å…³ç³»å¤±è´¥: {e}", exc_info=True)
            raise DataStorageError(f"ä¿å­˜ç¤¾äº¤å…³ç³»å¤±è´¥: {str(e)}")

    async def get_social_relations_by_group(self, group_id: str) -> List[Dict[str, Any]]:
        """è·å–æŒ‡å®šç¾¤ç»„çš„ç¤¾äº¤å…³ç³»"""
        conn = await self.get_group_connection(group_id)
        cursor = await conn.cursor()

        try:
            # æ·»åŠ  WHERE å­å¥æ¥è¿‡æ»¤ç‰¹å®šç¾¤ç»„çš„å…³ç³»
            # ç¤¾äº¤å…³ç³»ä¸­çš„ from_user å’Œ to_user æ ¼å¼ä¸º "group_id:user_id"
            await cursor.execute('''
                SELECT from_user, to_user, relation_type, strength, frequency, last_interaction
                FROM social_relations
                WHERE (from_user LIKE ? OR to_user LIKE ?)
                ORDER BY frequency DESC, strength DESC
            ''', (f'{group_id}:%', f'{group_id}:%'))

            rows = await cursor.fetchall()
            relations = []

            for row in rows:
                try:
                    # æ·»åŠ è¡Œæ•°æ®éªŒè¯
                    if len(row) < 6:
                        self._logger.warning(f"ç¤¾äº¤å…³ç³»æ•°æ®è¡Œä¸å®Œæ•´ (æœŸæœ›6ä¸ªå­—æ®µï¼Œå®é™…{len(row)}ä¸ª)ï¼Œè·³è¿‡: {row}")
                        continue

                    relations.append({
                        'from_user': row[0],
                        'to_user': row[1],
                        'relation_type': row[2],
                        'strength': float(row[3]) if row[3] else 0.0,
                        'frequency': int(row[4]) if row[4] else 0,
                        'last_interaction': row[5]
                    })
                except Exception as row_error:
                    self._logger.warning(f"å¤„ç†ç¤¾äº¤å…³ç³»æ•°æ®è¡Œæ—¶å‡ºé”™ï¼Œè·³è¿‡: {row_error}, row: {row}")

            self._logger.info(f"ç¾¤ç»„ {group_id} åŠ è½½äº† {len(relations)} æ¡ç¤¾äº¤å…³ç³»")
            return relations

        except aiosqlite.Error as e:
            logger.error(f"è·å–ç¤¾äº¤å…³ç³»å¤±è´¥: {e}", exc_info=True)
            return []

    async def get_user_social_relations(self, group_id: str, user_id: str) -> Dict[str, Any]:
        """
        è·å–æŒ‡å®šç”¨æˆ·åœ¨ç¾¤ç»„ä¸­çš„ç¤¾äº¤å…³ç³»

        Args:
            group_id: ç¾¤ç»„ID
            user_id: ç”¨æˆ·ID

        Returns:
            åŒ…å«ç”¨æˆ·ç¤¾äº¤å…³ç³»çš„å­—å…¸ï¼ŒåŒ…æ‹¬ï¼š
            - outgoing: è¯¥ç”¨æˆ·å‘èµ·çš„å…³ç³»åˆ—è¡¨
            - incoming: æŒ‡å‘è¯¥ç”¨æˆ·çš„å…³ç³»åˆ—è¡¨
            - total_relations: æ€»å…³ç³»æ•°
        """
        conn = await self.get_group_connection(group_id)
        cursor = await conn.cursor()

        try:
            user_key = f"{group_id}:{user_id}"

            # è·å–è¯¥ç”¨æˆ·å‘èµ·çš„å…³ç³»ï¼ˆoutgoingï¼‰
            await cursor.execute('''
                SELECT from_user, to_user, relation_type, strength, frequency, last_interaction
                FROM social_relations
                WHERE from_user = ? OR from_user = ?
                ORDER BY frequency DESC, strength DESC
                LIMIT 10
            ''', (user_key, user_id))

            outgoing_rows = await cursor.fetchall()
            outgoing_relations = []

            for row in outgoing_rows:
                outgoing_relations.append({
                    'from_user': row[0],
                    'to_user': row[1],
                    'relation_type': row[2],
                    'strength': row[3],
                    'frequency': row[4],
                    'last_interaction': row[5]
                })

            # è·å–æŒ‡å‘è¯¥ç”¨æˆ·çš„å…³ç³»ï¼ˆincomingï¼‰
            await cursor.execute('''
                SELECT from_user, to_user, relation_type, strength, frequency, last_interaction
                FROM social_relations
                WHERE to_user = ? OR to_user = ?
                ORDER BY frequency DESC, strength DESC
                LIMIT 10
            ''', (user_key, user_id))

            incoming_rows = await cursor.fetchall()
            incoming_relations = []

            for row in incoming_rows:
                incoming_relations.append({
                    'from_user': row[0],
                    'to_user': row[1],
                    'relation_type': row[2],
                    'strength': row[3],
                    'frequency': row[4],
                    'last_interaction': row[5]
                })

            return {
                'user_id': user_id,
                'group_id': group_id,
                'outgoing': outgoing_relations,
                'incoming': incoming_relations,
                'total_relations': len(outgoing_relations) + len(incoming_relations)
            }

        except aiosqlite.Error as e:
            logger.error(f"è·å–ç”¨æˆ·ç¤¾äº¤å…³ç³»å¤±è´¥: {e}", exc_info=True)
            return {
                'user_id': user_id,
                'group_id': group_id,
                'outgoing': [],
                'incoming': [],
                'total_relations': 0
            }


    async def save_raw_message(self, message_data) -> int:
        """
        å°†åŸå§‹æ¶ˆæ¯ä¿å­˜åˆ°å…¨å±€æ¶ˆæ¯æ•°æ®åº“ã€‚
        """
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()
            
            try:
                # æ£€æŸ¥message_dataæ˜¯å¦ä¸ºå­—å…¸æˆ–å¯¹è±¡
                if hasattr(message_data, 'sender_id'):
                    # å¦‚æœæ˜¯å¯¹è±¡ï¼Œç›´æ¥è®¿é—®å±æ€§
                    await cursor.execute('''
                        INSERT INTO raw_messages (sender_id, sender_name, message, group_id, platform, timestamp)
                        VALUES (?, ?, ?, ?, ?, ?)
                    ''', (
                        message_data.sender_id,
                        message_data.sender_name,
                        message_data.message,
                        message_data.group_id,
                        message_data.platform,
                        message_data.timestamp
                    ))
                else:
                    # å¦‚æœæ˜¯å­—å…¸ï¼Œä½¿ç”¨å­—å…¸è®¿é—®
                    await cursor.execute('''
                        INSERT INTO raw_messages (sender_id, sender_name, message, group_id, platform, timestamp)
                        VALUES (?, ?, ?, ?, ?, ?)
                    ''', (
                        message_data.get('sender_id'),
                        message_data.get('sender_name'),
                        message_data.get('message'),
                        message_data.get('group_id'),
                        message_data.get('platform'),
                        message_data.get('timestamp')
                    ))
                
                message_id = cursor.lastrowid
                await conn.commit()
                logger.info(f"ğŸ’¾ æ•°æ®åº“å†™å…¥æˆåŠŸ: ID={message_id}, timestamp={message_data.timestamp if hasattr(message_data, 'timestamp') else message_data.get('timestamp')}")
                return message_id
                
            except aiosqlite.Error as e:
                logger.error(f"ä¿å­˜åŸå§‹æ¶ˆæ¯å¤±è´¥: {e}", exc_info=True)
                raise DataStorageError(f"ä¿å­˜åŸå§‹æ¶ˆæ¯å¤±è´¥: {str(e)}")
            finally:
                await cursor.close()

    async def get_unprocessed_messages(self, limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        è·å–æœªå¤„ç†çš„åŸå§‹æ¶ˆæ¯

        Args:
            limit: é™åˆ¶è¿”å›çš„æ¶ˆæ¯æ•°é‡

        Returns:
            æœªå¤„ç†çš„æ¶ˆæ¯åˆ—è¡¨
        """
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()

            try:
                if limit:
                    await cursor.execute('''
                        SELECT id, sender_id, sender_name, message, group_id, platform, timestamp
                        FROM raw_messages
                        WHERE processed = FALSE
                        ORDER BY timestamp ASC
                        LIMIT ?
                    ''', (limit,))
                else:
                    await cursor.execute('''
                        SELECT id, sender_id, sender_name, message, group_id, platform, timestamp
                        FROM raw_messages
                        WHERE processed = FALSE
                        ORDER BY timestamp ASC
                    ''')

                messages = []
                for row in await cursor.fetchall():
                    messages.append({
                        'id': row[0],
                        'sender_id': row[1],
                        'sender_name': row[2],
                        'message': row[3],
                        'group_id': row[4],
                        'platform': row[5],
                        'timestamp': row[6]
                    })

                logger.debug(f"è·å–åˆ° {len(messages)} æ¡æœªå¤„ç†æ¶ˆæ¯")
                return messages

            except aiosqlite.Error as e:
                logger.error(f"è·å–æœªå¤„ç†æ¶ˆæ¯å¤±è´¥: {e}", exc_info=True)
                raise DataStorageError(f"è·å–æœªå¤„ç†æ¶ˆæ¯å¤±è´¥: {str(e)}")
            finally:
                await cursor.close()

    async def mark_messages_processed(self, message_ids: List[int]) -> bool:
        """
        æ ‡è®°æ¶ˆæ¯ä¸ºå·²å¤„ç†

        Args:
            message_ids: æ¶ˆæ¯IDåˆ—è¡¨

        Returns:
            æ˜¯å¦æˆåŠŸæ ‡è®°
        """
        if not message_ids:
            return True

        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()

            try:
                # æ‰¹é‡æ›´æ–°æ¶ˆæ¯çŠ¶æ€
                placeholders = ','.join(['?' for _ in message_ids])
                await cursor.execute(f'''
                    UPDATE raw_messages
                    SET processed = TRUE
                    WHERE id IN ({placeholders})
                ''', message_ids)

                await conn.commit()
                logger.debug(f"å·²æ ‡è®° {len(message_ids)} æ¡æ¶ˆæ¯ä¸ºå·²å¤„ç†")
                return True

            except aiosqlite.Error as e:
                logger.error(f"æ ‡è®°æ¶ˆæ¯å¤„ç†çŠ¶æ€å¤±è´¥: {e}", exc_info=True)
                raise DataStorageError(f"æ ‡è®°æ¶ˆæ¯å¤„ç†çŠ¶æ€å¤±è´¥: {str(e)}")
            finally:
                await cursor.close()

    async def add_filtered_message(self, filtered_data: Dict[str, Any]) -> int:
        """
        æ·»åŠ ç­›é€‰åçš„æ¶ˆæ¯
        
        Args:
            filtered_data: ç­›é€‰åçš„æ¶ˆæ¯æ•°æ®
            
        Returns:
            ç­›é€‰æ¶ˆæ¯çš„ID
        """
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()
            
            try:
                current_time = int(time.time())
                await cursor.execute('''
                    INSERT INTO filtered_messages
                    (raw_message_id, message, sender_id, confidence, filter_reason, timestamp, quality_scores, group_id, created_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    filtered_data.get('raw_message_id'),
                    filtered_data.get('message'),
                    filtered_data.get('sender_id'),
                    filtered_data.get('confidence', 0.8),
                    filtered_data.get('filter_reason', ''),
                    filtered_data.get('timestamp') or current_time,
                    json.dumps(filtered_data.get('quality_scores', {}), ensure_ascii=False),
                    filtered_data.get('group_id'),
                    current_time
                ))
                
                filtered_id = cursor.lastrowid
                await conn.commit()
                logger.debug(f"ç­›é€‰æ¶ˆæ¯å·²ä¿å­˜ï¼ŒID: {filtered_id}")
                return filtered_id
                
            except aiosqlite.Error as e:
                logger.error(f"æ·»åŠ ç­›é€‰æ¶ˆæ¯å¤±è´¥: {e}", exc_info=True)
                raise DataStorageError(f"æ·»åŠ ç­›é€‰æ¶ˆæ¯å¤±è´¥: {str(e)}")
            finally:
                await cursor.close()

    async def get_filtered_messages_for_learning(self, limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        è·å–ç”¨äºå­¦ä¹ çš„ç­›é€‰æ¶ˆæ¯
        
        Args:
            limit: é™åˆ¶è¿”å›çš„æ¶ˆæ¯æ•°é‡
            
        Returns:
            ç­›é€‰æ¶ˆæ¯åˆ—è¡¨
        """
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()
            
            try:
                if limit:
                    await cursor.execute('''
                        SELECT id, message, sender_id, confidence, quality_scores, timestamp, group_id
                        FROM filtered_messages 
                        WHERE used_for_learning = FALSE 
                        ORDER BY timestamp DESC 
                        LIMIT ?
                    ''', (limit,))
                else:
                    await cursor.execute('''
                        SELECT id, message, sender_id, confidence, quality_scores, timestamp, group_id
                        FROM filtered_messages 
                        WHERE used_for_learning = FALSE 
                        ORDER BY timestamp DESC
                    ''')
                
                messages = []
                for row in await cursor.fetchall():
                    try:
                        # æ·»åŠ è¡Œæ•°æ®éªŒè¯
                        if len(row) < 7:
                            self._logger.warning(f"ç­›é€‰æ¶ˆæ¯è¡Œæ•°æ®ä¸å®Œæ•´ (æœŸæœ›7ä¸ªå­—æ®µï¼Œå®é™…{len(row)}ä¸ª)ï¼Œè·³è¿‡: {row}")
                            continue

                        quality_scores = {}
                        try:
                            if row[4]:  # quality_scores
                                quality_scores = json.loads(row[4])
                        except (json.JSONDecodeError, TypeError):
                            pass

                        messages.append({
                            'id': row[0],
                            'message': row[1],
                            'sender_id': row[2],
                            'confidence': float(row[3]) if row[3] else 0.0,
                            'quality_scores': quality_scores,
                            'timestamp': float(row[5]) if row[5] else 0,
                            'group_id': row[6]
                        })
                    except Exception as row_error:
                        self._logger.warning(f"å¤„ç†ç­›é€‰æ¶ˆæ¯è¡Œæ—¶å‡ºé”™ï¼Œè·³è¿‡: {row_error}, row: {row if len(row) < 20 else 'too long'}")

                return messages
                
            except aiosqlite.Error as e:
                logger.error(f"è·å–å­¦ä¹ æ¶ˆæ¯å¤±è´¥: {e}", exc_info=True)
                raise DataStorageError(f"è·å–å­¦ä¹ æ¶ˆæ¯å¤±è´¥: {str(e)}")
            finally:
                await cursor.close()

    async def get_recent_filtered_messages(self, group_id: str, limit: int = 5) -> List[Dict[str, Any]]:
        """
        è·å–æŒ‡å®šç¾¤ç»„æœ€è¿‘çš„ç­›é€‰æ¶ˆæ¯
        
        Args:
            group_id: ç¾¤ç»„ID
            limit: æ¶ˆæ¯æ•°é‡é™åˆ¶
            
        Returns:
            ç­›é€‰æ¶ˆæ¯åˆ—è¡¨
        """
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()
            
            try:
                await cursor.execute('''
                    SELECT id, message, sender_id, confidence, quality_scores, timestamp
                    FROM filtered_messages 
                    WHERE group_id = ? 
                    ORDER BY timestamp DESC 
                    LIMIT ?
                ''', (group_id, limit))
                
                messages = []
                for row in await cursor.fetchall():
                    quality_scores = {}
                    try:
                        if row[4]:
                            quality_scores = json.loads(row[4])
                    except json.JSONDecodeError:
                        pass
                        
                    messages.append({
                        'id': row[0],
                        'message': row[1],
                        'sender_id': row[2],
                        'confidence': row[3],
                        'quality_scores': quality_scores,
                        'timestamp': row[5]
                    })
                    
                return messages
                
            except aiosqlite.Error as e:
                logger.error(f"è·å–æœ€è¿‘ç­›é€‰æ¶ˆæ¯å¤±è´¥: {e}", exc_info=True)
                return []
            finally:
                await cursor.close()

    async def get_recent_raw_messages(self, group_id: str, limit: int = 25) -> List[Dict[str, Any]]:
        """
        è·å–æŒ‡å®šç¾¤ç»„æœ€è¿‘çš„åŸå§‹æ¶ˆæ¯ï¼Œç”¨äºè¡¨è¾¾é£æ ¼å­¦ä¹ 
        
        Args:
            group_id: ç¾¤ç»„ID
            limit: æ¶ˆæ¯æ•°é‡é™åˆ¶
            
        Returns:
            åŸå§‹æ¶ˆæ¯åˆ—è¡¨
        """
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()
            
            try:
                await cursor.execute('''
                    SELECT id, sender_id, sender_name, message, group_id, platform, timestamp
                    FROM raw_messages 
                    WHERE group_id = ? 
                    ORDER BY timestamp DESC 
                    LIMIT ?
                ''', (group_id, limit))
                
                messages = []
                for row in await cursor.fetchall():
                    messages.append({
                        'id': row[0],
                        'sender_id': row[1],
                        'sender_name': row[2],
                        'message': row[3],
                        'group_id': row[4],
                        'platform': row[5],
                        'timestamp': row[6]
                    })
                    
                return messages
                
            except aiosqlite.Error as e:
                logger.error(f"è·å–æœ€è¿‘åŸå§‹æ¶ˆæ¯å¤±è´¥: {e}", exc_info=True)
                return []
            finally:
                await cursor.close()

    async def get_messages_statistics(self) -> Dict[str, Any]:
        """
        è·å–æ¶ˆæ¯ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()

            try:
                # è·å–åŸå§‹æ¶ˆæ¯ç»Ÿè®¡
                await cursor.execute('SELECT COUNT(*) FROM raw_messages')
                result = await cursor.fetchone()
                if not result or len(result) == 0:
                    total_messages = 0
                else:
                    total_messages = int(result[0]) if result[0] and str(result[0]).isdigit() else 0

                await cursor.execute('SELECT COUNT(*) FROM raw_messages WHERE processed = FALSE')
                result = await cursor.fetchone()
                unprocessed_messages = int(result[0]) if result and result[0] and str(result[0]).replace('-', '').isdigit() else 0

                # è·å–ç­›é€‰æ¶ˆæ¯ç»Ÿè®¡
                await cursor.execute('SELECT COUNT(*) FROM filtered_messages')
                result = await cursor.fetchone()
                filtered_messages = int(result[0]) if result and result[0] and str(result[0]).replace('-', '').isdigit() else 0

                await cursor.execute('SELECT COUNT(*) FROM filtered_messages WHERE used_for_learning = FALSE')
                result = await cursor.fetchone()
                unused_filtered_messages = int(result[0]) if result and result[0] and str(result[0]).replace('-', '').isdigit() else 0

                stats = {
                    'total_messages': total_messages,
                    'unprocessed_messages': unprocessed_messages,
                    'filtered_messages': filtered_messages,
                    'unused_filtered_messages': unused_filtered_messages,
                    'raw_messages': total_messages  # å…¼å®¹æ—§æ¥å£
                }

                # éªŒè¯è¿”å›çš„ç»Ÿè®¡æ•°æ®æ²¡æœ‰è¡¨å
                for key, value in stats.items():
                    if isinstance(value, str) and not value.replace('-', '').isdigit():
                        self._logger.error(f"get_messages_statistics è¿”å›äº†éæ•°å­—å­—ç¬¦ä¸²: {key}={value}ï¼Œè®¾ç½®ä¸º0")
                        stats[key] = 0

                return stats

            except aiosqlite.Error as e:
                self._logger.error(f"è·å–æ¶ˆæ¯ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
                return {
                    'total_messages': 0,
                    'unprocessed_messages': 0,
                    'filtered_messages': 0,
                    'unused_filtered_messages': 0,
                    'raw_messages': 0
                }
            finally:
                await cursor.close()

    async def get_pending_style_reviews(self, limit: int = 50) -> List[Dict[str, Any]]:
        """è·å–å¾…å®¡æŸ¥çš„é£æ ¼å­¦ä¹ è®°å½•"""
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()
            
            try:
                # ç¡®ä¿è¡¨å­˜åœ¨
                await self._ensure_style_review_table_exists(cursor)
                
                await cursor.execute('''
                    SELECT id, type, group_id, timestamp, learned_patterns, few_shots_content, 
                           status, description, created_at
                    FROM style_learning_reviews
                    WHERE status = 'pending'
                    ORDER BY timestamp DESC
                    LIMIT ?
                ''', (limit,))
                
                reviews = []
                for row in await cursor.fetchall():
                    learned_patterns = []
                    try:
                        if row[4]:  # learned_patterns
                            learned_patterns = json.loads(row[4])
                    except json.JSONDecodeError:
                        pass
                        
                    reviews.append({
                        'id': row[0],
                        'type': row[1],
                        'group_id': row[2],
                        'timestamp': row[3],
                        'learned_patterns': learned_patterns,
                        'few_shots_content': row[5],
                        'status': row[6],
                        'description': row[7],
                        'created_at': row[8]
                    })
                
                return reviews
                
            except Exception as e:
                self._logger.error(f"è·å–å¾…å®¡æŸ¥é£æ ¼å­¦ä¹ è®°å½•å¤±è´¥: {e}")
                return []
            finally:
                await cursor.close()

    async def get_reviewed_style_learning_updates(self, limit: int = 50, offset: int = 0, status_filter: str = None) -> List[Dict[str, Any]]:
        """è·å–å·²å®¡æŸ¥çš„é£æ ¼å­¦ä¹ è®°å½•"""
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()
            
            try:
                # ç¡®ä¿è¡¨å­˜åœ¨
                await self._ensure_style_review_table_exists(cursor)
                
                # æ„å»ºæŸ¥è¯¢æ¡ä»¶
                where_clause = "WHERE status != 'pending'"
                params = []
                
                if status_filter:
                    where_clause += " AND status = ?"
                    params.append(status_filter)
                
                params.extend([limit, offset])
                
                await cursor.execute(f'''
                    SELECT id, type, group_id, timestamp, learned_patterns, few_shots_content, 
                           status, description, created_at, updated_at
                    FROM style_learning_reviews
                    {where_clause}
                    ORDER BY updated_at DESC
                    LIMIT ? OFFSET ?
                ''', params)
                
                reviews = []
                for row in await cursor.fetchall():
                    learned_patterns = []
                    try:
                        if row[4]:  # learned_patterns
                            learned_patterns = json.loads(row[4])
                    except json.JSONDecodeError:
                        pass
                        
                    reviews.append({
                        'id': row[0],
                        'type': row[1],
                        'group_id': row[2],
                        'timestamp': row[3],
                        'learned_patterns': learned_patterns,
                        'few_shots_content': row[5],
                        'status': row[6],
                        'description': row[7],
                        'created_at': row[8],
                        'review_time': row[9] if len(row) > 9 else None
                    })
                
                return reviews
                
            except Exception as e:
                self._logger.error(f"è·å–å·²å®¡æŸ¥é£æ ¼å­¦ä¹ è®°å½•å¤±è´¥: {e}")
                return []
            finally:
                await cursor.close()

    async def get_detailed_metrics(self) -> Dict[str, Any]:
        """è·å–è¯¦ç»†ç›‘æ§æ•°æ®"""
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()
            
            try:
                detailed_data = {
                    'api_metrics': {
                        'hours': list(range(24)),
                        'response_times': [100 + i * 10 for i in range(24)]
                    },
                    'database_metrics': {
                        'table_stats': {}
                    },
                    'system_metrics': {
                        'memory_percent': 45.2,
                        'cpu_percent': 23.1,
                        'disk_percent': 67.8
                    },
                    'connection_pool_stats': {
                        'total_connections': self.connection_pool.total_connections,
                        'active_connections': self.connection_pool.active_connections,
                        'max_connections': self.connection_pool.max_connections,
                        'pool_usage': round(self.connection_pool.active_connections / self.connection_pool.max_connections * 100, 1) if self.connection_pool.max_connections > 0 else 0
                    }
                }
                
                # è·å–æ•°æ®åº“è¡¨ç»Ÿè®¡
                try:
                    tables = ['raw_messages', 'filtered_messages', 'expression_patterns']
                    for table in tables:
                        try:
                            await cursor.execute(f'SELECT COUNT(*) FROM {table}')
                            count = (await cursor.fetchone())[0]
                            detailed_data['database_metrics']['table_stats'][table] = {'count': count}
                        except:
                            detailed_data['database_metrics']['table_stats'][table] = {'count': 0}
                            
                except Exception as e:
                    self._logger.warning(f"è·å–æ•°æ®åº“è¡¨ç»Ÿè®¡å¤±è´¥: {e}")
                
                return detailed_data
                
            except Exception as e:
                self._logger.error(f"è·å–è¯¦ç»†ç›‘æ§æ•°æ®å¤±è´¥: {e}")
                return {
                    'api_metrics': {'hours': [], 'response_times': []},
                    'database_metrics': {'table_stats': {}},
                    'system_metrics': {'memory_percent': 0, 'cpu_percent': 0, 'disk_percent': 0},
                    'connection_pool_stats': {'total_connections': 0, 'active_connections': 0, 'max_connections': 0, 'pool_usage': 0}
                }
            finally:
                await cursor.close()

    async def get_message_statistics(self, group_id: str = None) -> Dict[str, Any]:
        """è·å–æ¶ˆæ¯ç»Ÿè®¡ä¿¡æ¯ï¼Œå…¼å®¹ webui.py çš„è°ƒç”¨"""
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()
            
            try:
                if group_id:
                    # è·å–ç‰¹å®šç¾¤ç»„çš„ç»Ÿè®¡
                    await cursor.execute('SELECT COUNT(*) FROM raw_messages WHERE group_id = ?', (group_id,))
                    total_messages = (await cursor.fetchone())[0]
                    
                    await cursor.execute('SELECT COUNT(*) FROM raw_messages WHERE group_id = ? AND processed = FALSE', (group_id,))
                    unprocessed_messages = (await cursor.fetchone())[0]
                    
                    await cursor.execute('SELECT COUNT(*) FROM filtered_messages WHERE group_id = ?', (group_id,))
                    filtered_messages = (await cursor.fetchone())[0]
                    
                    await cursor.execute('SELECT COUNT(*) FROM filtered_messages WHERE group_id = ? AND used_for_learning = FALSE', (group_id,))
                    unused_filtered_messages = (await cursor.fetchone())[0]
                else:
                    # è·å–å…¨å±€ç»Ÿè®¡
                    return await self.get_messages_statistics()
                
                return {
                    'total_messages': total_messages,
                    'unprocessed_messages': unprocessed_messages,
                    'filtered_messages': filtered_messages,
                    'unused_filtered_messages': unused_filtered_messages,
                    'raw_messages': total_messages,
                    'group_id': group_id
                }
                
            except aiosqlite.Error as e:
                self._logger.error(f"è·å–æ¶ˆæ¯ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
                return {
                    'total_messages': 0,
                    'unprocessed_messages': 0,
                    'filtered_messages': 0,
                    'unused_filtered_messages': 0,
                    'raw_messages': 0,
                    'group_id': group_id
                }
            finally:
                await cursor.close()

    async def get_recent_learning_batches(self, limit: int = 10) -> List[Dict[str, Any]]:
        """è·å–æœ€è¿‘çš„å­¦ä¹ æ‰¹æ¬¡è®°å½•"""
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()
            
            try:
                # ç¡®ä¿è¡¨å­˜åœ¨
                await cursor.execute('''
                    CREATE TABLE IF NOT EXISTS learning_batches (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        group_id TEXT NOT NULL,
                        batch_name TEXT NOT NULL,
                        start_time REAL NOT NULL,
                        end_time REAL,
                        quality_score REAL,
                        processed_messages INTEGER DEFAULT 0,
                        message_count INTEGER DEFAULT 0,
                        filtered_count INTEGER DEFAULT 0,
                        success BOOLEAN DEFAULT FALSE,
                        error_message TEXT
                    )
                ''')
                
                await cursor.execute('''
                    SELECT group_id, batch_name, start_time, end_time, quality_score,
                           processed_messages, message_count, filtered_count, success, error_message
                    FROM learning_batches 
                    ORDER BY start_time DESC 
                    LIMIT ?
                ''', (limit,))
                
                batches = []
                for row in await cursor.fetchall():
                    batches.append({
                        'group_id': row[0],
                        'batch_name': row[1],
                        'start_time': row[2],
                        'end_time': row[3],
                        'quality_score': row[4] or 0,
                        'processed_messages': row[5] or 0,
                        'message_count': row[6] or 0,
                        'filtered_count': row[7] or 0,
                        'success': bool(row[8]),
                        'error_message': row[9]
                    })
                
                return batches
                
            except Exception as e:
                self._logger.error(f"è·å–æœ€è¿‘å­¦ä¹ æ‰¹æ¬¡å¤±è´¥: {e}")
                return []
            finally:
                await cursor.close()

    async def get_style_progress_data(self) -> List[Dict[str, Any]]:
        """è·å–é£æ ¼è¿›åº¦æ•°æ®"""
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()

            try:
                # é¦–å…ˆæ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
                await cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='learning_batches'")
                if not await cursor.fetchone():
                    self._logger.info("learning_batches è¡¨ä¸å­˜åœ¨ï¼Œè¿”å›ç©ºåˆ—è¡¨")
                    return []

                # ä»å­¦ä¹ æ‰¹æ¬¡ä¸­è·å–è¿›åº¦æ•°æ®ï¼ŒåŒ…å«æ¶ˆæ¯æ•°é‡ä¿¡æ¯
                # âœ… åªæ˜¾ç¤ºæœ‰å®é™…æ¶ˆæ¯çš„è®°å½•ï¼ˆè¿‡æ»¤æ—§çš„ç©ºæ•°æ®ï¼‰
                await cursor.execute('''
                    SELECT group_id, start_time, quality_score, success,
                           processed_messages, filtered_count, batch_name
                    FROM learning_batches
                    WHERE quality_score IS NOT NULL
                      AND processed_messages > 0
                    ORDER BY start_time DESC
                    LIMIT 30
                ''')

                progress_data = []
                rows = await cursor.fetchall()

                self._logger.debug(f"get_style_progress_data è·å–åˆ° {len(rows)} è¡Œæ•°æ®")
                if rows and len(rows) > 0:
                    self._logger.debug(f"ç¬¬ä¸€è¡Œæ•°æ®: {rows[0]}, åˆ—æ•°: {len(rows[0])}")

                for row in rows:
                    try:
                        # æ·»åŠ è¡Œæ•°æ®éªŒè¯ï¼ˆç°åœ¨æœ‰7ä¸ªå­—æ®µï¼‰
                        if len(row) < 4:
                            self._logger.warning(f"å­¦ä¹ æ‰¹æ¬¡è¿›åº¦æ•°æ®è¡Œä¸å®Œæ•´ (æœŸæœ›è‡³å°‘4ä¸ªå­—æ®µï¼Œå®é™…{len(row)}ä¸ª)ï¼Œè·³è¿‡: {row}")
                            continue

                        progress_item = {
                            'group_id': row[0],
                            'timestamp': float(row[1]) if row[1] else 0,
                            'quality_score': float(row[2]) if row[2] else 0,
                            'success': bool(row[3])
                        }

                        # æ·»åŠ æ¶ˆæ¯æ•°é‡ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                        if len(row) > 4:
                            progress_item['processed_messages'] = int(row[4]) if row[4] else 0
                        if len(row) > 5:
                            progress_item['filtered_count'] = int(row[5]) if row[5] else 0
                        if len(row) > 6:
                            progress_item['batch_name'] = row[6] if row[6] else 'æœªå‘½å'

                        progress_data.append(progress_item)
                    except Exception as row_error:
                        self._logger.warning(f"å¤„ç†å­¦ä¹ æ‰¹æ¬¡è¿›åº¦æ•°æ®è¡Œæ—¶å‡ºé”™ï¼Œè·³è¿‡: {row_error}, row: {row}")

                return progress_data

            except Exception as e:
                self._logger.warning(f"ä»learning_batchesè¡¨è·å–è¿›åº¦æ•°æ®å¤±è´¥: {e}")
                return []
            finally:
                await cursor.close()

    async def get_style_learning_statistics(self) -> Dict[str, Any]:
        """è·å–é£æ ¼å­¦ä¹ ç»Ÿè®¡æ•°æ®"""
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()
            
            try:
                stats = {
                    'unique_styles': 0,
                    'avg_confidence': 0,
                    'total_samples': 0,
                    'latest_update': None
                }
                
                # ä»è¡¨è¾¾æ¨¡å¼è¡¨è·å–ç»Ÿè®¡
                try:
                    await cursor.execute('SELECT COUNT(*) FROM expression_patterns')
                    stats['total_samples'] = (await cursor.fetchone())[0] or 0

                    await cursor.execute('SELECT AVG(weight), MAX(create_time) FROM expression_patterns')
                    row = await cursor.fetchone()
                    if row[0]:
                        stats['avg_confidence'] = round((row[0] or 0) * 100, 1)
                    
                    if row[1]:
                        stats['latest_update'] = datetime.fromtimestamp(row[1]).strftime('%Y-%m-%d %H:%M')
                    
                    # è®¡ç®—ç‹¬ç‰¹é£æ ¼æ•°é‡ï¼ˆåŸºäºç¾¤ç»„ï¼‰
                    await cursor.execute('SELECT COUNT(DISTINCT group_id) FROM expression_patterns')
                    stats['unique_styles'] = (await cursor.fetchone())[0] or 0
                    
                except Exception as e:
                    self._logger.warning(f"ä»expression_patternsè¡¨è·å–ç»Ÿè®¡å¤±è´¥: {e}")
                
                return stats
                
            except Exception as e:
                self._logger.error(f"è·å–é£æ ¼å­¦ä¹ ç»Ÿè®¡å¤±è´¥: {e}")
                return {
                    'unique_styles': 0,
                    'avg_confidence': 0,
                    'total_samples': 0,
                    'latest_update': None
                }
            finally:
                await cursor.close()

    async def get_group_messages_statistics(self, group_id: str) -> Dict[str, Any]:
        """
        è·å–æŒ‡å®šç¾¤ç»„çš„æ¶ˆæ¯ç»Ÿè®¡ä¿¡æ¯

        Args:
            group_id: ç¾¤ç»„ID

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()

            try:
                # è·å–åŸå§‹æ¶ˆæ¯ç»Ÿè®¡
                await cursor.execute('SELECT COUNT(*) FROM raw_messages WHERE group_id = ?', (group_id,))
                result = await cursor.fetchone()
                total_messages = int(result[0]) if result and result[0] and str(result[0]).replace('-', '').isdigit() else 0

                await cursor.execute('SELECT COUNT(*) FROM raw_messages WHERE group_id = ? AND processed = FALSE', (group_id,))
                result = await cursor.fetchone()
                unprocessed_messages = int(result[0]) if result and result[0] and str(result[0]).replace('-', '').isdigit() else 0

                # è·å–ç­›é€‰æ¶ˆæ¯ç»Ÿè®¡
                await cursor.execute('SELECT COUNT(*) FROM filtered_messages WHERE group_id = ?', (group_id,))
                result = await cursor.fetchone()
                filtered_messages = int(result[0]) if result and result[0] and str(result[0]).replace('-', '').isdigit() else 0

                await cursor.execute('SELECT COUNT(*) FROM filtered_messages WHERE group_id = ? AND used_for_learning = FALSE', (group_id,))
                result = await cursor.fetchone()
                unused_filtered_messages = int(result[0]) if result and result[0] and str(result[0]).replace('-', '').isdigit() else 0

                stats = {
                    'total_messages': total_messages,
                    'unprocessed_messages': unprocessed_messages,
                    'filtered_messages': filtered_messages,
                    'unused_filtered_messages': unused_filtered_messages,
                    'raw_messages': total_messages  # å…¼å®¹æ—§æ¥å£
                }

                # éªŒè¯è¿”å›çš„ç»Ÿè®¡æ•°æ®æ²¡æœ‰è¡¨å
                for key, value in stats.items():
                    if isinstance(value, str) and not value.replace('-', '').isdigit():
                        self._logger.error(f"get_group_messages_statistics è¿”å›äº†éæ•°å­—å­—ç¬¦ä¸²: {key}={value}ï¼Œè®¾ç½®ä¸º0")
                        stats[key] = 0

                return stats

            except aiosqlite.Error as e:
                logger.error(f"è·å–ç¾¤ç»„æ¶ˆæ¯ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
                return {
                    'total_messages': 0,
                    'unprocessed_messages': 0,
                    'filtered_messages': 0,
                    'unused_filtered_messages': 0,
                    'raw_messages': 0
                }
            finally:
                await cursor.close()

    async def load_social_graph(self, group_id: str) -> List[Dict[str, Any]]:
        """åŠ è½½å®Œæ•´ç¤¾äº¤å›¾è°±"""
        self._logger.debug(f"[æ•°æ®åº“] å¼€å§‹åŠ è½½ç¾¤ç»„ {group_id} çš„ç¤¾äº¤å›¾è°±")
        conn = await self.get_group_connection(group_id)
        cursor = await conn.cursor()

        try:
            await cursor.execute('''
                SELECT from_user, to_user, relation_type, strength, frequency, last_interaction
                FROM social_relations ORDER BY strength DESC
            ''')

            relations = []
            for row in await cursor.fetchall():
                relations.append({
                    'from_user': row[0],
                    'to_user': row[1],
                    'relation_type': row[2],
                    'strength': row[3],
                    'frequency': row[4],
                    'last_interaction': row[5]
                })

            self._logger.info(f"[æ•°æ®åº“] æˆåŠŸåŠ è½½ç¾¤ç»„ {group_id} çš„ç¤¾äº¤å›¾è°±: {len(relations)} æ¡å…³ç³»è®°å½•")
            if len(relations) == 0:
                self._logger.warning(f"[æ•°æ®åº“] è­¦å‘Š: ç¾¤ç»„ {group_id} çš„social_relationsè¡¨ä¸­æ²¡æœ‰æ•°æ®!")
            else:
                # è¾“å‡ºå‰3æ¡ç¤ºä¾‹
                self._logger.debug(f"[æ•°æ®åº“] ç¤¾äº¤å…³ç³»ç¤ºä¾‹: {relations[:3]}")

            return relations

        except aiosqlite.Error as e:
            self._logger.error(f"[æ•°æ®åº“] åŠ è½½ç¤¾äº¤å›¾è°±å¤±è´¥ (ç¾¤ç»„: {group_id}): {e}", exc_info=True)
            return []

    async def get_messages_for_replay(self, group_id: str, days: int, limit: int) -> List[Dict[str, Any]]:
        """
        ä»å…¨å±€æ¶ˆæ¯æ•°æ®åº“è·å–æŒ‡å®šç¾¤ç»„åœ¨è¿‡å»ä¸€æ®µæ—¶é—´å†…çš„åŸå§‹æ¶ˆæ¯ï¼Œç”¨äºè®°å¿†é‡æ”¾ã€‚
        """
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()
            
            try:
                start_timestamp = time.time() - (days * 86400) # è½¬æ¢ä¸ºç§’
                
                await cursor.execute('''
                    SELECT id, sender_id, sender_name, message, group_id, platform, timestamp
                    FROM raw_messages 
                    WHERE group_id = ? AND timestamp > ?
                    ORDER BY timestamp DESC 
                    LIMIT ?
                ''', (group_id, start_timestamp, limit))
                
                messages = []
                for row in await cursor.fetchall():
                    messages.append({
                        'id': row[0],
                        'sender_id': row[1],
                        'sender_name': row[2],
                        'message': row[3],
                        'group_id': row[4],
                        'platform': row[5],
                        'timestamp': row[6]
                    })
                
                return messages
                
            except aiosqlite.Error as e:
                self._logger.error(f"è·å–è®°å¿†é‡æ”¾æ¶ˆæ¯å¤±è´¥: {e}", exc_info=True)
                return []
            finally:
                await cursor.close()

    async def backup_persona(self, group_id: str, backup_data: Dict[str, Any]) -> int:
        """å¤‡ä»½äººæ ¼æ•°æ®"""
        conn = await self.get_group_connection(group_id)
        cursor = await conn.cursor()
        
        try:
            # è·å–å½“å‰æ—¶é—´æˆ³
            current_timestamp = time.time()
            
            await cursor.execute(''' 
                INSERT INTO persona_backups (backup_name, timestamp, original_persona, imitation_dialogues, backup_reason)
                VALUES (?, ?, ?, ?, ?)
            ''', (
                backup_data['backup_name'],
                current_timestamp,
                json.dumps(backup_data['original_persona'], ensure_ascii=False),
                json.dumps(backup_data.get('imitation_dialogues', []), ensure_ascii=False),
                backup_data.get('backup_reason', 'Auto backup before update')
            ))
            
            backup_id = cursor.lastrowid
            await conn.commit()
            
            logger.info(f"äººæ ¼æ•°æ®å·²å¤‡ä»½ï¼Œå¤‡ä»½ID: {backup_id}")
            return backup_id
            
        except aiosqlite.Error as e:
            logger.error(f"å¤‡ä»½äººæ ¼æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise DataStorageError(f"å¤‡ä»½äººæ ¼æ•°æ®å¤±è´¥: {str(e)}")

    async def get_persona_backups(self, group_id: str, limit: int = 10) -> List[Dict[str, Any]]:
        """è·å–æœ€è¿‘çš„äººæ ¼å¤‡ä»½"""
        conn = await self.get_group_connection(group_id)
        cursor = await conn.cursor()
        
        try:
            await cursor.execute('''
                SELECT id, backup_name, created_at FROM persona_backups 
                ORDER BY created_at DESC LIMIT ?
            ''', (limit,))
            
            backups = []
            for row in await cursor.fetchall():
                backups.append({
                    'id': row[0],
                    'backup_name': row[1],
                    'created_at': row[2]
                })
            
            return backups
            
        except aiosqlite.Error as e:
            logger.error(f"è·å–äººæ ¼å¤‡ä»½å¤±è´¥: {e}", exc_info=True)
            return []

    async def restore_persona(self, group_id: str, backup_id: int) -> Optional[Dict[str, Any]]:
        """ä»å¤‡ä»½æ¢å¤äººæ ¼æ•°æ®"""
        conn = await self.get_group_connection(group_id)
        cursor = await conn.cursor()
        
        try:
            await cursor.execute('''
                SELECT backup_name, original_persona, imitation_dialogues, backup_reason 
                FROM persona_backups WHERE id = ?
            ''', (backup_id,))
            
            row = await cursor.fetchone()
            if not row:
                return None
            
            return {
                'backup_name': row[0],
                'original_persona': json.loads(row[1]),
                'imitation_dialogues': json.loads(row[2]),
                'backup_reason': row[3]
            }
            
        except aiosqlite.Error as e:
            logger.error(f"æ¢å¤äººæ ¼æ•°æ®å¤±è´¥: {e}", exc_info=True)
            return None

    async def save_persona_update_record(self, record: Dict[str, Any]) -> int:
        """ä¿å­˜äººæ ¼æ›´æ–°è®°å½•åˆ°æ•°æ®åº“"""
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()
            
            try:
                await cursor.execute('''
                    INSERT INTO persona_update_records (timestamp, group_id, update_type, original_content, new_content, reason, status)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                ''', (
                    record.get('timestamp', time.time()),
                    record.get('group_id'),
                    record.get('update_type'),
                    record.get('original_content'),
                    record.get('new_content'),
                    record.get('reason'),
                    record.get('status', 'pending')
                ))
                
                record_id = cursor.lastrowid
                await conn.commit()
                logger.debug(f"äººæ ¼æ›´æ–°è®°å½•å·²ä¿å­˜ï¼ŒID: {record_id}")
                return record_id
                
            except aiosqlite.Error as e:
                logger.error(f"ä¿å­˜äººæ ¼æ›´æ–°è®°å½•å¤±è´¥: {e}", exc_info=True)
                raise DataStorageError(f"ä¿å­˜äººæ ¼æ›´æ–°è®°å½•å¤±è´¥: {str(e)}")
            finally:
                await cursor.close()

    async def get_pending_persona_update_records(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰å¾…å®¡æŸ¥çš„äººæ ¼æ›´æ–°è®°å½•"""
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()
            
            try:
                # é¦–å…ˆæ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨ä»¥åŠåŒ…å«ä»€ä¹ˆæ•°æ®
                await cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='persona_update_records'")
                if not await cursor.fetchone():
                    self._logger.info("persona_update_records è¡¨ä¸å­˜åœ¨")
                    return []
                
                # æ£€æŸ¥è¡¨ä¸­æ€»å…±æœ‰å¤šå°‘è®°å½•
                await cursor.execute('SELECT COUNT(*) FROM persona_update_records')
                total_count = (await cursor.fetchone())[0]
                self._logger.info(f"persona_update_records è¡¨ä¸­æ€»å…±æœ‰ {total_count} æ¡è®°å½•")
                
                # æ£€æŸ¥å„ç§çŠ¶æ€çš„è®°å½•æ•°é‡
                await cursor.execute('SELECT status, COUNT(*) FROM persona_update_records GROUP BY status')
                status_counts = await cursor.fetchall()
                self._logger.info(f"å„çŠ¶æ€è®°å½•æ•°é‡: {dict(status_counts)}")
                
                # ä¼˜å…ˆæŸ¥è¯¢pendingçŠ¶æ€çš„è®°å½•
                await cursor.execute('''
                    SELECT id, timestamp, group_id, update_type, original_content, new_content, reason, status, reviewer_comment, review_time
                    FROM persona_update_records
                    WHERE status = 'pending'
                    ORDER BY timestamp DESC
                ''')
                
                records = []
                pending_rows = await cursor.fetchall()
                self._logger.info(f"æ‰¾åˆ° {len(pending_rows)} æ¡pendingçŠ¶æ€çš„è®°å½•")
                
                for row in pending_rows:
                    records.append({
                        'id': row[0],
                        'timestamp': row[1],
                        'group_id': row[2],
                        'update_type': row[3],
                        'original_content': row[4],
                        'new_content': row[5],
                        'reason': row[6],
                        'status': row[7],
                        'reviewer_comment': row[8],
                        'review_time': row[9]
                    })
                
                # å¦‚æœæ²¡æœ‰pendingçŠ¶æ€çš„è®°å½•ï¼Œå°è¯•æŸ¥è¯¢æ‰€æœ‰è®°å½•ï¼ˆå¯èƒ½statuså­—æ®µä¸ºç©ºæˆ–å…¶ä»–å€¼ï¼‰
                if not records and total_count > 0:
                    self._logger.info("æ²¡æœ‰pendingçŠ¶æ€è®°å½•ï¼ŒæŸ¥è¯¢æ‰€æœ‰è®°å½•...")
                    await cursor.execute('''
                        SELECT id, timestamp, group_id, update_type, original_content, new_content, reason, 
                               COALESCE(status, 'pending') as status, reviewer_comment, review_time
                        FROM persona_update_records
                        WHERE status IS NULL OR status = '' OR status = 'pending'
                        ORDER BY timestamp DESC
                        LIMIT 50
                    ''')
                    
                    all_rows = await cursor.fetchall()
                    self._logger.info(f"æ‰¾åˆ° {len(all_rows)} æ¡å¯èƒ½çš„å¾…å®¡æŸ¥è®°å½•")
                    
                    for row in all_rows:
                        records.append({
                            'id': row[0],
                            'timestamp': row[1],
                            'group_id': row[2],
                            'update_type': row[3],
                            'original_content': row[4],
                            'new_content': row[5],
                            'reason': row[6],
                            'status': 'pending',  # å¼ºåˆ¶è®¾ç½®ä¸ºpending
                            'reviewer_comment': row[8],
                            'review_time': row[9]
                        })
                
                return records
                
            except aiosqlite.Error as e:
                logger.error(f"è·å–å¾…å®¡æŸ¥äººæ ¼æ›´æ–°è®°å½•å¤±è´¥: {e}", exc_info=True)
                return []
            finally:
                await cursor.close()

    async def update_persona_update_record_status(self, record_id: int, status: str, reviewer_comment: Optional[str] = None) -> bool:
        """æ›´æ–°äººæ ¼æ›´æ–°è®°å½•çš„çŠ¶æ€"""
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()
            
            try:
                review_time = time.time()
                await cursor.execute('''
                    UPDATE persona_update_records
                    SET status = ?, reviewer_comment = ?, review_time = ?
                    WHERE id = ?
                ''', (status, reviewer_comment, review_time, record_id))
                
                await conn.commit()
                logger.debug(f"äººæ ¼æ›´æ–°è®°å½• {record_id} çŠ¶æ€å·²æ›´æ–°ä¸º {status}")
                return cursor.rowcount > 0
                
            except aiosqlite.Error as e:
                logger.error(f"æ›´æ–°äººæ ¼æ›´æ–°è®°å½•çŠ¶æ€å¤±è´¥: {e}", exc_info=True)
                raise DataStorageError(f"æ›´æ–°äººæ ¼æ›´æ–°è®°å½•çŠ¶æ€å¤±è´¥: {str(e)}")
            finally:
                await cursor.close()

    async def delete_persona_update_record(self, record_id: int) -> bool:
        """åˆ é™¤äººæ ¼æ›´æ–°è®°å½•"""
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()
            
            try:
                await cursor.execute('''
                    DELETE FROM persona_update_records
                    WHERE id = ?
                ''', (record_id,))
                
                await conn.commit()
                logger.debug(f"äººæ ¼æ›´æ–°è®°å½• {record_id} å·²åˆ é™¤")
                return cursor.rowcount > 0
                
            except aiosqlite.Error as e:
                logger.error(f"åˆ é™¤äººæ ¼æ›´æ–°è®°å½•å¤±è´¥: {e}", exc_info=True)
                raise DataStorageError(f"åˆ é™¤äººæ ¼æ›´æ–°è®°å½•å¤±è´¥: {str(e)}")
            finally:
                await cursor.close()

    async def get_persona_update_record_by_id(self, record_id: int) -> Optional[Dict[str, Any]]:
        """æ ¹æ®IDè·å–äººæ ¼æ›´æ–°è®°å½•"""
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()

            try:
                await cursor.execute('''
                    SELECT id, timestamp, group_id, update_type, original_content, new_content, reason, status, reviewer_comment, review_time
                    FROM persona_update_records
                    WHERE id = ?
                ''', (record_id,))

                row = await cursor.fetchone()
                if row:
                    return {
                        'id': row[0],
                        'timestamp': row[1],
                        'group_id': row[2],
                        'update_type': row[3],
                        'original_content': row[4],
                        'new_content': row[5],
                        'reason': row[6],
                        'status': row[7],
                        'reviewer_comment': row[8],
                        'review_time': row[9]
                    }
                return None

            except aiosqlite.Error as e:
                logger.error(f"è·å–äººæ ¼æ›´æ–°è®°å½•å¤±è´¥: {e}", exc_info=True)
                return None
            finally:
                await cursor.close()

    # ========== é«˜çº§åŠŸèƒ½æ•°æ®åº“æ“ä½œæ–¹æ³• ==========

    async def save_emotion_profile(self, group_id: str, user_id: str, profile_data: Dict[str, Any]) -> bool:
        """ä¿å­˜æƒ…æ„Ÿæ¡£æ¡ˆ"""
        conn = await self.get_group_connection(group_id)
        cursor = await conn.cursor()
        
        try:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è¡¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
            await cursor.execute('''
                CREATE TABLE IF NOT EXISTS emotion_profiles (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_id TEXT NOT NULL,
                    group_id TEXT NOT NULL,
                    dominant_emotions TEXT, -- JSONæ ¼å¼
                    emotion_patterns TEXT, -- JSONæ ¼å¼
                    empathy_level REAL DEFAULT 0.5,
                    emotional_stability REAL DEFAULT 0.5,
                    last_updated REAL NOT NULL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(user_id, group_id)
                )
            ''')
            
            await cursor.execute('''
                INSERT OR REPLACE INTO emotion_profiles 
                (user_id, group_id, dominant_emotions, emotion_patterns, empathy_level, emotional_stability, last_updated)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (
                user_id,
                group_id,
                json.dumps(profile_data.get('dominant_emotions', {}), ensure_ascii=False),
                json.dumps(profile_data.get('emotion_patterns', {}), ensure_ascii=False),
                profile_data.get('empathy_level', 0.5),
                profile_data.get('emotional_stability', 0.5),
                profile_data.get('last_updated', time.time())
            ))
            
            await conn.commit()
            return True
            
        except Exception as e:
            self._logger.error(f"ä¿å­˜æƒ…æ„Ÿæ¡£æ¡ˆå¤±è´¥: {e}")
            return False

    async def load_emotion_profile(self, group_id: str, user_id: str) -> Optional[Dict[str, Any]]:
        """åŠ è½½æƒ…æ„Ÿæ¡£æ¡ˆ"""
        conn = await self.get_group_connection(group_id)
        cursor = await conn.cursor()
        
        try:
            await cursor.execute('''
                SELECT dominant_emotions, emotion_patterns, empathy_level, emotional_stability, last_updated
                FROM emotion_profiles WHERE user_id = ? AND group_id = ?
            ''', (user_id, group_id))
            
            row = await cursor.fetchone()
            if not row:
                return None
                
            return {
                'user_id': user_id,
                'group_id': group_id,
                'dominant_emotions': json.loads(row[0]) if row[0] else {},
                'emotion_patterns': json.loads(row[1]) if row[1] else {},
                'empathy_level': row[2],
                'emotional_stability': row[3],
                'last_updated': row[4]
            }
            
        except Exception as e:
            self._logger.error(f"åŠ è½½æƒ…æ„Ÿæ¡£æ¡ˆå¤±è´¥: {e}")
            return None

    async def save_knowledge_entity(self, group_id: str, entity_data: Dict[str, Any]) -> bool:
        """ä¿å­˜çŸ¥è¯†å®ä½“"""
        conn = await self.get_group_connection(group_id)
        cursor = await conn.cursor()
        
        try:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è¡¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
            await cursor.execute('''
                CREATE TABLE IF NOT EXISTS knowledge_entities (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    entity_id TEXT UNIQUE NOT NULL,
                    name TEXT NOT NULL,
                    entity_type TEXT NOT NULL,
                    attributes TEXT, -- JSONæ ¼å¼
                    relationships TEXT, -- JSONæ ¼å¼
                    confidence REAL DEFAULT 0.5,
                    source_messages TEXT, -- JSONæ ¼å¼
                    last_mentioned REAL NOT NULL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            await cursor.execute('''
                INSERT OR REPLACE INTO knowledge_entities 
                (entity_id, name, entity_type, attributes, relationships, confidence, source_messages, last_mentioned)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                entity_data.get('entity_id'),
                entity_data.get('name', ''),
                entity_data.get('entity_type', 'unknown'),
                json.dumps(entity_data.get('attributes', {}), ensure_ascii=False),
                json.dumps(entity_data.get('relationships', []), ensure_ascii=False),
                entity_data.get('confidence', 0.5),
                json.dumps(entity_data.get('source_messages', []), ensure_ascii=False),
                entity_data.get('last_mentioned', time.time())
            ))
            
            await conn.commit()
            return True
            
        except Exception as e:
            self._logger.error(f"ä¿å­˜çŸ¥è¯†å®ä½“å¤±è´¥: {e}")
            return False

    async def get_knowledge_entities(self, group_id: str, limit: int = 100) -> List[Dict[str, Any]]:
        """è·å–çŸ¥è¯†å®ä½“åˆ—è¡¨"""
        conn = await self.get_group_connection(group_id)
        cursor = await conn.cursor()
        
        try:
            await cursor.execute('''
                SELECT entity_id, name, entity_type, attributes, relationships, confidence, source_messages, last_mentioned
                FROM knowledge_entities 
                ORDER BY last_mentioned DESC
                LIMIT ?
            ''', (limit,))
            
            entities = []
            for row in await cursor.fetchall():
                entities.append({
                    'entity_id': row[0],
                    'name': row[1],
                    'entity_type': row[2],
                    'attributes': json.loads(row[3]) if row[3] else {},
                    'relationships': json.loads(row[4]) if row[4] else [],
                    'confidence': row[5],
                    'source_messages': json.loads(row[6]) if row[6] else [],
                    'last_mentioned': row[7]
                })
            
            return entities
            
        except Exception as e:
            self._logger.error(f"è·å–çŸ¥è¯†å®ä½“å¤±è´¥: {e}")
            return []

    # æ–°å¢å¼ºåŒ–å­¦ä¹ ç›¸å…³æ–¹æ³•
    async def save_reinforcement_learning_result(self, group_id: str, result_data: Dict[str, Any]) -> bool:
        """ä¿å­˜å¼ºåŒ–å­¦ä¹ ç»“æœ"""
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()
            
            try:
                await cursor.execute('''
                    INSERT INTO reinforcement_learning_results 
                    (group_id, timestamp, replay_analysis, optimization_strategy, reinforcement_feedback, next_action)
                    VALUES (?, ?, ?, ?, ?, ?)
                ''', (
                    group_id,
                    result_data.get('timestamp', time.time()),
                    json.dumps(result_data.get('replay_analysis', {}), ensure_ascii=False),
                    json.dumps(result_data.get('optimization_strategy', {}), ensure_ascii=False),
                    json.dumps(result_data.get('reinforcement_feedback', {}), ensure_ascii=False),
                    result_data.get('next_action', '')
                ))
                
                await conn.commit()
                return True
                
            except Exception as e:
                logger.error(f"ä¿å­˜å¼ºåŒ–å­¦ä¹ ç»“æœå¤±è´¥: {e}")
                return False
            finally:
                await cursor.close()

    async def get_learning_history_for_reinforcement(self, group_id: str, limit: int = 50) -> List[Dict[str, Any]]:
        """è·å–ç”¨äºå¼ºåŒ–å­¦ä¹ çš„å†å²æ•°æ®"""
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()
        
        try:
            await cursor.execute('''
                SELECT timestamp, quality_score, success, successful_pattern, failed_pattern
                FROM learning_performance_history 
                WHERE group_id = ?
                ORDER BY timestamp DESC
                LIMIT ?
            ''', (group_id, limit))
            
            history = []
            for row in await cursor.fetchall():
                history.append({
                    'timestamp': row[0],
                    'quality_score': row[1],
                    'success': bool(row[2]),
                    'successful_pattern': row[3] or '',
                    'failed_pattern': row[4] or ''
                })
            
            return history
            
        except Exception as e:
            logger.error(f"è·å–å¼ºåŒ–å­¦ä¹ å†å²æ•°æ®å¤±è´¥: {e}")
            return []
        finally:
            await cursor.close()

    async def save_persona_fusion_result(self, group_id: str, fusion_data: Dict[str, Any]) -> bool:
        """ä¿å­˜äººæ ¼èåˆç»“æœ"""
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()
        
        try:
            await cursor.execute('''
                INSERT INTO persona_fusion_history 
                (group_id, timestamp, base_persona_hash, incremental_hash, fusion_result, compatibility_score)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                group_id,
                fusion_data.get('timestamp', time.time()),
                fusion_data.get('base_persona_hash'),
                fusion_data.get('incremental_hash'),
                json.dumps(fusion_data.get('fusion_result', {}), ensure_ascii=False),
                fusion_data.get('compatibility_score', 0.0)
            ))
            
            await conn.commit()
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜äººæ ¼èåˆç»“æœå¤±è´¥: {e}")
            return False
        finally:
            await cursor.close()

    async def get_persona_fusion_history(self, group_id: str, limit: int = 10) -> List[Dict[str, Any]]:
        """è·å–äººæ ¼èåˆå†å²"""
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()
        
        try:
            await cursor.execute('''
                SELECT timestamp, base_persona_hash, incremental_hash, fusion_result, compatibility_score
                FROM persona_fusion_history 
                WHERE group_id = ?
                ORDER BY timestamp DESC
                LIMIT ?
            ''', (group_id, limit))
            
            history = []
            for row in await cursor.fetchall():
                fusion_result = {}
                try:
                    fusion_result = json.loads(row[3]) if row[3] else {}
                except json.JSONDecodeError:
                    logger.warning(f"è§£æèåˆç»“æœJSONå¤±è´¥: {row[3]}")
                
                history.append({
                    'timestamp': row[0],
                    'base_persona_hash': row[1],
                    'incremental_hash': row[2],
                    'fusion_result': fusion_result,
                    'compatibility_score': row[4]
                })
            
            return history
            
        except Exception as e:
            logger.error(f"è·å–äººæ ¼èåˆå†å²å¤±è´¥: {e}")
            return []
        finally:
            await cursor.close()

    async def save_strategy_optimization_result(self, group_id: str, optimization_data: Dict[str, Any]) -> bool:
        """ä¿å­˜ç­–ç•¥ä¼˜åŒ–ç»“æœ"""
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()
        
        try:
            await cursor.execute('''
                INSERT INTO strategy_optimization_results 
                (group_id, timestamp, original_strategy, optimization_result, expected_improvement)
                VALUES (?, ?, ?, ?, ?)
            ''', (
                group_id,
                optimization_data.get('timestamp', time.time()),
                json.dumps(optimization_data.get('original_strategy', {}), ensure_ascii=False),
                json.dumps(optimization_data.get('optimization_result', {}), ensure_ascii=False),
                json.dumps(optimization_data.get('expected_improvement', {}), ensure_ascii=False)
            ))
            
            await conn.commit()
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜ç­–ç•¥ä¼˜åŒ–ç»“æœå¤±è´¥: {e}")
            return False
        finally:
            await cursor.close()

    async def get_learning_performance_history(self, group_id: str, limit: int = 30) -> List[Dict[str, Any]]:
        """è·å–å­¦ä¹ æ€§èƒ½å†å²æ•°æ®"""
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()
        
        try:
            await cursor.execute('''
                SELECT session_id, timestamp, quality_score, learning_time, success
                FROM learning_performance_history 
                WHERE group_id = ?
                ORDER BY timestamp DESC
                LIMIT ?
            ''', (group_id, limit))
            
            history = []
            for row in await cursor.fetchall():
                history.append({
                    'session_id': row[0],
                    'timestamp': row[1],
                    'quality_score': row[2] or 0.0,
                    'learning_time': row[3] or 0.0,
                    'success': bool(row[4])
                })
            
            return history
            
        except Exception as e:
            logger.error(f"è·å–å­¦ä¹ æ€§èƒ½å†å²å¤±è´¥: {e}")
            return []
        finally:
            await cursor.close()

    async def save_learning_performance_record(self, group_id: str, performance_data: Dict[str, Any]) -> bool:
        """ä¿å­˜å­¦ä¹ æ€§èƒ½è®°å½•"""
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()
        
        try:
            await cursor.execute('''
                INSERT INTO learning_performance_history 
                (group_id, session_id, timestamp, quality_score, learning_time, success, successful_pattern, failed_pattern)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                group_id,
                performance_data.get('session_id', ''),
                performance_data.get('timestamp', time.time()),
                performance_data.get('quality_score', 0.0),
                performance_data.get('learning_time', 0.0),
                performance_data.get('success', False),
                performance_data.get('successful_pattern', ''),
                performance_data.get('failed_pattern', '')
            ))
            
            await conn.commit()
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜å­¦ä¹ æ€§èƒ½è®°å½•å¤±è´¥: {e}")
            return False
        finally:
            await cursor.close()

    async def get_messages_for_replay(self, group_id: str, days: int = 30, limit: int = 100) -> List[Dict[str, Any]]:
        """è·å–ç”¨äºè®°å¿†é‡æ”¾çš„æ¶ˆæ¯"""
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()
            
            try:
                # è·å–æŒ‡å®šå¤©æ•°å†…çš„æ¶ˆæ¯
                cutoff_time = time.time() - (days * 24 * 3600)
                
                await cursor.execute('''
                    SELECT id, message, sender_id, group_id, timestamp
                    FROM raw_messages 
                    WHERE group_id = ? AND timestamp > ? AND processed = TRUE
                    ORDER BY timestamp DESC
                    LIMIT ?
                ''', (group_id, cutoff_time, limit))
                
                messages = []
                for row in await cursor.fetchall():
                    messages.append({
                        'message_id': row[0],
                        'message': row[1],
                        'sender_id': row[2],
                        'group_id': row[3],
                        'timestamp': row[4]
                    })
                
                return messages
                
            except Exception as e:
                logger.error(f"è·å–è®°å¿†é‡æ”¾æ¶ˆæ¯å¤±è´¥: {e}")
                return []
            finally:
                await cursor.close()

    async def save_user_preferences(self, group_id: str, user_id: str, preferences: Dict[str, Any]) -> bool:
        """ä¿å­˜ç”¨æˆ·åå¥½è®¾ç½®"""
        conn = await self.get_group_connection(group_id)
        cursor = await conn.cursor()
        
        try:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è¡¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
            await cursor.execute('''
                CREATE TABLE IF NOT EXISTS user_preferences (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_id TEXT NOT NULL,
                    group_id TEXT NOT NULL,
                    favorite_topics TEXT, -- JSONæ ¼å¼
                    interaction_style TEXT, -- JSONæ ¼å¼
                    learning_preferences TEXT, -- JSONæ ¼å¼
                    adaptive_rate REAL DEFAULT 0.5,
                    updated_at REAL NOT NULL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(user_id, group_id)
                )
            ''')
            
            await cursor.execute('''
                INSERT OR REPLACE INTO user_preferences 
                (user_id, group_id, favorite_topics, interaction_style, learning_preferences, adaptive_rate, updated_at)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (
                user_id,
                group_id,
                json.dumps(preferences.get('favorite_topics', []), ensure_ascii=False),
                json.dumps(preferences.get('interaction_style', {}), ensure_ascii=False),
                json.dumps(preferences.get('learning_preferences', {}), ensure_ascii=False),
                preferences.get('adaptive_rate', 0.5),
                time.time()
            ))
            
            await conn.commit()
            return True
            
        except Exception as e:
            self._logger.error(f"ä¿å­˜ç”¨æˆ·åå¥½å¤±è´¥: {e}")
            return False

    async def load_user_preferences(self, group_id: str, user_id: str) -> Optional[Dict[str, Any]]:
        """åŠ è½½ç”¨æˆ·åå¥½è®¾ç½®"""
        conn = await self.get_group_connection(group_id)
        cursor = await conn.cursor()
        
        try:
            await cursor.execute('''
                SELECT favorite_topics, interaction_style, learning_preferences, adaptive_rate, updated_at
                FROM user_preferences WHERE user_id = ? AND group_id = ?
            ''', (user_id, group_id))
            
            row = await cursor.fetchone()
            if not row:
                return None
                
            return {
                'favorite_topics': json.loads(row[0]) if row[0] else [],
                'interaction_style': json.loads(row[1]) if row[1] else {},
                'learning_preferences': json.loads(row[2]) if row[2] else {},
                'adaptive_rate': row[3],
                'updated_at': row[4]
            }
            
        except Exception as e:
            self._logger.error(f"åŠ è½½ç”¨æˆ·åå¥½å¤±è´¥: {e}")
            return None

    async def save_conversation_context(self, group_id: str, context_data: Dict[str, Any]) -> bool:
        """ä¿å­˜å¯¹è¯ä¸Šä¸‹æ–‡"""
        conn = await self.get_group_connection(group_id)
        cursor = await conn.cursor()
        
        try:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è¡¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
            await cursor.execute('''
                CREATE TABLE IF NOT EXISTS conversation_contexts (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    group_id TEXT NOT NULL,
                    context_id TEXT UNIQUE NOT NULL,
                    participants TEXT, -- JSONæ ¼å¼å­˜å‚¨å‚ä¸è€…åˆ—è¡¨
                    current_topic TEXT,
                    emotion_state TEXT, -- JSONæ ¼å¼å­˜å‚¨æƒ…æ„ŸçŠ¶æ€
                    context_messages TEXT, -- JSONæ ¼å¼å­˜å‚¨ä¸Šä¸‹æ–‡æ¶ˆæ¯
                    start_time REAL NOT NULL,
                    last_updated REAL NOT NULL,
                    is_active BOOLEAN DEFAULT TRUE,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            await cursor.execute('''
                INSERT OR REPLACE INTO conversation_contexts 
                (group_id, context_id, participants, current_topic, emotion_state, context_messages, start_time, last_updated, is_active)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                group_id,
                context_data.get('context_id'),
                json.dumps(list(context_data.get('participants', set())), ensure_ascii=False),
                context_data.get('current_topic'),
                json.dumps(context_data.get('emotion_state', {}), ensure_ascii=False),
                json.dumps(context_data.get('messages', []), ensure_ascii=False),
                context_data.get('start_time', time.time()),
                time.time(),
                context_data.get('is_active', True)
            ))
            
            await conn.commit()
            return True
            
        except Exception as e:
            self._logger.error(f"ä¿å­˜å¯¹è¯ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return False

    async def get_active_conversation_contexts(self, group_id: str) -> List[Dict[str, Any]]:
        """è·å–æ´»è·ƒçš„å¯¹è¯ä¸Šä¸‹æ–‡"""
        conn = await self.get_group_connection(group_id)
        cursor = await conn.cursor()
        
        try:
            await cursor.execute('''
                SELECT context_id, participants, current_topic, emotion_state, context_messages, start_time, last_updated
                FROM conversation_contexts 
                WHERE group_id = ? AND is_active = TRUE
                ORDER BY last_updated DESC
            ''', (group_id,))
            
            contexts = []
            for row in await cursor.fetchall():
                contexts.append({
                    'context_id': row[0],
                    'participants': set(json.loads(row[1])) if row[1] else set(),
                    'current_topic': row[2],
                    'emotion_state': json.loads(row[3]) if row[3] else {},
                    'messages': json.loads(row[4]) if row[4] else [],
                    'start_time': row[5],
                    'last_updated': row[6]
                })
            
            return contexts
            
        except Exception as e:
            self._logger.error(f"è·å–å¯¹è¯ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return []

    async def save_learning_session_record(self, group_id: str, session_data: Dict[str, Any]) -> bool:
        """ä¿å­˜å­¦ä¹ ä¼šè¯è®°å½•"""
        conn = await self.get_group_connection(group_id)
        cursor = await conn.cursor()
        
        try:
            await cursor.execute('''
                INSERT OR REPLACE INTO learning_sessions
                (session_id, start_time, end_time, messages_processed, filtered_messages, 
                 style_updates, quality_score, success)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                session_data.get('session_id'),
                session_data.get('start_time'),
                session_data.get('end_time'),
                session_data.get('messages_processed', 0),
                session_data.get('filtered_messages', 0),
                session_data.get('style_updates', 0),
                session_data.get('quality_score', 0.0),
                session_data.get('success', False)
            ))
            
            await conn.commit()
            return True
            
        except Exception as e:
            self._logger.error(f"ä¿å­˜å­¦ä¹ ä¼šè¯è®°å½•å¤±è´¥: {e}")
            return False

    async def get_recent_learning_sessions(self, group_id: str, days: int = 7) -> List[Dict[str, Any]]:
        """è·å–æœ€è¿‘çš„å­¦ä¹ ä¼šè¯è®°å½•"""
        conn = await self.get_group_connection(group_id)
        cursor = await conn.cursor()
        
        try:
            start_time = time.time() - (days * 24 * 3600)
            
            await cursor.execute('''
                SELECT session_id, start_time, end_time, messages_processed, filtered_messages,
                       style_updates, quality_score, success
                FROM learning_sessions 
                WHERE start_time >= ?
                ORDER BY start_time DESC
            ''', (start_time,))
            
            sessions = []
            for row in await cursor.fetchall():
                sessions.append({
                    'session_id': row[0],
                    'start_time': row[1],
                    'end_time': row[2],
                    'messages_processed': row[3],
                    'filtered_messages': row[4],
                    'style_updates': row[5],
                    'quality_score': row[6],
                    'success': row[7]
                })
            
            return sessions
            
        except Exception as e:
            self._logger.error(f"è·å–å­¦ä¹ ä¼šè¯è®°å½•å¤±è´¥: {e}")
            return []

    # ========== å¥½æ„Ÿåº¦ç³»ç»Ÿæ•°æ®åº“æ“ä½œæ–¹æ³• ==========

    async def get_user_affection(self, group_id: str, user_id: str) -> Optional[Dict[str, Any]]:
        """è·å–ç”¨æˆ·å¥½æ„Ÿåº¦"""
        conn = await self.get_group_connection(group_id)
        cursor = await conn.cursor()
        
        try:
            await cursor.execute('''
                SELECT affection_level, last_interaction, last_updated, interaction_count
                FROM user_affection WHERE user_id = ? AND group_id = ?
            ''', (user_id, group_id))
            
            row = await cursor.fetchone()
            if not row:
                return None
                
            return {
                'user_id': user_id,
                'group_id': group_id,
                'affection_level': row[0],
                'last_interaction': row[1],
                'last_updated': row[2],
                'interaction_count': row[3]
            }
            
        except Exception as e:
            self._logger.error(f"è·å–ç”¨æˆ·å¥½æ„Ÿåº¦å¤±è´¥: {e}")
            return None

    async def update_user_affection(self, group_id: str, user_id: str, 
                                  new_level: int, change_reason: str = "", 
                                  bot_mood: str = "") -> bool:
        """æ›´æ–°ç”¨æˆ·å¥½æ„Ÿåº¦"""
        conn = await self.get_group_connection(group_id)
        cursor = await conn.cursor()
        
        try:
            current_time = time.time()
            
            # è·å–å½“å‰å¥½æ„Ÿåº¦
            current_affection = await self.get_user_affection(group_id, user_id)
            previous_level = current_affection['affection_level'] if current_affection else 0
            interaction_count = current_affection['interaction_count'] if current_affection else 0
            
            # æ›´æ–°æˆ–æ’å…¥å¥½æ„Ÿåº¦è®°å½•
            await cursor.execute('''
                INSERT OR REPLACE INTO user_affection 
                (user_id, group_id, affection_level, last_interaction, last_updated, interaction_count)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (user_id, group_id, new_level, current_time, current_time, interaction_count + 1))
            
            # è®°å½•å¥½æ„Ÿåº¦å˜åŒ–å†å²
            change_amount = new_level - previous_level
            if change_amount != 0:
                await cursor.execute('''
                    INSERT INTO affection_history 
                    (user_id, group_id, change_amount, previous_level, new_level, 
                     change_reason, bot_mood, timestamp)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                ''', (user_id, group_id, change_amount, previous_level, new_level, 
                      change_reason, bot_mood, current_time))
            
            await conn.commit()
            return True
            
        except Exception as e:
            self._logger.error(f"æ›´æ–°ç”¨æˆ·å¥½æ„Ÿåº¦å¤±è´¥: {e}")
            return False

    async def get_all_user_affections(self, group_id: str) -> List[Dict[str, Any]]:
        """è·å–ç¾¤å†…æ‰€æœ‰ç”¨æˆ·å¥½æ„Ÿåº¦"""
        conn = await self.get_group_connection(group_id)
        cursor = await conn.cursor()
        
        try:
            await cursor.execute('''
                SELECT user_id, affection_level, last_interaction, last_updated, interaction_count
                FROM user_affection 
                WHERE group_id = ?
                ORDER BY affection_level DESC
            ''', (group_id,))
            
            affections = []
            for row in await cursor.fetchall():
                affections.append({
                    'user_id': row[0],
                    'group_id': group_id,
                    'affection_level': row[1],
                    'last_interaction': row[2],
                    'last_updated': row[3],
                    'interaction_count': row[4]
                })
            
            return affections
            
        except Exception as e:
            self._logger.error(f"è·å–æ‰€æœ‰ç”¨æˆ·å¥½æ„Ÿåº¦å¤±è´¥: {e}")
            return []

    async def get_total_affection(self, group_id: str) -> int:
        """è·å–ç¾¤å†…æ€»å¥½æ„Ÿåº¦"""
        conn = await self.get_group_connection(group_id)
        cursor = await conn.cursor()
        
        try:
            await cursor.execute('''
                SELECT SUM(affection_level) FROM user_affection WHERE group_id = ?
            ''', (group_id,))
            
            result = await cursor.fetchone()
            return result[0] if result[0] is not None else 0
            
        except Exception as e:
            self._logger.error(f"è·å–æ€»å¥½æ„Ÿåº¦å¤±è´¥: {e}")
            return 0

    async def save_bot_mood(self, group_id: str, mood_type: str, mood_intensity: float,
                           mood_description: str, duration_hours: int = 24) -> bool:
        """ä¿å­˜botæƒ…ç»ªçŠ¶æ€"""
        conn = await self.get_group_connection(group_id)
        cursor = await conn.cursor()
        
        try:
            current_time = time.time()
            end_time = current_time + (duration_hours * 3600)
            
            # å°†ä¹‹å‰çš„æƒ…ç»ªè®¾ä¸ºéæ´»è·ƒçŠ¶æ€
            await cursor.execute('''
                UPDATE bot_mood SET is_active = FALSE, end_time = ? WHERE group_id = ? AND is_active = TRUE
            ''', (current_time, group_id))
            
            # æ’å…¥æ–°çš„æƒ…ç»ªçŠ¶æ€
            await cursor.execute('''
                INSERT INTO bot_mood 
                (group_id, mood_type, mood_intensity, mood_description, start_time, end_time, is_active)
                VALUES (?, ?, ?, ?, ?, ?, TRUE)
            ''', (group_id, mood_type, mood_intensity, mood_description, current_time, end_time))
            
            await conn.commit()
            return True
            
        except Exception as e:
            self._logger.error(f"ä¿å­˜botæƒ…ç»ªå¤±è´¥: {e}")
            return False

    async def get_current_bot_mood(self, group_id: str) -> Optional[Dict[str, Any]]:
        """è·å–å½“å‰botæƒ…ç»ª"""
        conn = await self.get_group_connection(group_id)
        cursor = await conn.cursor()
        
        try:
            current_time = time.time()
            
            await cursor.execute('''
                SELECT mood_type, mood_intensity, mood_description, start_time, end_time
                FROM bot_mood 
                WHERE group_id = ? AND is_active = TRUE AND start_time <= ? AND (end_time IS NULL OR end_time > ?)
                ORDER BY start_time DESC
                LIMIT 1
            ''', (group_id, current_time, current_time))
            
            row = await cursor.fetchone()
            if not row:
                return None
                
            return {
                'mood_type': row[0],
                'mood_intensity': row[1],
                'mood_description': row[2],
                'start_time': row[3],
                'end_time': row[4]
            }
            
        except Exception as e:
            self._logger.error(f"è·å–å½“å‰botæƒ…ç»ªå¤±è´¥: {e}")
            return None

    async def get_affection_history(self, group_id: str, user_id: str = None, 
                                   days: int = 7) -> List[Dict[str, Any]]:
        """è·å–å¥½æ„Ÿåº¦å˜åŒ–å†å²"""
        conn = await self.get_group_connection(group_id)
        cursor = await conn.cursor()
        
        try:
            start_time = time.time() - (days * 24 * 3600)
            
            if user_id:
                await cursor.execute('''
                    SELECT user_id, change_amount, previous_level, new_level, 
                           change_reason, bot_mood, timestamp
                    FROM affection_history 
                    WHERE group_id = ? AND user_id = ? AND timestamp >= ?
                    ORDER BY timestamp DESC
                ''', (group_id, user_id, start_time))
            else:
                await cursor.execute('''
                    SELECT user_id, change_amount, previous_level, new_level, 
                           change_reason, bot_mood, timestamp
                    FROM affection_history 
                    WHERE group_id = ? AND timestamp >= ?
                    ORDER BY timestamp DESC
                ''', (group_id, start_time))
            
            history = []
            for row in await cursor.fetchall():
                history.append({
                    'user_id': row[0],
                    'change_amount': row[1],
                    'previous_level': row[2],
                    'new_level': row[3],
                    'change_reason': row[4],
                    'bot_mood': row[5],
                    'timestamp': row[6]
                })
            
            return history
            
        except Exception as e:
            self._logger.error(f"è·å–å¥½æ„Ÿåº¦å†å²å¤±è´¥: {e}")
            return []

    async def record_llm_call_statistics(self, provider_type: str, model_name: str, 
                                        success: bool, response_time_ms: int) -> bool:
        """è®°å½•LLMè°ƒç”¨ç»Ÿè®¡æ•°æ®"""
        try:
            async with self.get_db_connection() as conn:
                cursor = await conn.cursor()
                
                current_time = time.time()
                
                # æŸ¥è¯¢å½“å‰ç»Ÿè®¡æ•°æ®
                await cursor.execute('''
                    SELECT total_calls, success_calls, failed_calls, total_response_time_ms
                    FROM llm_call_statistics 
                    WHERE provider_type = ? AND model_name = ?
                ''', (provider_type, model_name))
                
                row = await cursor.fetchone()
                if row:
                    # æ›´æ–°ç°æœ‰è®°å½•
                    total_calls = row[0] + 1
                    success_calls = row[1] + (1 if success else 0)
                    failed_calls = row[2] + (0 if success else 1)
                    total_response_time = row[3] + response_time_ms
                    avg_response_time = total_response_time / total_calls
                    success_rate = success_calls / total_calls
                    
                    await cursor.execute('''
                        UPDATE llm_call_statistics 
                        SET total_calls = ?, success_calls = ?, failed_calls = ?, 
                            total_response_time_ms = ?, avg_response_time_ms = ?, 
                            success_rate = ?, last_call_time = ?, updated_at = CURRENT_TIMESTAMP
                        WHERE provider_type = ? AND model_name = ?
                    ''', (total_calls, success_calls, failed_calls, total_response_time,
                          avg_response_time, success_rate, current_time, provider_type, model_name))
                else:
                    # æ’å…¥æ–°è®°å½•
                    success_calls = 1 if success else 0
                    failed_calls = 0 if success else 1
                    success_rate = 1.0 if success else 0.0
                    
                    await cursor.execute('''
                        INSERT INTO llm_call_statistics 
                        (provider_type, model_name, total_calls, success_calls, failed_calls,
                         total_response_time_ms, avg_response_time_ms, success_rate, last_call_time)
                        VALUES (?, ?, 1, ?, ?, ?, ?, ?, ?)
                    ''', (provider_type, model_name, success_calls, failed_calls,
                          response_time_ms, response_time_ms, success_rate, current_time))
                
                await conn.commit()
                return True
                
        except Exception as e:
            self._logger.error(f"è®°å½•LLMè°ƒç”¨ç»Ÿè®¡å¤±è´¥: {e}")
            return False
        finally:
            await cursor.close()

    async def get_llm_call_statistics(self) -> Dict[str, Any]:
        """è·å–LLMè°ƒç”¨ç»Ÿè®¡æ•°æ®"""
        try:
            async with self.get_db_connection() as conn:
                cursor = await conn.cursor()
                
                await cursor.execute('''
                    SELECT provider_type, model_name, total_calls, success_calls, failed_calls,
                           avg_response_time_ms, success_rate, last_call_time
                    FROM llm_call_statistics
                    ORDER BY provider_type, total_calls DESC
                ''')
                
                statistics = {}
                total_calls = 0
                
                for row in await cursor.fetchall():
                    provider_type = row[0]
                    model_name = row[1] or f"{provider_type}_model"
                    
                    stats = {
                        "total_calls": row[2],
                        "success_calls": row[3], 
                        "failed_calls": row[4],
                        "avg_response_time_ms": row[5] or 0,
                        "success_rate": row[6] or 0,
                        "last_call_time": row[7]
                    }
                    
                    statistics[f"{provider_type}_{model_name}"] = stats
                    total_calls += row[2]
                
                # å¦‚æœæ²¡æœ‰ç»Ÿè®¡æ•°æ®ï¼Œè¿”å›é»˜è®¤ç»“æ„
                if not statistics:
                    statistics = {
                        "filter_provider": {"total_calls": 0, "avg_response_time_ms": 0, "success_rate": 0, "error_count": 0},
                        "refine_provider": {"total_calls": 0, "avg_response_time_ms": 0, "success_rate": 0, "error_count": 0}, 
                        "reinforce_provider": {"total_calls": 0, "avg_response_time_ms": 0, "success_rate": 0, "error_count": 0}
                    }
                
                return {
                    "statistics": statistics,
                    "total_calls": total_calls
                }
                
        except Exception as e:
            self._logger.error(f"è·å–LLMè°ƒç”¨ç»Ÿè®¡å¤±è´¥: {e}")
            return {
                "statistics": {
                    "filter_provider": {"total_calls": 0, "avg_response_time_ms": 0, "success_rate": 0, "error_count": 0},
                    "refine_provider": {"total_calls": 0, "avg_response_time_ms": 0, "success_rate": 0, "error_count": 0},
                    "reinforce_provider": {"total_calls": 0, "avg_response_time_ms": 0, "success_rate": 0, "error_count": 0}
                },
                "total_calls": 0
            }
        finally:
            await cursor.close()

    async def export_messages_learning_data(self) -> Dict[str, Any]:
        """å¯¼å‡ºæ¶ˆæ¯å­¦ä¹ æ•°æ®"""
        try:
            async with self.get_db_connection() as conn:
                cursor = await conn.cursor()

            # å¯¼å‡ºåŸå§‹æ¶ˆæ¯
            await cursor.execute('''
                SELECT id, sender_id, sender_name, message, group_id, platform, timestamp, processed
                FROM raw_messages ORDER BY timestamp DESC
            ''')
            raw_messages = []
            for row in await cursor.fetchall():
                raw_messages.append({
                    'id': row[0],
                    'sender_id': row[1],
                    'sender_name': row[2],
                    'message': row[3],
                    'group_id': row[4],
                    'platform': row[5],
                    'timestamp': row[6],
                    'processed': bool(row[7])
                })

            # å¯¼å‡ºç­›é€‰æ¶ˆæ¯
            await cursor.execute('''
                SELECT id, raw_message_id, message, sender_id, group_id, confidence,
                       filter_reason, timestamp, used_for_learning, quality_scores
                FROM filtered_messages ORDER BY timestamp DESC
            ''')
            filtered_messages = []
            for row in await cursor.fetchall():
                quality_scores = {}
                try:
                    if row[9]:  # quality_scores
                        quality_scores = json.loads(row[9])
                except (json.JSONDecodeError, TypeError):
                    pass

                filtered_messages.append({
                    'id': row[0],
                    'raw_message_id': row[1],
                    'message': row[2],
                    'sender_id': row[3],
                    'group_id': row[4],
                    'confidence': row[5],
                    'filter_reason': row[6],
                    'timestamp': row[7],
                    'used_for_learning': bool(row[8]),
                    'quality_scores': quality_scores
                })

            # å¯¼å‡ºå­¦ä¹ æ‰¹æ¬¡è®°å½•
            await cursor.execute('''
                SELECT id, group_id, start_time, end_time, quality_score,
                       processed_messages, batch_name, message_count,
                       filtered_count, success, error_message
                FROM learning_batches ORDER BY start_time DESC
            ''')
            learning_batches = []
            for row in await cursor.fetchall():
                learning_batches.append({
                    'id': row[0],
                    'group_id': row[1],
                    'start_time': row[2],
                    'end_time': row[3],
                    'quality_score': row[4],
                    'processed_messages': row[5],
                    'batch_name': row[6],
                    'message_count': row[7],
                    'filtered_count': row[8],
                    'success': bool(row[9]),
                    'error_message': row[10]
                })

            # å¯¼å‡ºäººæ ¼æ›´æ–°è®°å½•
            await cursor.execute('''
                SELECT id, timestamp, group_id, update_type, original_content,
                       new_content, reason, status, reviewer_comment, review_time
                FROM persona_update_records ORDER BY timestamp DESC
            ''')
            persona_update_records = []
            for row in await cursor.fetchall():
                persona_update_records.append({
                    'id': row[0],
                    'timestamp': row[1],
                    'group_id': row[2],
                    'update_type': row[3],
                    'original_content': row[4],
                    'new_content': row[5],
                    'reason': row[6],
                    'status': row[7],
                    'reviewer_comment': row[8],
                    'review_time': row[9]
                })

            # è·å–ç»Ÿè®¡ä¿¡æ¯
            statistics = await self.get_messages_statistics()

            export_data = {
                'export_timestamp': time.time(),
                'export_date': datetime.now().isoformat(),
                'statistics': statistics,
                'raw_messages': raw_messages,
                'filtered_messages': filtered_messages,
                'learning_batches': learning_batches,
                'persona_update_records': persona_update_records
            }

            self._logger.info(f"æˆåŠŸå¯¼å‡ºå­¦ä¹ æ•°æ®: {len(raw_messages)} æ¡åŸå§‹æ¶ˆæ¯, {len(filtered_messages)} æ¡ç­›é€‰æ¶ˆæ¯")
            return export_data

        except Exception as e:
            self._logger.error(f"å¯¼å‡ºæ¶ˆæ¯å­¦ä¹ æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise DataStorageError(f"å¯¼å‡ºæ¶ˆæ¯å­¦ä¹ æ•°æ®å¤±è´¥: {str(e)}")
        finally:
            await cursor.close()

    async def clear_all_messages_data(self):
        """æ¸…ç©ºæ‰€æœ‰æ¶ˆæ¯æ•°æ®"""
        try:
            async with self.get_db_connection() as conn:
                cursor = await conn.cursor()

            # æ¸…ç©ºæ‰€æœ‰è¡¨çš„æ•°æ®
            tables_to_clear = [
                'raw_messages',
                'filtered_messages',
                'learning_batches',
                'persona_update_records',
                'reinforcement_learning_results',
                'persona_fusion_history',
                'strategy_optimization_results',
                'learning_performance_history'
            ]

            for table in tables_to_clear:
                await cursor.execute(f'DELETE FROM {table}')
                self._logger.debug(f"å·²æ¸…ç©ºè¡¨: {table}")

            await conn.commit()
            self._logger.info("æ‰€æœ‰æ¶ˆæ¯æ•°æ®å·²æ¸…ç©º")

        except Exception as e:
            self._logger.error(f"æ¸…ç©ºæ‰€æœ‰æ¶ˆæ¯æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise DataStorageError(f"æ¸…ç©ºæ‰€æœ‰æ¶ˆæ¯æ•°æ®å¤±è´¥: {str(e)}")
        finally:
            await cursor.close()

    async def get_learning_patterns_data(self) -> Dict[str, Any]:
        """è·å–å­¦ä¹ æ¨¡å¼æ•°æ®"""
        try:
            # é¦–å…ˆå°è¯•è·å–è¡¨è¾¾æ¨¡å¼æ•°æ®ï¼ˆæ¥è‡ªexpression_patternsè¡¨ï¼‰
            expression_patterns = await self.get_expression_patterns_for_webui()
            
            # è·å–å…¶ä»–å­¦ä¹ æ•°æ®
            async with self.get_db_connection() as conn:
                cursor = await conn.cursor()
            
            # æ£€æŸ¥æ˜¯å¦æœ‰åŸå§‹æ¶ˆæ¯æ•°æ®
            await cursor.execute('SELECT COUNT(*) FROM raw_messages')
            raw_data_count = (await cursor.fetchone())[0]
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç­›é€‰æ¶ˆæ¯æ•°æ®
            await cursor.execute('SELECT COUNT(*) FROM filtered_messages')
            filtered_data_count = (await cursor.fetchone())[0]
            
            # å¦‚æœæœ‰è¡¨è¾¾æ¨¡å¼æ•°æ®ï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™ä½¿ç”¨é»˜è®¤æç¤º
            if expression_patterns:
                emotion_patterns = []
                for pattern in expression_patterns[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
                    situation = pattern.get('situation', 'åœºæ™¯æè¿°').strip()
                    expression = pattern.get('expression', 'è¡¨è¾¾æ–¹å¼').strip()
                    weight = pattern.get('weight', 0)
                    
                    # ç¡®ä¿ä¸æ˜¾ç¤ºç©ºçš„æˆ–æ— æ„ä¹‰çš„æ•°æ®
                    if situation and expression and situation != 'æœªçŸ¥' and expression != 'æœªçŸ¥':
                        pattern_name = f"æƒ…æ„Ÿè¡¨è¾¾-{situation[:10]}"  # æˆªå–å‰10ä¸ªå­—ç¬¦ä½œä¸ºæ¨¡å¼å
                        emotion_patterns.append({
                            'pattern': pattern_name,
                            'confidence': round(weight * 20, 2),  # å°†æƒé‡è½¬æ¢ä¸ºç½®ä¿¡åº¦ç™¾åˆ†æ¯”
                            'frequency': max(1, int(weight))  # ç¡®ä¿é¢‘ç‡è‡³å°‘ä¸º1
                        })
                
                # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„è¡¨è¾¾æ¨¡å¼ï¼Œæ·»åŠ ä¸€ä¸ªè¯´æ˜
                if not emotion_patterns:
                    emotion_patterns.append({
                        'pattern': 'æ­£åœ¨å­¦ä¹ è¡¨è¾¾æ¨¡å¼',
                        'confidence': 30.0,
                        'frequency': 1
                    })
            else:
                # å¦‚æœæ²¡æœ‰è¡¨è¾¾æ¨¡å¼ï¼Œä½†æœ‰åŸå§‹æ•°æ®ï¼Œæ˜¾ç¤ºå­¦ä¹ ä¸­çŠ¶æ€
                if raw_data_count > 0:
                    emotion_patterns = [{
                        'pattern': 'æ­£åœ¨å­¦ä¹ è¡¨è¾¾æ¨¡å¼ï¼Œè¯·ç¨å€™...',
                        'confidence': 50.0,
                        'frequency': raw_data_count
                    }]
                else:
                    emotion_patterns = [{
                        'pattern': 'æš‚æ— å¯¹è¯æ•°æ®ï¼Œè¯·å…ˆè¿›è¡Œå¯¹è¯',
                        'confidence': 0.0,
                        'frequency': 0
                    }]
            
            # è¯­è¨€é£æ ¼åˆ†æï¼ˆåŸºäºåŸå§‹æ¶ˆæ¯é•¿åº¦åˆ†å¸ƒï¼‰
            await cursor.execute('''
                SELECT 
                    CASE 
                        WHEN LENGTH(message) < 10 THEN 'ç®€çŸ­è¡¨è¾¾'
                        WHEN LENGTH(message) < 30 THEN 'é€‚ä¸­è¡¨è¾¾'
                        WHEN LENGTH(message) < 100 THEN 'è¯¦ç»†è¡¨è¾¾'
                        ELSE 'é•¿ç¯‡è¡¨è¾¾'
                    END as style_type,
                    COUNT(*) as count
                FROM raw_messages
                WHERE message IS NOT NULL AND LENGTH(TRIM(message)) > 0
                GROUP BY style_type
            ''')
            
            language_patterns = []
            for row in await cursor.fetchall():
                language_patterns.append({
                    'style': row[0],  # æ”¹ä¸ºstyleå­—æ®µä»¥åŒ¹é…å‰ç«¯
                    'type': row[0],   # ä¿ç•™typeç”¨äºå…¼å®¹æ€§
                    'count': row[1],
                    'frequency': row[1],  # æ·»åŠ frequencyå­—æ®µç”¨äºå‰ç«¯æ˜¾ç¤º
                    'context': 'general',
                    'environment': 'general'
                })
            
            # å¦‚æœæ²¡æœ‰è¯­è¨€æ¨¡å¼æ•°æ®
            if not language_patterns:
                language_patterns = [{
                    'style': 'æš‚æ— è¯­è¨€é£æ ¼æ•°æ®',
                    'type': 'æš‚æ— è¯­è¨€é£æ ¼æ•°æ®',
                    'count': 0,
                    'frequency': 0,
                    'context': 'general',
                    'environment': 'general'
                }]
            
            # è¯é¢˜åå¥½åˆ†æï¼ˆåŸºäºç¾¤ç»„æ´»è·ƒåº¦å’Œæ™ºèƒ½ä¸»é¢˜è¯†åˆ«ï¼‰
            topic_preferences = []

            # è·å–å„ä¸ªç¾¤ç»„çš„æ¶ˆæ¯æ•°æ®è¿›è¡Œä¸»é¢˜åˆ†æ
            await cursor.execute('''
                SELECT
                    group_id,
                    COUNT(*) as message_count,
                    AVG(LENGTH(message)) as avg_length
                FROM raw_messages
                WHERE group_id IS NOT NULL AND LENGTH(TRIM(message)) > 3
                GROUP BY group_id
                HAVING COUNT(*) > 10
                ORDER BY message_count DESC
                LIMIT 8
            ''')

            group_data = await cursor.fetchall()

            # å…ˆæ”¶é›†æ‰€æœ‰group_dataï¼Œé¿å…åµŒå¥—æŸ¥è¯¢
            for row in group_data:
                try:
                    # æ·»åŠ è¡Œæ•°æ®éªŒè¯
                    if len(row) < 3:
                        self._logger.warning(f"ç¾¤ç»„è¯é¢˜æ•°æ®è¡Œä¸å®Œæ•´ (æœŸæœ›3ä¸ªå­—æ®µï¼Œå®é™…{len(row)}ä¸ª)ï¼Œè·³è¿‡: {row}")
                        continue

                    group_id = row[0]
                    message_count = int(row[1]) if row[1] else 0
                    avg_length = float(row[2]) if row[2] else 0

                    # åˆ›å»ºæ–°çš„cursoræ¥æ‰§è¡ŒåµŒå¥—æŸ¥è¯¢ï¼ˆé¿å…cursorçŠ¶æ€å†²çªï¼‰
                    async with self.get_db_connection() as nested_conn:
                        nested_cursor = await nested_conn.cursor()

                        # è·å–è¯¥ç¾¤ç»„çš„ä»£è¡¨æ€§æ¶ˆæ¯è¿›è¡Œä¸»é¢˜åˆ†æ
                        await nested_cursor.execute('''
                            SELECT message
                            FROM raw_messages
                            WHERE group_id = ? AND LENGTH(TRIM(message)) > 5 AND LENGTH(TRIM(message)) < 200
                            ORDER BY LENGTH(message) DESC, timestamp DESC
                            LIMIT 20
                        ''', (group_id,))

                        messages = await nested_cursor.fetchall()
                        await nested_cursor.close()

                        if not messages:
                            continue

                        # æ™ºèƒ½ä¸»é¢˜è¯†åˆ«
                        topic_analysis = self._analyze_topic_from_messages([msg[0] for msg in messages])
                        topic_name = topic_analysis['topic']
                        conversation_style = topic_analysis['style']

                        # æ ¹æ®æ¶ˆæ¯é•¿åº¦å’Œæ•°é‡æ¨æ–­å…´è¶£åº¦
                        interest_level = min(100, max(10, (message_count * avg_length) / 50))

                        topic_preferences.append({
                            'topic': topic_name,
                            'style': conversation_style,
                            'interest_level': round(interest_level, 1)
                        })
                except Exception as row_error:
                    self._logger.warning(f"å¤„ç†ç¾¤ç»„è¯é¢˜æ•°æ®è¡Œæ—¶å‡ºé”™ï¼Œè·³è¿‡: {row_error}, row: {row if 'row' in locals() and len(str(row)) < 100 else 'row too long'}")
                    continue

            # å»é‡ï¼šç¡®ä¿æ¯ä¸ªè¯é¢˜åªå‡ºç°ä¸€æ¬¡ï¼Œä¿ç•™å…´è¶£åº¦æœ€é«˜çš„
            seen_topics = {}
            for pref in topic_preferences:
                try:
                    topic = pref['topic']
                    # ç¡®ä¿ interest_level æ˜¯æ•°å­—ç±»å‹
                    current_interest = float(pref.get('interest_level', 0))
                    pref['interest_level'] = current_interest

                    if topic not in seen_topics:
                        seen_topics[topic] = pref
                    else:
                        existing_interest = float(seen_topics[topic].get('interest_level', 0))
                        if current_interest > existing_interest:
                            seen_topics[topic] = pref
                except (ValueError, TypeError, KeyError) as e:
                    self._logger.warning(f"å¤„ç†è¯é¢˜åå¥½æ—¶å‡ºé”™ï¼Œè·³è¿‡: {e}, pref: {pref}")

            topic_preferences = list(seen_topics.values())

            # å¦‚æœæ²¡æœ‰è¯é¢˜åå¥½æ•°æ®
            if not topic_preferences:
                topic_preferences = [{
                    'topic': 'æš‚æ— è¯é¢˜æ•°æ®',
                    'style': 'ç­‰å¾…ä¸­',
                    'interest_level': 0.0
                }]
            
            return {
                'emotion_patterns': emotion_patterns,
                'language_patterns': language_patterns,
                'topic_preferences': topic_preferences
            }
            
        except Exception as e:
            self._logger.error(f"è·å–å­¦ä¹ æ¨¡å¼æ•°æ®å¤±è´¥: {e}")
            return {
                'emotion_patterns': [
                    {'pattern': 'æ•°æ®è·å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç³»ç»ŸçŠ¶æ€', 'confidence': 0, 'frequency': 0}
                ],
                'language_patterns': [
                    {'type': 'æ•°æ®è·å–å¤±è´¥', 'count': 0, 'environment': 'general'}
                ],
                'topic_preferences': [
                    {'topic': 'æ•°æ®è·å–å¤±è´¥', 'style': 'normal', 'interest_level': 0}
                ]
            }
        finally:
            if 'cursor' in locals():
                await cursor.close()

    async def get_expression_patterns_for_webui(self, limit: int = 20) -> List[Dict[str, Any]]:
        """è·å–è¡¨è¾¾æ¨¡å¼æ•°æ®ç”¨äºWebUIæ˜¾ç¤º"""
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()

            try:
                # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
                await cursor.execute('''
                    SELECT name FROM sqlite_master
                    WHERE type='table' AND name='expression_patterns'
                ''')

                table_exists = await cursor.fetchone()
                if not table_exists:
                    self._logger.debug("expression_patternsè¡¨ä¸å­˜åœ¨")
                    return []

                # è·å–è¡¨è¾¾æ¨¡å¼æ•°æ®
                await cursor.execute('''
                    SELECT situation, expression, weight, last_active_time, group_id
                    FROM expression_patterns
                    ORDER BY weight DESC, last_active_time DESC
                    LIMIT ?
                ''', (limit,))

                patterns = []
                for row in await cursor.fetchall():
                    try:
                        # æ·»åŠ è¡Œæ•°æ®éªŒè¯
                        if len(row) < 5:
                            self._logger.warning(f"è¡¨è¾¾æ¨¡å¼è¡Œæ•°æ®ä¸å®Œæ•´ (æœŸæœ›5ä¸ªå­—æ®µï¼Œå®é™…{len(row)}ä¸ª)ï¼Œè·³è¿‡: {row}")
                            continue

                        patterns.append({
                            'situation': row[0],
                            'expression': row[1],
                            'weight': float(row[2]) if row[2] else 0.0,
                            'last_active_time': row[3],
                            'group_id': row[4]
                        })
                    except Exception as row_error:
                        self._logger.warning(f"å¤„ç†è¡¨è¾¾æ¨¡å¼è¡Œæ—¶å‡ºé”™ï¼Œè·³è¿‡: {row_error}, row: {row}")
                        continue

                return patterns

            except Exception as e:
                self._logger.error(f"è·å–è¡¨è¾¾æ¨¡å¼å¤±è´¥: {e}", exc_info=True)
                return []
            finally:
                await cursor.close()

    async def create_style_learning_review(self, review_data: Dict[str, Any]) -> int:
        """åˆ›å»ºå¯¹è¯é£æ ¼å­¦ä¹ å®¡æŸ¥è®°å½•"""
        try:
            async with self.get_db_connection() as conn:
                cursor = await conn.cursor()

                # ç¡®ä¿å®¡æŸ¥è¡¨å­˜åœ¨
                await self._ensure_style_review_table_exists(cursor)

                # æ’å…¥å®¡æŸ¥è®°å½•
                await cursor.execute('''
                    INSERT INTO style_learning_reviews
                    (type, group_id, timestamp, learned_patterns, few_shots_content, status, description)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                ''', (
                    review_data['type'],
                    review_data['group_id'],
                    review_data['timestamp'],
                    json.dumps(review_data['learned_patterns'], ensure_ascii=False),
                    review_data['few_shots_content'],
                    review_data['status'],
                    review_data['description']
                ))

                review_id = cursor.lastrowid
                await conn.commit()

                self._logger.info(f"åˆ›å»ºé£æ ¼å­¦ä¹ å®¡æŸ¥è®°å½•æˆåŠŸï¼ŒID: {review_id}")
                return review_id

        except Exception as e:
            self._logger.error(f"åˆ›å»ºé£æ ¼å­¦ä¹ å®¡æŸ¥è®°å½•å¤±è´¥: {e}")
            raise DataStorageError(f"åˆ›å»ºé£æ ¼å­¦ä¹ å®¡æŸ¥è®°å½•å¤±è´¥: {str(e)}")

    async def _ensure_style_review_table_exists(self, cursor):
        """ç¡®ä¿é£æ ¼å­¦ä¹ å®¡æŸ¥è¡¨å­˜åœ¨"""
        # æ ¹æ®æ•°æ®åº“ç±»å‹é€‰æ‹©ä¸åŒçš„ DDL
        if self.config.db_type.lower() == 'mysql':
            await cursor.execute('''
                CREATE TABLE IF NOT EXISTS style_learning_reviews (
                    id INT PRIMARY KEY AUTO_INCREMENT,
                    type VARCHAR(100) NOT NULL,
                    group_id VARCHAR(255) NOT NULL,
                    timestamp DOUBLE NOT NULL,
                    learned_patterns TEXT,
                    few_shots_content TEXT,
                    status VARCHAR(50) DEFAULT 'pending',
                    description TEXT,
                    reviewer_comment TEXT,
                    review_time DOUBLE,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
                    INDEX idx_status (status),
                    INDEX idx_group (group_id),
                    INDEX idx_timestamp (timestamp)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
            ''')

            # âœ… æ•°æ®åº“è¿ç§»ï¼šæ·»åŠ ç¼ºå¤±çš„å­—æ®µï¼ˆå¦‚æœè¡¨å·²å­˜åœ¨ä½†ç¼ºå°‘è¿™äº›å­—æ®µï¼‰
            try:
                # æ£€æŸ¥å¹¶æ·»åŠ  reviewer_comment å­—æ®µ
                await cursor.execute('''
                    SELECT COUNT(*)
                    FROM INFORMATION_SCHEMA.COLUMNS
                    WHERE TABLE_SCHEMA = DATABASE()
                    AND TABLE_NAME = 'style_learning_reviews'
                    AND COLUMN_NAME = 'reviewer_comment'
                ''')
                if (await cursor.fetchone())[0] == 0:
                    await cursor.execute('ALTER TABLE style_learning_reviews ADD COLUMN reviewer_comment TEXT')
                    self._logger.info("âœ… è¿ç§»ï¼šå·²æ·»åŠ  reviewer_comment å­—æ®µåˆ° style_learning_reviews è¡¨")

                # æ£€æŸ¥å¹¶æ·»åŠ  review_time å­—æ®µ
                await cursor.execute('''
                    SELECT COUNT(*)
                    FROM INFORMATION_SCHEMA.COLUMNS
                    WHERE TABLE_SCHEMA = DATABASE()
                    AND TABLE_NAME = 'style_learning_reviews'
                    AND COLUMN_NAME = 'review_time'
                ''')
                if (await cursor.fetchone())[0] == 0:
                    await cursor.execute('ALTER TABLE style_learning_reviews ADD COLUMN review_time DOUBLE')
                    self._logger.info("âœ… è¿ç§»ï¼šå·²æ·»åŠ  review_time å­—æ®µåˆ° style_learning_reviews è¡¨")
            except Exception as migration_error:
                self._logger.warning(f"æ•°æ®åº“è¿ç§»æ£€æŸ¥å¤±è´¥ï¼ˆå¯èƒ½æ˜¯é MySQL æ•°æ®åº“ï¼‰: {migration_error}")
        else:
            await cursor.execute('''
                CREATE TABLE IF NOT EXISTS style_learning_reviews (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    type TEXT NOT NULL,
                    group_id TEXT NOT NULL,
                    timestamp REAL NOT NULL,
                    learned_patterns TEXT,
                    few_shots_content TEXT,
                    status TEXT DEFAULT 'pending',
                    description TEXT,
                    reviewer_comment TEXT,
                    review_time REAL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')

            # âœ… SQLite æ•°æ®åº“è¿ç§»ï¼šæ·»åŠ ç¼ºå¤±çš„å­—æ®µ
            try:
                # æ£€æŸ¥è¡¨ç»“æ„
                await cursor.execute("PRAGMA table_info(style_learning_reviews)")
                columns = {row[1] for row in await cursor.fetchall()}

                # æ·»åŠ  reviewer_comment å­—æ®µï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                if 'reviewer_comment' not in columns:
                    await cursor.execute('ALTER TABLE style_learning_reviews ADD COLUMN reviewer_comment TEXT')
                    self._logger.info("âœ… è¿ç§»ï¼šå·²æ·»åŠ  reviewer_comment å­—æ®µåˆ° style_learning_reviews è¡¨ (SQLite)")

                # æ·»åŠ  review_time å­—æ®µï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                if 'review_time' not in columns:
                    await cursor.execute('ALTER TABLE style_learning_reviews ADD COLUMN review_time REAL')
                    self._logger.info("âœ… è¿ç§»ï¼šå·²æ·»åŠ  review_time å­—æ®µåˆ° style_learning_reviews è¡¨ (SQLite)")
            except Exception as migration_error:
                self._logger.warning(f"SQLite æ•°æ®åº“è¿ç§»å¤±è´¥: {migration_error}")

    # æ³¨æ„ï¼šget_pending_style_reviews æ–¹æ³•å·²åœ¨ä¸Šé¢å®šä¹‰ï¼ˆçº¦1456è¡Œï¼‰ï¼Œè¿™é‡Œåˆ é™¤é‡å¤å®šä¹‰
    # ç¬¬ä¸€ä¸ªç‰ˆæœ¬æ˜¯æ­£ç¡®çš„ï¼Œç¬¬äºŒä¸ªç‰ˆæœ¬æœ‰async withç¼©è¿›bug

    async def get_pending_persona_learning_reviews(self, limit: int = 50) -> List[Dict[str, Any]]:
        """è·å–å¾…å®¡æŸ¥çš„äººæ ¼å­¦ä¹ è®°å½•ï¼ˆè´¨é‡ä¸è¾¾æ ‡çš„å­¦ä¹ ç»“æœï¼‰"""
        try:
            async with self.get_db_connection() as conn:
                cursor = await conn.cursor()

                # ç¡®ä¿è¡¨å­˜åœ¨ï¼ˆæ ¹æ®æ•°æ®åº“ç±»å‹ä½¿ç”¨ä¸åŒçš„DDLï¼‰
                if self.config.db_type.lower() == 'mysql':
                    await cursor.execute('''
                        CREATE TABLE IF NOT EXISTS persona_update_reviews (
                            id INT PRIMARY KEY AUTO_INCREMENT,
                            timestamp DOUBLE NOT NULL,
                            group_id VARCHAR(255) NOT NULL,
                            update_type VARCHAR(100) NOT NULL,
                            original_content TEXT,
                            new_content TEXT,
                            proposed_content TEXT,
                            confidence_score DOUBLE,
                            reason TEXT,
                            status VARCHAR(50) NOT NULL DEFAULT 'pending',
                            reviewer_comment TEXT,
                            review_time DOUBLE,
                            INDEX idx_status (status),
                            INDEX idx_group_id (group_id)
                        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
                    ''')
                else:
                    await cursor.execute('''
                        CREATE TABLE IF NOT EXISTS persona_update_reviews (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            timestamp REAL NOT NULL,
                            group_id TEXT NOT NULL,
                            update_type TEXT NOT NULL,
                            original_content TEXT,
                            new_content TEXT,
                            proposed_content TEXT, -- å»ºè®®çš„æ–°å†…å®¹ï¼ˆå…¼å®¹å­—æ®µï¼‰
                            confidence_score REAL, -- ç½®ä¿¡åº¦å¾—åˆ†
                            reason TEXT,
                            status TEXT NOT NULL DEFAULT 'pending',
                            reviewer_comment TEXT,
                            review_time REAL
                        )
                    ''')

                # å°è¯•æ·»åŠ metadataåˆ—ï¼ˆå¦‚æœè¡¨å·²å­˜åœ¨ä½†æ²¡æœ‰æ­¤åˆ—ï¼‰
                try:
                    await cursor.execute('ALTER TABLE persona_update_reviews ADD COLUMN metadata TEXT')
                except:
                    pass  # åˆ—å·²å­˜åœ¨

                await cursor.execute('''
                    SELECT id, timestamp, group_id, update_type, original_content,
                           new_content, proposed_content, confidence_score, reason, status,
                           reviewer_comment, review_time, metadata
                    FROM persona_update_reviews
                    WHERE status = 'pending'
                    ORDER BY timestamp DESC
                    LIMIT ?
                ''', (limit,))

                reviews = []
                import json
                for row in await cursor.fetchall():
                    # ç¡®ä¿æœ‰proposed_contentå­—æ®µï¼Œå¦‚æœä¸ºç©ºåˆ™ä½¿ç”¨new_content
                    proposed_content = row[6] if row[6] else row[5]  # proposed_contentæˆ–new_content
                    confidence_score = row[7] if row[7] is not None else 0.5  # ä½¿ç”¨æ•°æ®åº“ä¸­çš„ç½®ä¿¡åº¦

                    # è§£æmetadata JSON
                    metadata = {}
                    if row[12]:  # metadataå­—æ®µ
                        try:
                            metadata = json.loads(row[12])
                        except:
                            metadata = {}

                    reviews.append({
                        'id': row[0],
                        'timestamp': row[1],
                        'group_id': row[2],
                        'update_type': row[3],
                        'original_content': row[4],
                        'new_content': row[5],
                        'proposed_content': proposed_content,
                        'confidence_score': confidence_score,
                        'reason': row[8],
                        'status': row[9],
                        'reviewer_comment': row[10],
                        'review_time': row[11],
                        'metadata': metadata  # æ·»åŠ metadataå­—æ®µ
                    })

                return reviews

        except Exception as e:
            self._logger.error(f"è·å–å¾…å®¡æŸ¥äººæ ¼å­¦ä¹ è®°å½•å¤±è´¥: {e}")
            return []

    async def update_persona_learning_review_status(self, review_id: int, status: str, comment: str = None, modified_content: str = None) -> bool:
        """æ›´æ–°äººæ ¼å­¦ä¹ å®¡æŸ¥çŠ¶æ€"""
        try:
            async with self.get_db_connection() as conn:
                cursor = await conn.cursor()

                # æ ¹æ®æ•°æ®åº“ç±»å‹ä½¿ç”¨ä¸åŒçš„å ä½ç¬¦
                placeholder = '%s' if self.config.db_type.lower() == 'mysql' else '?'

                # å¦‚æœæœ‰ä¿®æ”¹åçš„å†…å®¹ï¼Œä¹Ÿè¦æ›´æ–°proposed_contentå­—æ®µ
                if modified_content:
                    if self.config.db_type.lower() == 'mysql':
                        await cursor.execute(f'''
                            UPDATE persona_update_reviews
                            SET status = {placeholder}, reviewer_comment = {placeholder}, review_time = {placeholder},
                                proposed_content = {placeholder}, new_content = {placeholder}
                            WHERE id = {placeholder}
                        ''', (status, comment, time.time(), modified_content, modified_content, review_id))
                    else:
                        await cursor.execute('''
                            UPDATE persona_update_reviews
                            SET status = ?, reviewer_comment = ?, review_time = ?, proposed_content = ?, new_content = ?
                            WHERE id = ?
                        ''', (status, comment, time.time(), modified_content, modified_content, review_id))
                else:
                    if self.config.db_type.lower() == 'mysql':
                        await cursor.execute(f'''
                            UPDATE persona_update_reviews
                            SET status = {placeholder}, reviewer_comment = {placeholder}, review_time = {placeholder}
                            WHERE id = {placeholder}
                        ''', (status, comment, time.time(), review_id))
                    else:
                        await cursor.execute('''
                            UPDATE persona_update_reviews
                            SET status = ?, reviewer_comment = ?, review_time = ?
                            WHERE id = ?
                        ''', (status, comment, time.time(), review_id))

                await conn.commit()
                return cursor.rowcount > 0

        except Exception as e:
            self._logger.error(f"æ›´æ–°äººæ ¼å­¦ä¹ å®¡æŸ¥çŠ¶æ€å¤±è´¥: {e}")
            return False

    async def delete_persona_learning_review_by_id(self, review_id: int) -> bool:
        """åˆ é™¤æŒ‡å®šIDçš„äººæ ¼å­¦ä¹ å®¡æŸ¥è®°å½•"""
        try:
            async with self.get_db_connection() as conn:
                cursor = await conn.cursor()

                # æ ¹æ®æ•°æ®åº“ç±»å‹ä½¿ç”¨ä¸åŒçš„å ä½ç¬¦
                placeholder = '%s' if self.config.db_type.lower() == 'mysql' else '?'

                # åˆ é™¤å®¡æŸ¥è®°å½•
                await cursor.execute(f'''
                    DELETE FROM persona_update_reviews WHERE id = {placeholder}
                ''', (review_id,))

                await conn.commit()
                deleted_count = cursor.rowcount

                if deleted_count > 0:
                    self._logger.info(f"æˆåŠŸåˆ é™¤äººæ ¼å­¦ä¹ å®¡æŸ¥è®°å½•ï¼ŒID: {review_id}")
                    return True
                else:
                    self._logger.warning(f"æœªæ‰¾åˆ°è¦åˆ é™¤çš„äººæ ¼å­¦ä¹ å®¡æŸ¥è®°å½•ï¼ŒID: {review_id}")
                    return False

        except Exception as e:
            self._logger.error(f"åˆ é™¤äººæ ¼å­¦ä¹ å®¡æŸ¥è®°å½•å¤±è´¥: {e}")
            return False

    async def delete_all_persona_learning_reviews(self, group_id: Optional[str] = None) -> int:
        """
        æ‰¹é‡åˆ é™¤äººæ ¼å­¦ä¹ å®¡æŸ¥è®°å½•

        Args:
            group_id: ç¾¤ç»„IDï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚æœæŒ‡å®šåˆ™åªåˆ é™¤è¯¥ç¾¤ç»„çš„è®°å½•ï¼Œå¦åˆ™åˆ é™¤æ‰€æœ‰è®°å½•

        Returns:
            int: åˆ é™¤çš„è®°å½•æ•°é‡
        """
        try:
            async with self.get_db_connection() as conn:
                cursor = await conn.cursor()

                # æ ¹æ®æ•°æ®åº“ç±»å‹ä½¿ç”¨ä¸åŒçš„å ä½ç¬¦
                placeholder = '%s' if self.config.db_type.lower() == 'mysql' else '?'

                if group_id:
                    # åˆ é™¤æŒ‡å®šç¾¤ç»„çš„å®¡æŸ¥è®°å½•
                    await cursor.execute(f'''
                        DELETE FROM persona_update_reviews WHERE group_id = {placeholder}
                    ''', (group_id,))
                    self._logger.info(f"åˆ é™¤ç¾¤ç»„ {group_id} çš„æ‰€æœ‰äººæ ¼å­¦ä¹ å®¡æŸ¥è®°å½•")
                else:
                    # åˆ é™¤æ‰€æœ‰å®¡æŸ¥è®°å½•
                    await cursor.execute('''
                        DELETE FROM persona_update_reviews
                    ''')
                    self._logger.info("åˆ é™¤æ‰€æœ‰äººæ ¼å­¦ä¹ å®¡æŸ¥è®°å½•")

                await conn.commit()
                deleted_count = cursor.rowcount

                self._logger.info(f"âœ… æˆåŠŸåˆ é™¤ {deleted_count} æ¡äººæ ¼å­¦ä¹ å®¡æŸ¥è®°å½•")
                return deleted_count

        except Exception as e:
            self._logger.error(f"æ‰¹é‡åˆ é™¤äººæ ¼å­¦ä¹ å®¡æŸ¥è®°å½•å¤±è´¥: {e}")
            return 0
    
    async def get_persona_learning_review_by_id(self, review_id: int) -> Optional[Dict[str, Any]]:
        """è·å–æŒ‡å®šIDçš„äººæ ¼å­¦ä¹ å®¡æŸ¥è®°å½•è¯¦æƒ…"""
        try:
            async with self.get_db_connection() as conn:
                cursor = await conn.cursor()
            
            await cursor.execute('''
                SELECT id, group_id, original_content, new_content, proposed_content, 
                       confidence_score, reason, status, reviewer_comment, review_time, timestamp
                FROM persona_update_reviews
                WHERE id = ?
            ''', (review_id,))
            
            row = await cursor.fetchone()
            if row:
                return {
                    'id': row[0],
                    'group_id': row[1],
                    'original_content': row[2],
                    'new_content': row[3],
                    'proposed_content': row[4] if row[4] else row[3],  # proposed_contentæˆ–new_content
                    'confidence_score': row[5] if row[5] is not None else 0.5,
                    'reason': row[6],
                    'status': row[7],
                    'reviewer_comment': row[8],
                    'review_time': row[9],
                    'timestamp': row[10]
                }
            return None
            
        except Exception as e:
            self._logger.error(f"è·å–äººæ ¼å­¦ä¹ å®¡æŸ¥è®°å½•å¤±è´¥: {e}")
            return None

    async def save_style_learning_record(self, record_data: Dict[str, Any]) -> bool:
        """ä¿å­˜é£æ ¼å­¦ä¹ è®°å½•åˆ°æ•°æ®åº“"""
        try:
            async with self.get_db_connection() as conn:
                cursor = await conn.cursor()
            
            await cursor.execute('''
                INSERT INTO style_learning_records 
                (style_type, learned_patterns, confidence_score, sample_count, group_id, learning_time)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                record_data.get('style_type'),
                record_data.get('learned_patterns'),
                record_data.get('confidence_score'),
                record_data.get('sample_count'),
                record_data.get('group_id'),
                record_data.get('learning_time')
            ))
            
            await conn.commit()
            return True
            
        except Exception as e:
            self._logger.error(f"ä¿å­˜é£æ ¼å­¦ä¹ è®°å½•å¤±è´¥: {e}")
            return False

    async def save_language_style_pattern(self, pattern_data: Dict[str, Any]) -> bool:
        """ä¿å­˜è¯­è¨€é£æ ¼æ¨¡å¼åˆ°æ•°æ®åº“"""
        try:
            async with self.get_db_connection() as conn:
                cursor = await conn.cursor()
            
            # å…ˆæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒçš„è¯­è¨€é£æ ¼
            await cursor.execute('''
                SELECT id FROM language_style_patterns 
                WHERE language_style = ? AND group_id = ?
            ''', (pattern_data.get('language_style'), pattern_data.get('group_id')))
            
            existing = await cursor.fetchone()
            
            if existing:
                # æ›´æ–°ç°æœ‰è®°å½•
                await cursor.execute('''
                    UPDATE language_style_patterns 
                    SET example_phrases = ?, usage_frequency = ?, context_type = ?, last_updated = ?
                    WHERE id = ?
                ''', (
                    pattern_data.get('example_phrases'),
                    pattern_data.get('usage_frequency'),
                    pattern_data.get('context_type'),
                    pattern_data.get('last_updated'),
                    existing[0]
                ))
            else:
                # æ’å…¥æ–°è®°å½•
                await cursor.execute('''
                    INSERT INTO language_style_patterns 
                    (language_style, example_phrases, usage_frequency, context_type, group_id, last_updated)
                    VALUES (?, ?, ?, ?, ?, ?)
                ''', (
                    pattern_data.get('language_style'),
                    pattern_data.get('example_phrases'),
                    pattern_data.get('usage_frequency'),
                    pattern_data.get('context_type'),
                    pattern_data.get('group_id'),
                    pattern_data.get('last_updated')
                ))
            
            await conn.commit()
            return True
            
        except Exception as e:
            self._logger.error(f"ä¿å­˜è¯­è¨€é£æ ¼æ¨¡å¼å¤±è´¥: {e}")
            return False

    async def get_reviewed_persona_learning_updates(self, limit: int = 50, offset: int = 0, status_filter: str = None) -> List[Dict[str, Any]]:
        """è·å–å·²å®¡æŸ¥çš„äººæ ¼å­¦ä¹ æ›´æ–°è®°å½•"""
        try:
            async with self.get_db_connection() as conn:
                cursor = await conn.cursor()
            
            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            where_clause = "WHERE status != 'pending'"
            params = []
            
            if status_filter:
                where_clause += " AND status = ?"
                params.append(status_filter)
            
            # é¦–å…ˆæ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨å¹¶è·å–è¡¨ç»“æ„
            await cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='persona_update_reviews'")
            table_exists = await cursor.fetchone()
            
            if not table_exists:
                self._logger.info("persona_update_reviewsè¡¨ä¸å­˜åœ¨ï¼Œè¿”å›ç©ºåˆ—è¡¨")
                return []
            
            # æ£€æŸ¥è¡¨ç»“æ„ï¼Œç¡®å®šæ­£ç¡®çš„å­—æ®µå
            await cursor.execute("PRAGMA table_info(persona_update_reviews)")
            columns = await cursor.fetchall()
            column_names = [col[1] for col in columns]
            
            # æ ¹æ®å®é™…çš„åˆ—åæ„å»ºæŸ¥è¯¢
            if 'proposed_content' in column_names:
                content_field = 'proposed_content'
            elif 'new_content' in column_names:
                content_field = 'new_content'
            else:
                # å¦‚æœä¸¤ä¸ªå­—æ®µéƒ½ä¸å­˜åœ¨ï¼Œä½¿ç”¨åŸå§‹å†…å®¹
                content_field = 'original_content'

            # æ£€æŸ¥æ˜¯å¦æœ‰metadataåˆ—
            has_metadata = 'metadata' in column_names

            # ä½¿ç”¨å®é™…å­˜åœ¨çš„å­—æ®µè¿›è¡ŒæŸ¥è¯¢ï¼Œå¹¶å¤„ç†NULLå€¼
            metadata_field = ', metadata' if has_metadata else ''
            await cursor.execute(f'''
                SELECT id, group_id, original_content, {content_field}, reason,
                       status, reviewer_comment, review_time, timestamp{metadata_field}
                FROM persona_update_reviews
                {where_clause}
                ORDER BY COALESCE(review_time, timestamp) DESC
                LIMIT ? OFFSET ?
            ''', params + [limit, offset])

            rows = await cursor.fetchall()
            updates = []

            import json
            for row in rows:
                # è§£æmetadataï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                metadata = {}
                if has_metadata and len(row) > 9 and row[9]:
                    try:
                        metadata = json.loads(row[9])
                    except:
                        metadata = {}

                updates.append({
                    'id': f"persona_learning_{row[0]}",
                    'group_id': row[1] or 'default',
                    'original_content': row[2] or '',
                    'proposed_content': row[3] or '',  # ä½¿ç”¨å®é™…å­˜åœ¨çš„å­—æ®µ
                    'reason': row[4] or 'äººæ ¼å­¦ä¹ æ›´æ–°',
                    'confidence_score': metadata.get('confidence_score', 0.8),  # ä»metadataè·å–æˆ–ä½¿ç”¨é»˜è®¤å€¼
                    'status': row[5],
                    'reviewer_comment': row[6] or '',
                    'review_time': row[7] if row[7] else 0,
                    'timestamp': row[8] if row[8] else 0,
                    'update_type': 'persona_learning_review',
                    # æ·»åŠ metadataä¸­çš„å…³é”®å­—æ®µ
                    'features_content': metadata.get('features_content', ''),
                    'llm_response': metadata.get('llm_response', ''),
                    'total_raw_messages': metadata.get('total_raw_messages', 0),
                    'messages_analyzed': metadata.get('messages_analyzed', 0),
                    'metadata': metadata
                })
            
            return updates
            
        except Exception as e:
            self._logger.error(f"è·å–å·²å®¡æŸ¥äººæ ¼å­¦ä¹ è®°å½•å¤±è´¥: {e}")
            # å¦‚æœæ˜¯è¡¨æˆ–åˆ—ä¸å­˜åœ¨çš„é”™è¯¯ï¼Œè¿”å›ç©ºåˆ—è¡¨
            if "no such table" in str(e).lower() or "no such column" in str(e).lower():
                self._logger.info("äººæ ¼å­¦ä¹ å®¡æŸ¥è¡¨æˆ–å­—æ®µä¸å­˜åœ¨ï¼Œè¿”å›ç©ºåˆ—è¡¨")
                return []
            return []

    async def get_reviewed_style_learning_updates(self, limit: int = 50, offset: int = 0, status_filter: str = None) -> List[Dict[str, Any]]:
        """è·å–å·²å®¡æŸ¥çš„é£æ ¼å­¦ä¹ æ›´æ–°è®°å½•"""
        try:
            async with self.get_db_connection() as conn:
                cursor = await conn.cursor()
            
            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            where_clause = "WHERE status != 'pending'"
            params = []
            
            if status_filter:
                where_clause += " AND status = ?"
                params.append(status_filter)
            
            # ä½¿ç”¨æ­£ç¡®çš„å­—æ®µåï¼Œæ²¡æœ‰review_timeå­—æ®µï¼Œä½¿ç”¨updated_atï¼Œå¹¶å¤„ç†NULLå€¼
            await cursor.execute(f'''
                SELECT id, type, group_id, timestamp, learned_patterns, status, updated_at, description
                FROM style_learning_reviews
                {where_clause}
                ORDER BY COALESCE(updated_at, timestamp) DESC
                LIMIT ? OFFSET ?
            ''', params + [limit, offset])
            
            rows = await cursor.fetchall()
            updates = []
            
            for row in rows:
                # æ·»åŠ è¡Œæ•°æ®éªŒè¯
                try:
                    if len(row) < 8:
                        self._logger.warning(f"é£æ ¼å­¦ä¹ è®°å½•è¡Œæ•°æ®ä¸å®Œæ•´ï¼Œè·³è¿‡: {row}")
                        continue

                    # å°è¯•è§£ælearned_patternsä»¥è·å–æ›´å¤šä¿¡æ¯
                    try:
                        learned_patterns = json.loads(row[4]) if row[4] else {}
                        reason = learned_patterns.get('reason', 'é£æ ¼å­¦ä¹ æ›´æ–°')
                        original_content = learned_patterns.get('original_content', 'åŸå§‹é£æ ¼ç‰¹å¾')
                        proposed_content = learned_patterns.get('proposed_content', row[4])  # ä½¿ç”¨å®Œæ•´çš„learned_patternsä½œä¸ºproposed_content
                        confidence_score = learned_patterns.get('confidence_score', 0.8)
                    except (json.JSONDecodeError, AttributeError):
                        reason = row[7] if len(row) > 7 and row[7] else 'é£æ ¼å­¦ä¹ æ›´æ–°'  # ä½¿ç”¨descriptionå­—æ®µ
                        original_content = 'åŸå§‹é£æ ¼ç‰¹å¾'
                        proposed_content = row[4] if len(row) > 4 and row[4] else 'æ— å†…å®¹'
                        confidence_score = 0.8

                    updates.append({
                        'id': row[0],
                        'group_id': row[2],
                        'original_content': original_content,
                        'proposed_content': proposed_content,
                        'reason': reason,
                        'confidence_score': confidence_score,
                        'status': row[5],
                        'reviewer_comment': '',  # é£æ ¼å®¡æŸ¥æ²¡æœ‰å¤‡æ³¨å­—æ®µ
                        'review_time': row[6] if len(row) > 6 else None,  # ä½¿ç”¨updated_atå­—æ®µ
                        'timestamp': row[3],
                        'update_type': f'style_learning_{row[1]}'
                    })
                except Exception as row_error:
                    self._logger.warning(f"å¤„ç†é£æ ¼å­¦ä¹ è®°å½•è¡Œæ—¶å‡ºé”™ï¼Œè·³è¿‡: {row_error}, row: {row if len(row) < 20 else 'too long'}")
            
            return updates
            
        except Exception as e:
            self._logger.error(f"è·å–å·²å®¡æŸ¥é£æ ¼å­¦ä¹ è®°å½•å¤±è´¥: {e}")
            # å¦‚æœè¡¨ä¸å­˜åœ¨ï¼Œè¿”å›ç©ºåˆ—è¡¨
            if "no such table" in str(e).lower():
                self._logger.info("é£æ ¼å­¦ä¹ å®¡æŸ¥è¡¨ä¸å­˜åœ¨ï¼Œè¿”å›ç©ºåˆ—è¡¨")
                return []
            return []

    async def get_reviewed_persona_update_records(self, limit: int = 50, offset: int = 0, status_filter: str = None) -> List[Dict[str, Any]]:
        """è·å–å·²å®¡æŸ¥çš„ä¼ ç»Ÿäººæ ¼æ›´æ–°è®°å½•"""
        try:
            async with self.get_db_connection() as conn:
                cursor = await conn.cursor()

            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            where_clause = "WHERE status != 'pending'"
            params = []

            if status_filter:
                where_clause += " AND status = ?"
                params.append(status_filter)

            query = f'''
                SELECT id, timestamp, group_id, update_type, original_content, new_content,
                       reason, status, reviewer_comment, review_time
                FROM persona_update_records
                {where_clause}
                ORDER BY COALESCE(review_time, timestamp) DESC
                LIMIT ? OFFSET ?
            '''

            self._logger.debug(f"æ‰§è¡Œäººæ ¼æ›´æ–°è®°å½•æŸ¥è¯¢: params={params + [limit, offset]}")
            await cursor.execute(query, params + [limit, offset])
            
            rows = await cursor.fetchall()
            records = []

            for row in rows:
                # æ·»åŠ è¡Œæ•°æ®éªŒè¯
                try:
                    if len(row) < 10:
                        self._logger.warning(f"äººæ ¼æ›´æ–°è®°å½•è¡Œæ•°æ®ä¸å®Œæ•´ (æœŸæœ›10ä¸ªå­—æ®µï¼Œå®é™…{len(row)}ä¸ª)ï¼Œè·³è¿‡: {row}")
                        continue

                    records.append({
                        'id': row[0],
                        'timestamp': row[1],
                        'group_id': row[2],
                        'update_type': row[3],
                        'original_content': row[4],
                        'new_content': row[5],
                        'reason': row[6],
                        'status': row[7],
                        'reviewer_comment': row[8] if row[8] else '',
                        'review_time': row[9]
                    })
                except Exception as row_error:
                    self._logger.warning(f"å¤„ç†äººæ ¼æ›´æ–°è®°å½•è¡Œæ—¶å‡ºé”™ï¼Œè·³è¿‡: {row_error}, row: {row if len(row) < 20 else 'too long'}")
            
            return records
            
        except Exception as e:
            self._logger.error(f"è·å–å·²å®¡æŸ¥ä¼ ç»Ÿäººæ ¼æ›´æ–°è®°å½•å¤±è´¥: {e}")
            return []

    async def update_style_review_status(self, review_id: int, status: str, group_id: str = None) -> bool:
        """æ›´æ–°é£æ ¼å­¦ä¹ å®¡æŸ¥çŠ¶æ€"""
        try:
            async with self.get_db_connection() as conn:
                cursor = await conn.cursor()

                await cursor.execute('''
                    UPDATE style_learning_reviews
                    SET status = ?, updated_at = ?
                    WHERE id = ?
                ''', (status, time.time(), review_id))

                await conn.commit()

                if cursor.rowcount > 0:
                    self._logger.info(f"æ›´æ–°é£æ ¼å­¦ä¹ å®¡æŸ¥çŠ¶æ€æˆåŠŸ: ID={review_id}, çŠ¶æ€={status}")
                    return True
                else:
                    self._logger.warning(f"æ›´æ–°é£æ ¼å­¦ä¹ å®¡æŸ¥çŠ¶æ€å¤±è´¥: æœªæ‰¾åˆ°ID={review_id}çš„è®°å½•")
                    return False

        except Exception as e:
            self._logger.error(f"æ›´æ–°é£æ ¼å­¦ä¹ å®¡æŸ¥çŠ¶æ€å¤±è´¥: {e}")
            return False

    async def delete_style_review_by_id(self, review_id: int) -> bool:
        """åˆ é™¤æŒ‡å®šIDçš„é£æ ¼å­¦ä¹ å®¡æŸ¥è®°å½•"""
        try:
            async with self.get_db_connection() as conn:
                cursor = await conn.cursor()

                # åˆ é™¤å®¡æŸ¥è®°å½•
                await cursor.execute('''
                    DELETE FROM style_learning_reviews WHERE id = ?
                ''', (review_id,))

                await conn.commit()
                deleted_count = cursor.rowcount

                await cursor.close()

                if deleted_count > 0:
                    self._logger.info(f"æˆåŠŸåˆ é™¤é£æ ¼å­¦ä¹ å®¡æŸ¥è®°å½•ï¼ŒID: {review_id}")
                    return True
                else:
                    self._logger.warning(f"æœªæ‰¾åˆ°è¦åˆ é™¤çš„é£æ ¼å­¦ä¹ å®¡æŸ¥è®°å½•ï¼ŒID: {review_id}")
                    return False

        except Exception as e:
            self._logger.error(f"åˆ é™¤é£æ ¼å­¦ä¹ å®¡æŸ¥è®°å½•å¤±è´¥: {e}")
            return False

    async def get_detailed_metrics(self) -> Dict[str, Any]:
        """è·å–è¯¦ç»†æ€§èƒ½ç›‘æ§æ•°æ®"""
        try:
            async with self.get_db_connection() as conn:
                cursor = await conn.cursor()
            
            # APIæŒ‡æ ‡ï¼ˆåŸºäºå­¦ä¹ æ‰¹æ¬¡çš„æ‰§è¡Œæ—¶é—´ï¼‰
            # âœ… ä¿®å¤ï¼šä½¿ç”¨æ•°æ®åº“æ— å…³çš„æ—¶é—´æ ¼å¼åŒ–æ–¹å¼
            if self.config.db_type == 'sqlite':  # âœ… ä¿®æ­£ï¼šself.db_type â†’ self.config.db_type
                # SQLiteè¯­æ³•
                await cursor.execute('''
                    SELECT
                        strftime('%H', datetime(start_time, 'unixepoch')) as hour,
                        AVG((CASE WHEN end_time IS NOT NULL THEN end_time - start_time ELSE 0 END)) as avg_response_time
                    FROM learning_batches
                    WHERE start_time > ? AND end_time IS NOT NULL
                    GROUP BY hour
                    ORDER BY hour
                ''', (time.time() - 86400,))
            else:
                # MySQLè¯­æ³•
                await cursor.execute('''
                    SELECT
                        HOUR(FROM_UNIXTIME(start_time)) as hour,
                        AVG((CASE WHEN end_time IS NOT NULL THEN end_time - start_time ELSE 0 END)) as avg_response_time
                    FROM learning_batches
                    WHERE start_time > %s AND end_time IS NOT NULL
                    GROUP BY hour
                    ORDER BY hour
                ''', (time.time() - 86400,))
            
            api_hours = []
            api_response_times = []
            for row in await cursor.fetchall():
                api_hours.append(f"{row[0]}:00")
                api_response_times.append(round(row[1] * 1000, 2))  # è½¬æ¢ä¸ºæ¯«ç§’
            
            # æ•°æ®åº“è¡¨ç»Ÿè®¡
            tables_to_check = ['raw_messages', 'filtered_messages', 'learning_batches', 'persona_update_records']
            table_stats = {}
            
            for table in tables_to_check:
                try:
                    await cursor.execute(f'SELECT COUNT(*) FROM {table}')
                    count = await cursor.fetchone()
                    table_stats[table] = count[0] if count else 0
                except Exception as table_error:
                    self._logger.debug(f"æ— æ³•è·å–è¡¨ {table} ç»Ÿè®¡: {table_error}")
                    table_stats[table] = 0
            
            # ç³»ç»ŸæŒ‡æ ‡
            import psutil
            try:
                memory = psutil.virtual_memory()
                # åœ¨Windowsä¸Šä½¿ç”¨ä¸»é©±åŠ¨å™¨
                disk_path = 'C:\\' if os.name == 'nt' else '/'
                disk = psutil.disk_usage(disk_path)
                
                system_metrics = {
                    'memory_percent': memory.percent,
                    'cpu_percent': psutil.cpu_percent(),
                    'disk_percent': round(disk.used / disk.total * 100, 2)
                }
            except Exception as system_error:
                self._logger.warning(f"è·å–ç³»ç»ŸæŒ‡æ ‡å¤±è´¥: {system_error}")
                system_metrics = {
                    'memory_percent': 0,
                    'cpu_percent': 0,
                    'disk_percent': 0
                }
            
            return {
                'api_metrics': {
                    'hours': api_hours,
                    'response_times': api_response_times
                },
                'database_metrics': {
                    'table_stats': table_stats
                },
                'system_metrics': system_metrics
            }
            
        except Exception as e:
            self._logger.error(f"è·å–è¯¦ç»†ç›‘æ§æ•°æ®å¤±è´¥: {e}")
            return {
                'api_metrics': {
                    'hours': [],
                    'response_times': []
                },
                'database_metrics': {
                    'table_stats': {}
                },
                'system_metrics': {
                    'memory_percent': 0,
                    'cpu_percent': 0,
                    'disk_percent': 0
                }
            }

    async def get_trends_data(self) -> Dict[str, Any]:
        """è·å–æŒ‡æ ‡è¶‹åŠ¿æ•°æ®"""
        try:
            async with self.get_db_connection() as conn:
                cursor = await conn.cursor()
            
            # è®¡ç®—7å¤©å’Œ30å¤©å‰çš„æ—¶é—´æˆ³
            now = time.time()
            week_ago = now - (7 * 24 * 3600)
            month_ago = now - (30 * 24 * 3600)
            
            # æ¶ˆæ¯å¢é•¿è¶‹åŠ¿
            await cursor.execute('''
                SELECT 
                    COUNT(CASE WHEN timestamp > ? THEN 1 END) as week_count,
                    COUNT(CASE WHEN timestamp > ? THEN 1 END) as month_count,
                    COUNT(*) as total_count
                FROM raw_messages
            ''', (week_ago, month_ago))
            
            message_stats = await cursor.fetchone()
            if message_stats and len(message_stats) >= 3:
                week_messages = int(message_stats[0]) if message_stats[0] else 0
                month_messages = int(message_stats[1]) if message_stats[1] else 0
                total_messages = int(message_stats[2]) if message_stats[2] else 0

                # è®¡ç®—å¢é•¿ç‡
                if month_messages > week_messages:
                    message_growth = ((week_messages * 4 - (month_messages - week_messages)) / (month_messages - week_messages) * 100) if (month_messages - week_messages) > 0 else 0
                else:
                    message_growth = 0
            elif message_stats:
                self._logger.warning(f"æ¶ˆæ¯ç»Ÿè®¡æ•°æ®è¡Œä¸å®Œæ•´ (æœŸæœ›3ä¸ªå­—æ®µï¼Œå®é™…{len(message_stats)}ä¸ª): {message_stats}")
                message_growth = 0
                week_messages = 0
                month_messages = 0
                total_messages = 0
            else:
                message_growth = 0
                week_messages = 0
                month_messages = 0
                total_messages = 0
            
            # ç­›é€‰æ¶ˆæ¯å¢é•¿è¶‹åŠ¿
            await cursor.execute('''
                SELECT 
                    COUNT(CASE WHEN timestamp > ? THEN 1 END) as week_filtered,
                    COUNT(CASE WHEN timestamp > ? THEN 1 END) as month_filtered
                FROM filtered_messages
            ''', (week_ago, month_ago))
            
            filtered_stats = await cursor.fetchone()
            if filtered_stats and len(filtered_stats) >= 2:
                week_filtered = int(filtered_stats[0]) if filtered_stats[0] else 0
                month_filtered = int(filtered_stats[1]) if filtered_stats[1] else 0

                # è®¡ç®—å¢é•¿ç‡
                if month_filtered > week_filtered:
                    filtered_growth = ((week_filtered * 4 - (month_filtered - week_filtered)) / (month_filtered - week_filtered) * 100) if (month_filtered - week_filtered) > 0 else 0
                else:
                    filtered_growth = 0
            elif filtered_stats:
                self._logger.warning(f"ç­›é€‰æ¶ˆæ¯ç»Ÿè®¡æ•°æ®è¡Œä¸å®Œæ•´ (æœŸæœ›2ä¸ªå­—æ®µï¼Œå®é™…{len(filtered_stats)}ä¸ª): {filtered_stats}")
                week_filtered = 0
                month_filtered = 0
                filtered_growth = 0
            else:
                week_filtered = 0
                month_filtered = 0
                filtered_growth = 0
            
            # LLMè°ƒç”¨å¢é•¿ï¼ˆåŸºäºå­¦ä¹ æ‰¹æ¬¡ï¼‰
            await cursor.execute('''
                SELECT 
                    COUNT(CASE WHEN start_time > ? THEN 1 END) as week_sessions,
                    COUNT(CASE WHEN start_time > ? THEN 1 END) as month_sessions
                FROM learning_batches
            ''', (week_ago, month_ago))
            
            session_stats = await cursor.fetchone()
            if session_stats and len(session_stats) >= 2:
                week_sessions = int(session_stats[0]) if session_stats[0] else 0
                month_sessions = int(session_stats[1]) if session_stats[1] else 0

                # è®¡ç®—å¢é•¿ç‡
                if month_sessions > week_sessions:
                    sessions_growth = ((week_sessions * 4 - (month_sessions - week_sessions)) / (month_sessions - week_sessions) * 100) if (month_sessions - week_sessions) > 0 else 0
                else:
                    sessions_growth = 0
            elif session_stats:
                self._logger.warning(f"å­¦ä¹ æ‰¹æ¬¡ç»Ÿè®¡æ•°æ®è¡Œä¸å®Œæ•´ (æœŸæœ›2ä¸ªå­—æ®µï¼Œå®é™…{len(session_stats)}ä¸ª): {session_stats}")
                week_sessions = 0
                month_sessions = 0
                sessions_growth = 0
            else:
                week_sessions = 0
                month_sessions = 0
                sessions_growth = 0
            
            return {
                'message_growth': round(message_growth, 1),
                'filtered_growth': round(filtered_growth, 1),
                'llm_growth': round(sessions_growth, 1),
                'sessions_growth': round(sessions_growth, 1)
            }
            
        except Exception as e:
            self._logger.error(f"è·å–è¶‹åŠ¿æ•°æ®å¤±è´¥: {e}")
            return {
                'message_growth': 0,
                'filtered_growth': 0,
                'llm_growth': 0,
                'sessions_growth': 0
            }

    def _analyze_topic_from_messages(self, messages: List[str]) -> Dict[str, str]:
        """
        åŸºäºæ¶ˆæ¯å†…å®¹æ™ºèƒ½åˆ†æç¾¤èŠä¸»é¢˜
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            åŒ…å«topicå’Œstyleçš„å­—å…¸
        """
        try:
            if not messages:
                return {'topic': 'ç©ºç¾¤èŠ', 'style': 'unknown'}
            
            # åˆå¹¶æ‰€æœ‰æ¶ˆæ¯æ–‡æœ¬
            all_text = ' '.join(messages).lower()
            
            # å®šä¹‰ä¸»é¢˜å…³é”®è¯åº“
            topic_keywords = {
                'æŠ€æœ¯è®¨è®º': ['ä»£ç ', 'ç¼–ç¨‹', 'python', 'java', 'javascript', 'bug', 'ç®—æ³•', 'å¼€å‘', 'å‰ç«¯', 'åç«¯', 'api', 'æ•°æ®åº“', 'sql', 'git', 'é¡¹ç›®', 'éœ€æ±‚', 'æµ‹è¯•', 'éƒ¨ç½²'],
                'æ¸¸æˆå¨±ä¹': ['æ¸¸æˆ', 'ç©å®¶', 'æ”»ç•¥', 'è£…å¤‡', 'å‰¯æœ¬', 'å…¬ä¼š', 'pvp', 'è§’è‰²', 'æŠ€èƒ½', 'ç­‰çº§', 'ç»éªŒ', 'ä»»åŠ¡', 'æ´»åŠ¨', 'å……å€¼', 'æŠ½å¡', 'å¼€é»‘', 'ä¸Šåˆ†'],
                'å­¦ä¹ äº¤æµ': ['å­¦ä¹ ', 'ä½œä¸š', 'è€ƒè¯•', 'å¤ä¹ ', 'ç¬”è®°', 'è¯¾ç¨‹', 'è€å¸ˆ', 'åŒå­¦', 'çŸ¥è¯†', 'é—®é¢˜', 'ç­”æ¡ˆ', 'æ•™ç¨‹', 'èµ„æ–™', 'ä¹¦ç±', 'è®ºæ–‡', 'ç ”ç©¶'],
                'å·¥ä½œåä½œ': ['å·¥ä½œ', 'ä¼šè®®', 'é¡¹ç›®', 'ä»»åŠ¡', 'è¿›åº¦', 'æ±‡æŠ¥', 'å®¢æˆ·', 'åˆä½œ', 'å›¢é˜Ÿ', 'é¢†å¯¼', 'åŒäº‹', 'ä¸šåŠ¡', 'æ–¹æ¡ˆ', 'æ–‡æ¡£', 'æµç¨‹', 'å®¡æ‰¹'],
                'ç”Ÿæ´»æ—¥å¸¸': ['åƒé¥­', 'ç¡è§‰', 'å¤©æ°”', 'å¿ƒæƒ…', 'å®¶äºº', 'æœ‹å‹', 'è´­ç‰©', 'ç”µå½±', 'éŸ³ä¹', 'æ—…æ¸¸', 'ç¾é£Ÿ', 'å¥åº·', 'è¿åŠ¨', 'ä¼‘æ¯', 'å‘¨æœ«'],
                'å…´è¶£çˆ±å¥½': ['æ‘„å½±', 'ç»˜ç”»', 'éŸ³ä¹', 'ç”µå½±', 'ä¹¦ç±', 'æ—…è¡Œ', 'ç¾é£Ÿ', 'è¿åŠ¨', 'å¥èº«', 'ç‘œä¼½', 'è·‘æ­¥', 'éª‘è¡Œ', 'çˆ¬å±±', 'æ¸¸æ³³', 'ç¯®çƒ'],
                'å•†åŠ¡åˆä½œ': ['åˆä½œ', 'å•†åŠ¡', 'ä¸šåŠ¡', 'å®¢æˆ·', 'é¡¹ç›®', 'æ–¹æ¡ˆ', 'æŠ¥ä»·', 'åˆåŒ', 'ä»˜æ¬¾', 'å‘ç¥¨', 'äº§å“', 'æœåŠ¡', 'å¸‚åœº', 'é”€å”®', 'æ¨å¹¿'],
                'æŠ€æœ¯æ”¯æŒ': ['é—®é¢˜', 'æ•…éšœ', 'é”™è¯¯', 'ä¿®å¤', 'è§£å†³', 'å¸®åŠ©', 'æ”¯æŒ', 'æ•™ç¨‹', 'æŒ‡å¯¼', 'æ“ä½œ', 'é…ç½®', 'å®‰è£…', 'æ›´æ–°', 'ç»´æŠ¤', 'ä¼˜åŒ–'],
                'é—²èŠçŒæ°´': ['å“ˆå“ˆ', 'å˜¿å˜¿', 'ğŸ˜‚', 'ğŸ˜„', 'ç¬‘æ­»', 'æœ‰è¶£', 'æ— èŠ', 'éšä¾¿', 'èŠå¤©', 'æ‰¯æ·¡', 'åæ§½', 'æç¬‘', 'æ®µå­', 'è¡¨æƒ…', 'å‘å‘†'],
                'é€šçŸ¥å…¬å‘Š': ['é€šçŸ¥', 'å…¬å‘Š', 'é‡è¦', 'æ³¨æ„', 'æé†’', 'æˆªæ­¢', 'æ—¶é—´', 'å®‰æ’', 'æ´»åŠ¨', 'æŠ¥å', 'å‚åŠ ', 'ä¼šè®®', 'åŸ¹è®­', 'è®²åº§', 'æ´»åŠ¨']
            }
            
            # åˆ†æä¸»é¢˜åŒ¹é…åº¦
            topic_scores = {}
            for topic, keywords in topic_keywords.items():
                score = 0
                for keyword in keywords:
                    score += all_text.count(keyword)
                topic_scores[topic] = score
            
            # è·å–å¾—åˆ†æœ€é«˜çš„ä¸»é¢˜
            best_topic = max(topic_scores.items(), key=lambda x: x[1])
            
            if best_topic[1] == 0:  # æ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•å…³é”®è¯
                return {'topic': 'ç»¼åˆèŠå¤©', 'style': 'æ—¥å¸¸å¯¹è¯'}
            
            # æ ¹æ®ä¸»é¢˜ç¡®å®šå¯¹è¯é£æ ¼
            style_mapping = {
                'æŠ€æœ¯è®¨è®º': 'æŠ€æœ¯äº¤æµ',
                'æ¸¸æˆå¨±ä¹': 'è½»æ¾å¨±ä¹', 
                'å­¦ä¹ äº¤æµ': 'å­¦æœ¯è®¨è®º',
                'å·¥ä½œåä½œ': 'å·¥ä½œåè°ƒ',
                'ç”Ÿæ´»æ—¥å¸¸': 'æ—¥å¸¸é—²èŠ',
                'å…´è¶£çˆ±å¥½': 'å…´è¶£åˆ†äº«',
                'å•†åŠ¡åˆä½œ': 'å•†åŠ¡æ²Ÿé€š',
                'æŠ€æœ¯æ”¯æŒ': 'æŠ€æœ¯ç­”ç–‘',
                'é—²èŠçŒæ°´': 'è½»æ¾èŠå¤©',
                'é€šçŸ¥å…¬å‘Š': 'ä¿¡æ¯é€šçŸ¥'
            }
            
            topic = best_topic[0]
            style = style_mapping.get(topic, 'æ—¥å¸¸å¯¹è¯')
            
            return {
                'topic': topic,
                'style': style
            }
            
        except Exception as e:
            self._logger.error(f"ä¸»é¢˜åˆ†æå¤±è´¥: {e}")
            return {'topic': 'æœªçŸ¥ä¸»é¢˜', 'style': 'æ—¥å¸¸å¯¹è¯'}

    async def get_recent_learning_batches(self, limit: int = 10) -> List[Dict[str, Any]]:
        """è·å–æœ€è¿‘çš„å­¦ä¹ æ‰¹æ¬¡è®°å½•"""
        try:
            async with self.get_db_connection() as conn:
                cursor = await conn.cursor()
            
            await cursor.execute('''
                SELECT id, group_id, start_time, end_time, quality_score,
                       processed_messages, batch_name, message_count, 
                       filtered_count, success, error_message
                FROM learning_batches 
                ORDER BY start_time DESC 
                LIMIT ?
            ''', (limit,))
            
            batches = []
            for row in await cursor.fetchall():
                try:
                    # æ·»åŠ è¡Œæ•°æ®éªŒè¯
                    if len(row) < 11:
                        self._logger.warning(f"å­¦ä¹ æ‰¹æ¬¡è®°å½•è¡Œæ•°æ®ä¸å®Œæ•´ (æœŸæœ›11ä¸ªå­—æ®µï¼Œå®é™…{len(row)}ä¸ª)ï¼Œè·³è¿‡: {row}")
                        continue

                    batches.append({
                        'id': int(row[0]) if row[0] else 0,
                        'group_id': row[1],
                        'start_time': float(row[2]) if row[2] else 0,
                        'end_time': float(row[3]) if row[3] else 0,
                        'quality_score': float(row[4]) if row[4] else 0,
                        'processed_messages': int(row[5]) if row[5] else 0,
                        'batch_name': row[6],
                        'message_count': int(row[7]) if row[7] else 0,
                        'filtered_count': int(row[8]) if row[8] else 0,
                        'success': bool(row[9]) if row[9] is not None else False,
                        'error_message': row[10]
                    })
                except Exception as row_error:
                    self._logger.warning(f"å¤„ç†å­¦ä¹ æ‰¹æ¬¡è®°å½•è¡Œæ—¶å‡ºé”™ï¼Œè·³è¿‡: {row_error}, row: {row if len(str(row)) < 100 else 'row too long'}")
                    continue
            
            return batches

        except Exception as e:
            self._logger.error(f"è·å–å­¦ä¹ æ‰¹æ¬¡è®°å½•å¤±è´¥: {e}")
            return []

    async def add_persona_learning_review(
        self,
        group_id: str,
        proposed_content: str,
        learning_source: str = UPDATE_TYPE_EXPRESSION_LEARNING,  # âœ… ä½¿ç”¨å¸¸é‡ä½œä¸ºé»˜è®¤å€¼
        confidence_score: float = 0.5,
        raw_analysis: str = "",
        metadata: Dict[str, Any] = None,
        original_content: str = "",  # âœ… æ–°å¢ï¼šåŸäººæ ¼å®Œæ•´æ–‡æœ¬
        new_content: str = ""  # âœ… æ–°å¢ï¼šæ–°äººæ ¼å®Œæ•´æ–‡æœ¬ï¼ˆåŸäººæ ¼+å¢é‡ï¼‰
    ) -> int:
        """æ·»åŠ äººæ ¼å­¦ä¹ å®¡æŸ¥è®°å½•

        Args:
            group_id: ç¾¤ç»„ID
            proposed_content: å»ºè®®çš„å¢é‡äººæ ¼å†…å®¹
            learning_source: å­¦ä¹ æ¥æº
            confidence_score: ç½®ä¿¡åº¦åˆ†æ•°
            raw_analysis: åŸå§‹åˆ†æç»“æœ
            metadata: å…ƒæ•°æ®(åŒ…å«features_content, llm_response, sample countsç­‰)
            original_content: åŸäººæ ¼å®Œæ•´æ–‡æœ¬ï¼ˆç”¨äºå‰ç«¯æ˜¾ç¤ºå¯¹æ¯”ï¼‰
            new_content: æ–°äººæ ¼å®Œæ•´æ–‡æœ¬ï¼ˆåŸäººæ ¼+å¢é‡ï¼Œç”¨äºå‰ç«¯é«˜äº®æ˜¾ç¤ºï¼‰

        Returns:
            æ’å…¥è®°å½•çš„ID
        """
        try:
            async with self.get_db_connection() as conn:
                cursor = await conn.cursor()

                # ç¡®ä¿è¡¨å­˜åœ¨å¹¶æ·»åŠ metadataåˆ—
                # æ ¹æ®æ•°æ®åº“ç±»å‹ä½¿ç”¨ä¸åŒçš„DDL
                if self.config.db_type.lower() == 'mysql':
                    await cursor.execute('''
                        CREATE TABLE IF NOT EXISTS persona_update_reviews (
                            id INT PRIMARY KEY AUTO_INCREMENT,
                            timestamp DOUBLE NOT NULL,
                            group_id VARCHAR(255) NOT NULL,
                            update_type VARCHAR(100) NOT NULL,
                            original_content TEXT,
                            new_content TEXT,
                            proposed_content TEXT,
                            confidence_score DOUBLE,
                            reason TEXT,
                            status VARCHAR(50) NOT NULL DEFAULT 'pending',
                            reviewer_comment TEXT,
                            review_time DOUBLE,
                            metadata JSON,
                            INDEX idx_group_id (group_id),
                            INDEX idx_status (status),
                            INDEX idx_timestamp (timestamp)
                        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
                    ''')
                else:
                    await cursor.execute('''
                        CREATE TABLE IF NOT EXISTS persona_update_reviews (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            timestamp REAL NOT NULL,
                            group_id TEXT NOT NULL,
                            update_type TEXT NOT NULL,
                            original_content TEXT,
                            new_content TEXT,
                            proposed_content TEXT,
                            confidence_score REAL,
                            reason TEXT,
                            status TEXT NOT NULL DEFAULT 'pending',
                            reviewer_comment TEXT,
                            review_time REAL,
                            metadata TEXT
                        )
                    ''')

                # å°è¯•æ·»åŠ metadataåˆ—ï¼ˆå¦‚æœè¡¨å·²å­˜åœ¨ä½†æ²¡æœ‰æ­¤åˆ—ï¼‰
                try:
                    await cursor.execute('ALTER TABLE persona_update_reviews ADD COLUMN metadata TEXT')
                except:
                    pass  # åˆ—å·²å­˜åœ¨

                # å‡†å¤‡å…ƒæ•°æ®JSON
                import json
                metadata_json = json.dumps(metadata if metadata else {}, ensure_ascii=False)

                # âœ… ä¿®å¤ï¼šä½¿ç”¨ä¼ å…¥çš„ original_content å’Œ new_content
                # å¦‚æœ new_content ä¸ºç©ºï¼Œåˆ™ä½¿ç”¨ proposed_contentï¼ˆå‘åå…¼å®¹ï¼‰
                final_new_content = new_content if new_content else proposed_content

                # æ ¹æ®æ•°æ®åº“ç±»å‹ä½¿ç”¨ä¸åŒçš„å ä½ç¬¦
                placeholder = '%s' if self.config.db_type.lower() == 'mysql' else '?'

                # æ’å…¥è®°å½•
                placeholders = ', '.join([placeholder] * 10)
                await cursor.execute(f'''
                    INSERT INTO persona_update_reviews
                    (timestamp, group_id, update_type, original_content, new_content,
                     proposed_content, confidence_score, reason, status, metadata)
                    VALUES ({placeholders})
                ''', (
                    time.time(),
                    group_id,
                    learning_source,  # update_typeå°±æ˜¯learning_source
                    original_content,  # âœ… ä½¿ç”¨ä¼ å…¥çš„åŸäººæ ¼æ–‡æœ¬
                    final_new_content,  # âœ… ä½¿ç”¨å®Œæ•´çš„æ–°äººæ ¼æ–‡æœ¬
                    proposed_content,  # proposed_contentä¿æŒä¸ºå¢é‡éƒ¨åˆ†
                    confidence_score,
                    raw_analysis,  # reasonå­—æ®µå­˜å‚¨raw_analysis
                    'pending',
                    metadata_json
                ))

                await conn.commit()
                record_id = cursor.lastrowid

                self._logger.info(f"æ·»åŠ äººæ ¼å­¦ä¹ å®¡æŸ¥è®°å½•æˆåŠŸï¼ŒID: {record_id}, ç¾¤ç»„: {group_id}")
                return record_id

        except Exception as e:
            self._logger.error(f"æ·»åŠ äººæ ¼å­¦ä¹ å®¡æŸ¥è®°å½•å¤±è´¥: {e}")
            raise

    async def get_messages_by_group_and_timerange(
        self,
        group_id: str,
        start_time: float = None,
        end_time: float = None,
        limit: int = 100
    ) -> List[Dict[str, Any]]:
        """
        è·å–æŒ‡å®šç¾¤ç»„åœ¨æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„èŠå¤©è®°å½•

        Args:
            group_id: ç¾¤ç»„ID
            start_time: å¼€å§‹æ—¶é—´æˆ³ï¼ˆç§’ï¼‰ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶
            end_time: ç»“æŸæ—¶é—´æˆ³ï¼ˆç§’ï¼‰ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶
            limit: è¿”å›æ¶ˆæ¯æ•°é‡é™åˆ¶

        Returns:
            æ¶ˆæ¯è®°å½•åˆ—è¡¨
        """
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()

            try:
                query = '''
                    SELECT id, sender_id, sender_name, message, group_id, platform, timestamp, processed
                    FROM raw_messages
                    WHERE group_id = ?
                '''
                params = [group_id]

                if start_time is not None:
                    query += ' AND timestamp >= ?'
                    params.append(start_time)

                if end_time is not None:
                    query += ' AND timestamp <= ?'
                    params.append(end_time)

                query += ' ORDER BY timestamp DESC LIMIT ?'
                params.append(limit)

                await cursor.execute(query, params)

                messages = []
                for row in await cursor.fetchall():
                    messages.append({
                        'id': row[0],
                        'sender_id': row[1],
                        'sender_name': row[2],
                        'content': row[3],  # å¤–éƒ¨APIä½¿ç”¨ 'content' å­—æ®µå
                        'group_id': row[4],
                        'platform': row[5],
                        'timestamp': row[6],
                        'processed': row[7]
                    })

                self._logger.info(f"ğŸ“– APIæŸ¥è¯¢ç»“æœ: group={group_id}, è¿”å›{len(messages)}æ¡æ¶ˆæ¯, æœ€æ–°timestamp={messages[0]['timestamp'] if messages else 'N/A'}")
                return messages

            except aiosqlite.Error as e:
                self._logger.error(f"è·å–æ—¶é—´èŒƒå›´æ¶ˆæ¯å¤±è´¥: {e}", exc_info=True)
                return []
            finally:
                await cursor.close()

    async def get_new_messages_since(
        self,
        group_id: str,
        last_message_id: int = None,
        last_timestamp: float = None
    ) -> List[Dict[str, Any]]:
        """
        è·å–æŒ‡å®šç¾¤ç»„çš„å¢é‡æ¶ˆæ¯ï¼ˆè‡ªä¸Šæ¬¡è·å–åçš„æ–°æ¶ˆæ¯ï¼‰

        Args:
            group_id: ç¾¤ç»„ID
            last_message_id: ä¸Šæ¬¡è·å–çš„æœ€åä¸€æ¡æ¶ˆæ¯ID
            last_timestamp: ä¸Šæ¬¡è·å–çš„æœ€åä¸€æ¡æ¶ˆæ¯æ—¶é—´æˆ³

        Returns:
            æ–°æ¶ˆæ¯åˆ—è¡¨
        """
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()

            try:
                # ä¼˜å…ˆä½¿ç”¨message_idï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨timestamp
                if last_message_id is not None:
                    query = '''
                        SELECT id, sender_id, sender_name, message, group_id, platform, timestamp, processed
                        FROM raw_messages
                        WHERE group_id = ? AND id > ?
                        ORDER BY timestamp ASC
                    '''
                    params = (group_id, last_message_id)
                elif last_timestamp is not None:
                    query = '''
                        SELECT id, sender_id, sender_name, message, group_id, platform, timestamp, processed
                        FROM raw_messages
                        WHERE group_id = ? AND timestamp > ?
                        ORDER BY timestamp ASC
                    '''
                    params = (group_id, last_timestamp)
                else:
                    # å¦‚æœä¸¤ä¸ªå‚æ•°éƒ½æ²¡æœ‰ï¼Œè¿”å›æœ€è¿‘çš„æ¶ˆæ¯
                    query = '''
                        SELECT id, sender_id, sender_name, message, group_id, platform, timestamp, processed
                        FROM raw_messages
                        WHERE group_id = ?
                        ORDER BY timestamp DESC
                        LIMIT 20
                    '''
                    params = (group_id,)

                await cursor.execute(query, params)

                messages = []
                for row in await cursor.fetchall():
                    messages.append({
                        'id': row[0],
                        'sender_id': row[1],
                        'sender_name': row[2],
                        'content': row[3],  # å¤–éƒ¨APIä½¿ç”¨ 'content' å­—æ®µå
                        'group_id': row[4],
                        'platform': row[5],
                        'timestamp': row[6],
                        'processed': row[7]
                    })

                return messages

            except aiosqlite.Error as e:
                self._logger.error(f"è·å–å¢é‡æ¶ˆæ¯å¤±è´¥: {e}", exc_info=True)
                return []
            finally:
                await cursor.close()

    async def get_current_topic_summary(self, group_id: str, recent_messages_count: int = 20) -> Dict[str, Any]:
        """
        è·å–æŒ‡å®šç¾¤ç»„å½“å‰çš„èŠå¤©è¯é¢˜æ€»ç»“

        ä¼˜å…ˆä»æ•°æ®åº“ä¸­è¯»å–æœ€è¿‘çš„è¯é¢˜æ€»ç»“,å¦‚æœæ²¡æœ‰æˆ–è¿‡æœŸ(è¶…è¿‡30åˆ†é’Ÿ),åˆ™åˆ†ææœ€è¿‘æ¶ˆæ¯ç”Ÿæˆæ–°çš„æ€»ç»“

        Args:
            group_id: ç¾¤ç»„ID
            recent_messages_count: åˆ†æçš„æœ€è¿‘æ¶ˆæ¯æ•°é‡

        Returns:
            è¯é¢˜æ€»ç»“ä¿¡æ¯
        """
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()

            try:
                # é¦–å…ˆå°è¯•ä»æ•°æ®åº“è·å–æœ€è¿‘30åˆ†é’Ÿå†…çš„è¯é¢˜æ€»ç»“
                thirty_minutes_ago = time.time() - 1800
                await cursor.execute('''
                    SELECT topic, summary, participants, message_count,
                           start_timestamp, end_timestamp, generated_at
                    FROM topic_summaries
                    WHERE group_id = ? AND generated_at > ?
                    ORDER BY generated_at DESC
                    LIMIT 1
                ''', (group_id, thirty_minutes_ago))

                cached_summary = await cursor.fetchone()

                if cached_summary:
                    # è¿”å›ç¼“å­˜çš„è¯é¢˜æ€»ç»“
                    import json
                    participants = json.loads(cached_summary[2]) if cached_summary[2] else []

                    return {
                        'group_id': group_id,
                        'topic': cached_summary[0],
                        'summary': cached_summary[1],
                        'participants': participants,
                        'message_count': cached_summary[3],
                        'start_timestamp': cached_summary[4],
                        'latest_timestamp': cached_summary[5],
                        'generated_at': cached_summary[6],
                        'from_cache': True
                    }

                # å¦‚æœæ²¡æœ‰ç¼“å­˜,è·å–æœ€è¿‘çš„æ¶ˆæ¯ç”Ÿæˆæ–°æ€»ç»“
                await cursor.execute('''
                    SELECT message, sender_name, timestamp
                    FROM raw_messages
                    WHERE group_id = ?
                    ORDER BY timestamp DESC
                    LIMIT ?
                ''', (group_id, recent_messages_count))

                messages = []
                latest_timestamp = None
                earliest_timestamp = None
                for row in await cursor.fetchall():
                    messages.append({
                        'message': row[0],
                        'sender_name': row[1],
                        'timestamp': row[2]
                    })
                    if latest_timestamp is None or row[2] > latest_timestamp:
                        latest_timestamp = row[2]
                    if earliest_timestamp is None or row[2] < earliest_timestamp:
                        earliest_timestamp = row[2]

                if not messages:
                    return {
                        'group_id': group_id,
                        'topic': 'æš‚æ— èŠå¤©è®°å½•',
                        'participants': [],
                        'message_count': 0,
                        'latest_timestamp': 0,
                        'summary': 'ç¾¤ç»„æš‚æ— èŠå¤©æ´»åŠ¨',
                        'from_cache': False
                    }

                # ç»Ÿè®¡å‚ä¸è€…
                participants = list(set([msg['sender_name'] for msg in messages]))

                # ä½¿ç”¨å·²æœ‰çš„è¯é¢˜åˆ†ææ–¹æ³•
                messages_text = [msg['message'] for msg in messages]
                topic_analysis = self._analyze_topic_from_messages(messages_text)

                topic_result = {
                    'group_id': group_id,
                    'topic': topic_analysis['topic'],
                    'summary': f"æœ€è¿‘{len(messages)}æ¡æ¶ˆæ¯è®¨è®ºäº†{topic_analysis['topic']},å¯¹è¯é£æ ¼ä¸º{topic_analysis['style']}",
                    'participants': participants,
                    'message_count': len(messages),
                    'start_timestamp': earliest_timestamp,
                    'latest_timestamp': latest_timestamp,
                    'generated_at': time.time(),
                    'recent_messages': messages[:5],  # è¿”å›æœ€è¿‘5æ¡æ¶ˆæ¯å†…å®¹ä¾›å‚è€ƒ
                    'from_cache': False
                }

                # ä¿å­˜åˆ°æ•°æ®åº“ä»¥ä¾›åç»­æŸ¥è¯¢
                # ä¸ç­‰å¾…ä¿å­˜å®Œæˆ,é¿å…é˜»å¡APIå“åº”
                asyncio.create_task(self._save_topic_summary(group_id, topic_result))

                return topic_result

            except aiosqlite.Error as e:
                self._logger.error(f"è·å–è¯é¢˜æ€»ç»“å¤±è´¥: {e}", exc_info=True)
                return {
                    'group_id': group_id,
                    'topic': 'è·å–å¤±è´¥',
                    'participants': [],
                    'message_count': 0,
                    'latest_timestamp': 0,
                    'summary': f'è·å–è¯é¢˜å¤±è´¥: {str(e)}',
                    'from_cache': False
                }
            finally:
                await cursor.close()

    async def _save_topic_summary(self, group_id: str, topic_data: Dict[str, Any]):
        """
        ä¿å­˜è¯é¢˜æ€»ç»“åˆ°æ•°æ®åº“

        Args:
            group_id: ç¾¤ç»„ID
            topic_data: è¯é¢˜æ•°æ®
        """
        try:
            import json
            async with self.get_db_connection() as conn:
                cursor = await conn.cursor()

                await cursor.execute('''
                    INSERT INTO topic_summaries
                    (group_id, topic, summary, participants, message_count,
                     start_timestamp, end_timestamp, generated_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    group_id,
                    topic_data.get('topic', ''),
                    topic_data.get('summary', ''),
                    json.dumps(topic_data.get('participants', []), ensure_ascii=False),
                    topic_data.get('message_count', 0),
                    topic_data.get('start_timestamp'),
                    topic_data.get('latest_timestamp'),
                    topic_data.get('generated_at', time.time())
                ))

                await conn.commit()
                await cursor.close()

                self._logger.debug(f"å·²ä¿å­˜ç¾¤ç»„ {group_id} çš„è¯é¢˜æ€»ç»“")

        except Exception as e:
            self._logger.error(f"ä¿å­˜è¯é¢˜æ€»ç»“å¤±è´¥: {e}", exc_info=True)

    def _extract_simple_keywords(self, messages: List[str], max_keywords: int = 10) -> List[str]:
        """
        ç®€å•çš„å…³é”®è¯æå–ï¼ˆåç»­å¯ä»¥ç”¨LLMä¼˜åŒ–ï¼‰

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            max_keywords: æœ€å¤§å…³é”®è¯æ•°é‡

        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        # åˆå¹¶æ‰€æœ‰æ¶ˆæ¯
        text = ' '.join(messages)

        # ç®€å•çš„è¯é¢‘ç»Ÿè®¡ï¼ˆè¿™é‡Œå¯ä»¥ç”¨jiebaç­‰å·¥å…·ä¼˜åŒ–ï¼‰
        import re
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—
        words = re.findall(r'[\u4e00-\u9fa5]+|[a-zA-Z]+', text)

        # ç»Ÿè®¡è¯é¢‘
        word_freq = {}
        for word in words:
            if len(word) >= 2:  # åªç»Ÿè®¡é•¿åº¦>=2çš„è¯
                word_freq[word] = word_freq.get(word, 0) + 1

        # æŒ‰é¢‘ç‡æ’åº
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)

        return [word for word, freq in sorted_words[:max_keywords]]

    async def get_all_expression_patterns(self, group_id: str) -> List[Dict[str, Any]]:
        """
        è·å–æŒ‡å®šç¾¤ç»„çš„æ‰€æœ‰è¡¨è¾¾æ¨¡å¼

        Args:
            group_id: ç¾¤ç»„ID

        Returns:
            è¡¨è¾¾æ¨¡å¼åˆ—è¡¨
        """
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()

            try:
                await cursor.execute('''
                    SELECT context, expression, quality_score, last_used_timestamp
                    FROM expression_patterns
                    WHERE group_id = ?
                    ORDER BY quality_score DESC, last_used_timestamp DESC
                ''', (group_id,))

                patterns = []
                for row in await cursor.fetchall():
                    patterns.append({
                        'context': row[0],
                        'expression': row[1],
                        'quality_score': row[2],
                        'last_used_timestamp': row[3]
                    })

                return patterns

            except aiosqlite.Error as e:
                self._logger.error(f"è·å–è¡¨è¾¾æ¨¡å¼å¤±è´¥: {e}", exc_info=True)
                return []
            finally:
                await cursor.close()

    async def get_all_expression_patterns_by_group(self) -> Dict[str, List[Dict[str, Any]]]:
        """
        è·å–æ‰€æœ‰ç¾¤ç»„çš„è¡¨è¾¾æ¨¡å¼ï¼ˆæŒ‰ç¾¤ç»„åˆ†ç»„ï¼‰

        Returns:
            Dict[str, List[Dict[str, Any]]]: ç¾¤ç»„ID -> è¡¨è¾¾æ¨¡å¼åˆ—è¡¨çš„æ˜ å°„
        """
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()

            try:
                await cursor.execute('''
                    SELECT id, situation, expression, weight, last_active_time, create_time, group_id
                    FROM expression_patterns
                    ORDER BY group_id, last_active_time DESC
                ''')

                patterns_by_group = {}
                for row in await cursor.fetchall():
                    group_id = row[6]
                    if group_id not in patterns_by_group:
                        patterns_by_group[group_id] = []

                    patterns_by_group[group_id].append({
                        'id': row[0],
                        'situation': row[1],
                        'expression': row[2],
                        'weight': row[3],
                        'last_active_time': row[4],
                        'created_time': row[5],
                        'group_id': group_id,
                        'style_type': 'general'
                    })

                return patterns_by_group

            except Exception as e:
                self._logger.error(f"è·å–æ‰€æœ‰è¡¨è¾¾æ¨¡å¼å¤±è´¥: {e}", exc_info=True)
                return {}
            finally:
                await cursor.close()

    async def get_recent_week_expression_patterns(self, group_id: str = None, limit: int = 20, hours: int = 168) -> List[Dict[str, Any]]:
        """
        è·å–æœ€è¿‘æŒ‡å®šå°æ—¶å†…å­¦ä¹ åˆ°çš„è¡¨è¾¾æ¨¡å¼ï¼ˆæŒ‰è´¨é‡åˆ†æ•°å’Œæ—¶é—´æ’åºï¼‰

        Args:
            group_id: ç¾¤ç»„IDï¼Œå¦‚æœä¸ºNoneåˆ™è·å–å…¨å±€æ‰€æœ‰ç¾¤ç»„çš„è¡¨è¾¾æ¨¡å¼
            limit: è·å–æ•°é‡é™åˆ¶
            hours: æ—¶é—´èŒƒå›´(å°æ—¶)ï¼Œé»˜è®¤168å°æ—¶(ä¸€å‘¨)

        Returns:
            è¡¨è¾¾æ¨¡å¼åˆ—è¡¨ï¼ŒåŒ…å«åœºæ™¯(situation)å’Œè¡¨è¾¾(expression)
        """
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()

            try:
                # è®¡ç®—æ—¶é—´é˜ˆå€¼
                time_threshold = time.time() - (hours * 3600)

                # æ ¹æ®group_idæ˜¯å¦ä¸ºNoneå†³å®šæŸ¥è¯¢æ¡ä»¶
                if group_id is None:
                    # å…¨å±€æŸ¥è¯¢ï¼šä»æ‰€æœ‰ç¾¤ç»„è·å–è¡¨è¾¾æ¨¡å¼
                    await cursor.execute('''
                        SELECT situation, expression, weight, last_active_time, create_time, group_id
                        FROM expression_patterns
                        WHERE last_active_time > ?
                        ORDER BY weight DESC, last_active_time DESC
                        LIMIT ?
                    ''', (time_threshold, limit))
                else:
                    # å•ç¾¤ç»„æŸ¥è¯¢ï¼šåªè·å–æŒ‡å®šç¾¤ç»„çš„è¡¨è¾¾æ¨¡å¼
                    await cursor.execute('''
                        SELECT situation, expression, weight, last_active_time, create_time, group_id
                        FROM expression_patterns
                        WHERE group_id = ? AND last_active_time > ?
                        ORDER BY weight DESC, last_active_time DESC
                        LIMIT ?
                    ''', (group_id, time_threshold, limit))

                patterns = []
                for row in await cursor.fetchall():
                    patterns.append({
                        'situation': row[0],  # åœºæ™¯æè¿°
                        'expression': row[1],  # è¡¨è¾¾æ–¹å¼
                        'weight': row[2],  # æƒé‡
                        'last_active_time': row[3],  # æœ€åæ´»è·ƒæ—¶é—´
                        'create_time': row[4],  # åˆ›å»ºæ—¶é—´
                        'group_id': row[5] if len(row) > 5 else group_id  # ç¾¤ç»„IDï¼ˆå…¨å±€æŸ¥è¯¢æ—¶æœ‰ç”¨ï¼‰
                    })

                return patterns

            except aiosqlite.Error as e:
                self._logger.error(f"è·å–æœ€è¿‘ä¸€å‘¨è¡¨è¾¾æ¨¡å¼å¤±è´¥: {e}", exc_info=True)
                return []
            finally:
                await cursor.close()

    async def get_recent_bot_responses(self, group_id: str, limit: int = 10) -> List[str]:
        """
        è·å–Botæœ€è¿‘çš„å›å¤å†…å®¹ï¼ˆç”¨äºåŒè´¨åŒ–åˆ†æï¼‰- ä»bot_messagesè¡¨è¯»å–

        Args:
            group_id: ç¾¤ç»„ID
            limit: è·å–æ•°é‡

        Returns:
            å›å¤å†…å®¹åˆ—è¡¨
        """
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()

            try:
                # ä»bot_messagesè¡¨è¯»å–Botçš„å›å¤
                await cursor.execute('''
                    SELECT message
                    FROM bot_messages
                    WHERE group_id = ?
                    ORDER BY timestamp DESC
                    LIMIT ?
                ''', (group_id, limit))

                responses = []
                for row in await cursor.fetchall():
                    responses.append(row[0])

                return responses

            except aiosqlite.Error as e:
                self._logger.error(f"è·å–Botæœ€è¿‘å›å¤å¤±è´¥: {e}", exc_info=True)
                return []
            finally:
                await cursor.close()

    async def save_bot_message(
        self,
        group_id: str,
        user_id: str,
        message: str,
        response_to_message_id: Optional[int] = None,
        context_type: str = "normal",
        temperature: float = 0.7,
        language_style: Optional[str] = None,
        response_pattern: Optional[str] = None
    ) -> bool:
        """
        ä¿å­˜Botå‘é€çš„æ¶ˆæ¯åˆ°æ•°æ®åº“

        Args:
            group_id: ç¾¤ç»„ID
            user_id: å›å¤çš„ç”¨æˆ·ID
            message: Botçš„å›å¤å†…å®¹
            response_to_message_id: å›å¤çš„æ¶ˆæ¯ID (æ¥è‡ªraw_messagesè¡¨)
            context_type: ä¸Šä¸‹æ–‡ç±»å‹ (normal/creative/preciseç­‰)
            temperature: ä½¿ç”¨çš„temperatureå‚æ•°
            language_style: ä½¿ç”¨çš„è¯­è¨€é£æ ¼
            response_pattern: ä½¿ç”¨çš„å›å¤æ¨¡å¼

        Returns:
            bool: æ˜¯å¦æˆåŠŸä¿å­˜
        """
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()

            try:
                await cursor.execute('''
                    INSERT INTO bot_messages
                    (group_id, user_id, message, response_to_message_id, context_type,
                     temperature, language_style, response_pattern, timestamp)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    group_id,
                    user_id,
                    message,
                    response_to_message_id,
                    context_type,
                    temperature,
                    language_style,
                    response_pattern,
                    time.time()
                ))

                await conn.commit()
                self._logger.debug(f"âœ… Botæ¶ˆæ¯å·²ä¿å­˜: group={group_id}, msg_preview={message[:50]}...")
                return True

            except aiosqlite.Error as e:
                self._logger.error(f"ä¿å­˜Botæ¶ˆæ¯å¤±è´¥: {e}", exc_info=True)
                return False
            finally:
                await cursor.close()

    async def get_bot_message_statistics(self, group_id: str, time_range_hours: int = 24) -> Dict[str, Any]:
        """
        è·å–Botæ¶ˆæ¯ç»Ÿè®¡ä¿¡æ¯ (ç”¨äºå¤šæ ·æ€§åˆ†æ)

        Args:
            group_id: ç¾¤ç»„ID
            time_range_hours: ç»Ÿè®¡æ—¶é—´èŒƒå›´(å°æ—¶)

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()

            try:
                cutoff_time = time.time() - (time_range_hours * 3600)

                # ç»Ÿè®¡æ¶ˆæ¯æ€»æ•°
                await cursor.execute('''
                    SELECT COUNT(*) as total,
                           AVG(temperature) as avg_temp,
                           COUNT(DISTINCT language_style) as unique_styles,
                           COUNT(DISTINCT response_pattern) as unique_patterns
                    FROM bot_messages
                    WHERE group_id = ? AND timestamp > ?
                ''', (group_id, cutoff_time))

                row = await cursor.fetchone()

                # è·å–æœ€å¸¸ç”¨çš„é£æ ¼å’Œæ¨¡å¼
                await cursor.execute('''
                    SELECT language_style, COUNT(*) as count
                    FROM bot_messages
                    WHERE group_id = ? AND timestamp > ? AND language_style IS NOT NULL
                    GROUP BY language_style
                    ORDER BY count DESC
                    LIMIT 5
                ''', (group_id, cutoff_time))

                top_styles = [{'style': row[0], 'count': row[1]} for row in await cursor.fetchall()]

                await cursor.execute('''
                    SELECT response_pattern, COUNT(*) as count
                    FROM bot_messages
                    WHERE group_id = ? AND timestamp > ? AND response_pattern IS NOT NULL
                    GROUP BY response_pattern
                    ORDER BY count DESC
                    LIMIT 5
                ''', (group_id, cutoff_time))

                top_patterns = [{'pattern': row[0], 'count': row[1]} for row in await cursor.fetchall()]

                return {
                    'total_messages': row[0] if row else 0,
                    'average_temperature': round(row[1], 2) if row and row[1] else 0.7,
                    'unique_styles_count': row[2] if row else 0,
                    'unique_patterns_count': row[3] if row else 0,
                    'top_styles': top_styles,
                    'top_patterns': top_patterns,
                    'time_range_hours': time_range_hours
                }

            except aiosqlite.Error as e:
                self._logger.error(f"è·å–Botæ¶ˆæ¯ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
                return {}
            finally:
                await cursor.close()

    # ========== é»‘è¯å­¦ä¹ ç³»ç»Ÿæ•°æ®åº“æ“ä½œæ–¹æ³• ==========

    async def get_jargon(self, chat_id: str, content: str) -> Optional[Dict[str, Any]]:
        """
        æŸ¥è¯¢æŒ‡å®šé»‘è¯

        Args:
            chat_id: ç¾¤ç»„ID
            content: é»‘è¯è¯æ¡

        Returns:
            é»‘è¯è®°å½•å­—å…¸æˆ–None
        """
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()

            try:
                await cursor.execute('''
                    SELECT id, content, raw_content, meaning, is_jargon, count,
                           last_inference_count, is_complete, is_global, chat_id,
                           created_at, updated_at
                    FROM jargon
                    WHERE chat_id = ? AND content = ?
                ''', (chat_id, content))

                row = await cursor.fetchone()
                if not row:
                    return None

                return {
                    'id': row[0],
                    'content': row[1],
                    'raw_content': row[2],
                    'meaning': row[3],
                    'is_jargon': bool(row[4]) if row[4] is not None else None,
                    'count': row[5],
                    'last_inference_count': row[6],
                    'is_complete': bool(row[7]),
                    'is_global': bool(row[8]),
                    'chat_id': row[9],
                    'created_at': row[10],
                    'updated_at': row[11]
                }

            except aiosqlite.Error as e:
                logger.error(f"æŸ¥è¯¢é»‘è¯å¤±è´¥: {e}", exc_info=True)
                return None
            finally:
                await cursor.close()

    async def insert_jargon(self, jargon: Dict[str, Any]) -> int:
        """
        æ’å…¥æ–°çš„é»‘è¯è®°å½•

        Args:
            jargon: é»‘è¯æ•°æ®å­—å…¸

        Returns:
            æ’å…¥è®°å½•çš„ID
        """
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()

            try:
                await cursor.execute('''
                    INSERT INTO jargon
                    (content, raw_content, meaning, is_jargon, count, last_inference_count,
                     is_complete, is_global, chat_id, created_at, updated_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    jargon.get('content'),
                    jargon.get('raw_content', '[]'),
                    jargon.get('meaning'),
                    jargon.get('is_jargon'),
                    jargon.get('count', 1),
                    jargon.get('last_inference_count', 0),
                    jargon.get('is_complete', False),
                    jargon.get('is_global', False),
                    jargon.get('chat_id'),
                    jargon.get('created_at'),
                    jargon.get('updated_at')
                ))

                jargon_id = cursor.lastrowid
                await conn.commit()
                logger.debug(f"æ’å…¥é»‘è¯è®°å½•æˆåŠŸ, ID: {jargon_id}")
                return jargon_id

            except aiosqlite.Error as e:
                logger.error(f"æ’å…¥é»‘è¯å¤±è´¥: {e}", exc_info=True)
                raise
            finally:
                await cursor.close()

    async def update_jargon(self, jargon: Dict[str, Any]) -> bool:
        """
        æ›´æ–°ç°æœ‰é»‘è¯è®°å½•

        Args:
            jargon: é»‘è¯æ•°æ®å­—å…¸(å¿…é¡»åŒ…å«id)

        Returns:
            æ˜¯å¦æˆåŠŸæ›´æ–°
        """
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()

            try:
                await cursor.execute('''
                    UPDATE jargon
                    SET content = ?, raw_content = ?, meaning = ?, is_jargon = ?,
                        count = ?, last_inference_count = ?, is_complete = ?,
                        is_global = ?, updated_at = ?
                    WHERE id = ?
                ''', (
                    jargon.get('content'),
                    jargon.get('raw_content'),
                    jargon.get('meaning'),
                    jargon.get('is_jargon'),
                    jargon.get('count'),
                    jargon.get('last_inference_count'),
                    jargon.get('is_complete'),
                    jargon.get('is_global'),
                    jargon.get('updated_at'),
                    jargon.get('id')
                ))

                await conn.commit()
                logger.debug(f"æ›´æ–°é»‘è¯è®°å½•æˆåŠŸ, ID: {jargon.get('id')}")
                return cursor.rowcount > 0

            except aiosqlite.Error as e:
                logger.error(f"æ›´æ–°é»‘è¯å¤±è´¥: {e}", exc_info=True)
                return False
            finally:
                await cursor.close()

    async def search_jargon(
        self,
        keyword: str,
        chat_id: Optional[str] = None,
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """
        æœç´¢é»‘è¯(ç”¨äºLLMå·¥å…·è°ƒç”¨)

        Args:
            keyword: æœç´¢å…³é”®è¯
            chat_id: ç¾¤ç»„ID (Noneè¡¨ç¤ºæœç´¢å…¨å±€é»‘è¯)
            limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶

        Returns:
            é»‘è¯è®°å½•åˆ—è¡¨
        """
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()

            try:
                # æ ¹æ®æ•°æ®åº“ç±»å‹é€‰æ‹©å ä½ç¬¦
                placeholder = '%s' if self.config.db_type.lower() == 'mysql' else '?'

                if chat_id:
                    # æœç´¢æŒ‡å®šç¾¤ç»„çš„é»‘è¯
                    query = f'''
                        SELECT id, content, meaning, is_jargon, count, is_complete
                        FROM jargon
                        WHERE chat_id = {placeholder} AND content LIKE {placeholder} AND is_jargon = 1
                        ORDER BY count DESC, updated_at DESC
                        LIMIT {placeholder}
                    '''
                    await cursor.execute(query, (chat_id, f'%{keyword}%', limit))
                else:
                    # æœç´¢å…¨å±€é»‘è¯
                    query = f'''
                        SELECT id, content, meaning, is_jargon, count, is_complete
                        FROM jargon
                        WHERE content LIKE {placeholder} AND is_jargon = 1 AND is_global = 1
                        ORDER BY count DESC, updated_at DESC
                        LIMIT {placeholder}
                    '''
                    await cursor.execute(query, (f'%{keyword}%', limit))

                results = []
                for row in await cursor.fetchall():
                    results.append({
                        'id': row[0],
                        'content': row[1],
                        'meaning': row[2],
                        'is_jargon': bool(row[3]),
                        'count': row[4],
                        'is_complete': bool(row[5])
                    })

                return results

            except Exception as e:
                logger.error(f"æœç´¢é»‘è¯å¤±è´¥: {e}", exc_info=True)
                return []
            finally:
                await cursor.close()

    async def get_jargon_statistics(self, chat_id: Optional[str] = None) -> Dict[str, Any]:
        """
        è·å–é»‘è¯å­¦ä¹ ç»Ÿè®¡ä¿¡æ¯

        Args:
            chat_id: ç¾¤ç»„ID (Noneè¡¨ç¤ºè·å–å…¨å±€ç»Ÿè®¡)

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()

            try:
                # æ ¹æ®æ•°æ®åº“ç±»å‹é€‰æ‹©å ä½ç¬¦
                placeholder = '%s' if self.config.db_type.lower() == 'mysql' else '?'

                if chat_id:
                    # ç¾¤ç»„ç»Ÿè®¡
                    query = f'''
                        SELECT
                            COUNT(*) as total,
                            COUNT(CASE WHEN is_jargon = 1 THEN 1 END) as confirmed_jargon,
                            COUNT(CASE WHEN is_complete = 1 THEN 1 END) as completed,
                            SUM(count) as total_occurrences,
                            AVG(count) as avg_count
                        FROM jargon
                        WHERE chat_id = {placeholder}
                    '''
                    await cursor.execute(query, (chat_id,))
                else:
                    # å…¨å±€ç»Ÿè®¡
                    await cursor.execute('''
                        SELECT
                            COUNT(*) as total,
                            COUNT(CASE WHEN is_jargon = 1 THEN 1 END) as confirmed_jargon,
                            COUNT(CASE WHEN is_complete = 1 THEN 1 END) as completed,
                            SUM(count) as total_occurrences,
                            AVG(count) as avg_count,
                            COUNT(DISTINCT chat_id) as active_groups
                        FROM jargon
                    ''')

                row = await cursor.fetchone()

                # æ·»åŠ è¡Œæ•°æ®éªŒè¯
                if not row or len(row) < 5:
                    self._logger.warning(f"é»‘è¯ç»Ÿè®¡æ•°æ®è¡Œä¸å®Œæ•´ (æœŸæœ›è‡³å°‘5ä¸ªå­—æ®µï¼Œå®é™…{len(row) if row else 0}ä¸ª)ï¼Œè¿”å›é»˜è®¤å€¼")
                    return {
                        'total_candidates': 0,
                        'confirmed_jargon': 0,
                        'completed_inference': 0,
                        'total_occurrences': 0,
                        'average_count': 0,
                        'active_groups': 0
                    }

                stats = {
                    'total_candidates': int(row[0]) if row[0] else 0,
                    'confirmed_jargon': int(row[1]) if row[1] else 0,
                    'completed_inference': int(row[2]) if row[2] else 0,
                    'total_occurrences': int(row[3]) if row[3] else 0,
                    'average_count': round(float(row[4]), 1) if row[4] else 0
                }

                if not chat_id and len(row) > 5:
                    stats['active_groups'] = int(row[5]) if row[5] else 0

                return stats

            except Exception as e:
                logger.error(f"è·å–é»‘è¯ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
                return {
                    'total_candidates': 0,
                    'confirmed_jargon': 0,
                    'completed_inference': 0,
                    'total_occurrences': 0,
                    'average_count': 0
                }
            finally:
                await cursor.close()

    async def get_recent_jargon_list(
        self,
        chat_id: Optional[str] = None,
        limit: int = 20,
        only_confirmed: bool = True
    ) -> List[Dict[str, Any]]:
        """
        è·å–æœ€è¿‘å­¦ä¹ åˆ°çš„é»‘è¯åˆ—è¡¨

        Args:
            chat_id: ç¾¤ç»„ID (Noneè¡¨ç¤ºè·å–æ‰€æœ‰)
            limit: è¿”å›æ•°é‡é™åˆ¶
            only_confirmed: æ˜¯å¦åªè¿”å›å·²ç¡®è®¤çš„é»‘è¯

        Returns:
            é»‘è¯åˆ—è¡¨
        """
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()

            try:
                # æ ¹æ®æ•°æ®åº“ç±»å‹é€‰æ‹©å ä½ç¬¦
                placeholder = '%s' if self.config.db_type.lower() == 'mysql' else '?'

                query = '''
                    SELECT id, content, meaning, is_jargon, count,
                           last_inference_count, is_complete, chat_id, updated_at, is_global
                    FROM jargon
                    WHERE 1=1
                '''
                params = []

                if chat_id:
                    query += f' AND chat_id = {placeholder}'
                    params.append(chat_id)

                if only_confirmed:
                    query += ' AND is_jargon = 1'

                query += f' ORDER BY updated_at DESC LIMIT {placeholder}'
                params.append(limit)

                await cursor.execute(query, tuple(params))

                jargon_list = []
                for row in await cursor.fetchall():
                    try:
                        # æ·»åŠ è¡Œæ•°æ®éªŒè¯
                        if len(row) < 10:
                            self._logger.warning(f"é»‘è¯è®°å½•è¡Œæ•°æ®ä¸å®Œæ•´ (æœŸæœ›10ä¸ªå­—æ®µï¼Œå®é™…{len(row)}ä¸ª)ï¼Œè·³è¿‡: {row}")
                            continue

                        jargon_list.append({
                            'id': row[0],
                            'content': row[1],
                            'meaning': row[2],
                            'is_jargon': bool(row[3]) if row[3] is not None else None,
                            'count': int(row[4]) if row[4] else 0,
                            'last_inference_count': int(row[5]) if row[5] else 0,
                            'is_complete': bool(row[6]),
                            'chat_id': row[7],
                            'updated_at': row[8],
                            'is_global': bool(row[9]) if row[9] is not None else False
                        })
                    except Exception as row_error:
                        self._logger.warning(f"å¤„ç†é»‘è¯è®°å½•è¡Œæ—¶å‡ºé”™ï¼Œè·³è¿‡: {row_error}, row: {row}")
                        continue

                return jargon_list

            except Exception as e:
                logger.error(f"è·å–é»‘è¯åˆ—è¡¨å¤±è´¥: {e}", exc_info=True)
                return []
            finally:
                await cursor.close()

    async def get_jargon_by_id(self, jargon_id: int) -> Optional[Dict[str, Any]]:
        """
        æ ¹æ®IDè·å–é»‘è¯è®°å½•

        Args:
            jargon_id: é»‘è¯è®°å½•ID

        Returns:
            é»‘è¯è®°å½•æˆ–None
        """
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()

            try:
                # æ ¹æ®æ•°æ®åº“ç±»å‹é€‰æ‹©å ä½ç¬¦
                placeholder = '%s' if self.config.db_type.lower() == 'mysql' else '?'

                query = f'''
                    SELECT id, content, meaning, is_jargon, count,
                           last_inference_count, is_complete, chat_id, updated_at, is_global
                    FROM jargon
                    WHERE id = {placeholder}
                '''
                await cursor.execute(query, (jargon_id,))
                row = await cursor.fetchone()

                if row:
                    return {
                        'id': row[0],
                        'content': row[1],
                        'meaning': row[2],
                        'is_jargon': bool(row[3]) if row[3] is not None else None,
                        'count': row[4],
                        'last_inference_count': row[5],
                        'is_complete': bool(row[6]),
                        'chat_id': row[7],
                        'updated_at': row[8],
                        'is_global': bool(row[9]) if row[9] is not None else False
                    }
                return None

            except Exception as e:
                logger.error(f"è·å–é»‘è¯è®°å½•å¤±è´¥: {e}", exc_info=True)
                return None
            finally:
                await cursor.close()

    async def delete_jargon_by_id(self, jargon_id: int) -> bool:
        """
        æ ¹æ®IDåˆ é™¤é»‘è¯è®°å½•

        Args:
            jargon_id: é»‘è¯è®°å½•ID

        Returns:
            æ˜¯å¦æˆåŠŸåˆ é™¤
        """
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()

            try:
                # æ ¹æ®æ•°æ®åº“ç±»å‹é€‰æ‹©å ä½ç¬¦
                placeholder = '%s' if self.config.db_type.lower() == 'mysql' else '?'

                query = f'DELETE FROM jargon WHERE id = {placeholder}'
                await cursor.execute(query, (jargon_id,))
                await conn.commit()
                deleted = cursor.rowcount > 0
                if deleted:
                    logger.debug(f"åˆ é™¤é»‘è¯è®°å½•æˆåŠŸ, ID: {jargon_id}")
                return deleted

            except Exception as e:
                logger.error(f"åˆ é™¤é»‘è¯å¤±è´¥: {e}", exc_info=True)
                return False
            finally:
                await cursor.close()

    async def get_global_jargon_list(self, limit: int = 50) -> List[Dict[str, Any]]:
        """
        è·å–å…¨å±€å…±äº«çš„é»‘è¯åˆ—è¡¨

        Args:
            limit: è¿”å›æ•°é‡é™åˆ¶

        Returns:
            å…¨å±€é»‘è¯åˆ—è¡¨
        """
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()

            try:
                await cursor.execute('''
                    SELECT id, content, meaning, is_jargon, count,
                           last_inference_count, is_complete, is_global, chat_id, updated_at
                    FROM jargon
                    WHERE is_jargon = 1 AND is_global = 1
                    ORDER BY count DESC, updated_at DESC
                    LIMIT ?
                ''', (limit,))

                jargon_list = []
                for row in await cursor.fetchall():
                    jargon_list.append({
                        'id': row[0],
                        'content': row[1],
                        'meaning': row[2],
                        'is_jargon': bool(row[3]),
                        'count': row[4],
                        'last_inference_count': row[5],
                        'is_complete': bool(row[6]),
                        'is_global': bool(row[7]),
                        'chat_id': row[8],
                        'updated_at': row[9]
                    })

                return jargon_list

            except aiosqlite.Error as e:
                logger.error(f"è·å–å…¨å±€é»‘è¯åˆ—è¡¨å¤±è´¥: {e}", exc_info=True)
                return []
            finally:
                await cursor.close()

    async def set_jargon_global(self, jargon_id: int, is_global: bool) -> bool:
        """
        è®¾ç½®é»‘è¯çš„å…¨å±€å…±äº«çŠ¶æ€

        Args:
            jargon_id: é»‘è¯è®°å½•ID
            is_global: æ˜¯å¦å…¨å±€å…±äº«

        Returns:
            æ˜¯å¦æˆåŠŸæ›´æ–°
        """
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()

            try:
                await cursor.execute('''
                    UPDATE jargon
                    SET is_global = ?, updated_at = CURRENT_TIMESTAMP
                    WHERE id = ?
                ''', (is_global, jargon_id))

                await conn.commit()
                updated = cursor.rowcount > 0
                if updated:
                    logger.info(f"é»‘è¯å…¨å±€çŠ¶æ€å·²æ›´æ–°: ID={jargon_id}, is_global={is_global}")
                return updated

            except aiosqlite.Error as e:
                logger.error(f"æ›´æ–°é»‘è¯å…¨å±€çŠ¶æ€å¤±è´¥: {e}", exc_info=True)
                return False
            finally:
                await cursor.close()

    async def sync_global_jargon_to_group(self, target_chat_id: str) -> Dict[str, Any]:
        """
        å°†å…¨å±€é»‘è¯åŒæ­¥åˆ°æŒ‡å®šç¾¤ç»„

        Args:
            target_chat_id: ç›®æ ‡ç¾¤ç»„ID

        Returns:
            åŒæ­¥ç»“æœç»Ÿè®¡
        """
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()

            try:
                # è·å–å…¨å±€é»‘è¯åˆ—è¡¨
                await cursor.execute('''
                    SELECT content, meaning, count
                    FROM jargon
                    WHERE is_jargon = 1 AND is_global = 1 AND chat_id != ?
                ''', (target_chat_id,))

                global_jargon = await cursor.fetchall()

                synced_count = 0
                skipped_count = 0

                for content, meaning, count in global_jargon:
                    # æ£€æŸ¥ç›®æ ‡ç¾¤ç»„æ˜¯å¦å·²å­˜åœ¨è¯¥é»‘è¯
                    await cursor.execute('''
                        SELECT id FROM jargon
                        WHERE chat_id = ? AND content = ?
                    ''', (target_chat_id, content))

                    existing = await cursor.fetchone()

                    if existing:
                        # å·²å­˜åœ¨ï¼Œè·³è¿‡
                        skipped_count += 1
                    else:
                        # ä¸å­˜åœ¨ï¼ŒåŒæ­¥åˆ°ç›®æ ‡ç¾¤ç»„
                        await cursor.execute('''
                            INSERT INTO jargon
                            (content, raw_content, meaning, is_jargon, count, last_inference_count,
                             is_complete, is_global, chat_id, created_at, updated_at)
                            VALUES (?, '[]', ?, 1, 1, 0, 0, 0, ?, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
                        ''', (content, meaning, target_chat_id))
                        synced_count += 1

                await conn.commit()

                logger.info(f"åŒæ­¥å…¨å±€é»‘è¯åˆ°ç¾¤ç»„ {target_chat_id}: åŒæ­¥ {synced_count} æ¡, è·³è¿‡ {skipped_count} æ¡")

                return {
                    'success': True,
                    'synced_count': synced_count,
                    'skipped_count': skipped_count,
                    'total_global': len(global_jargon)
                }

            except aiosqlite.Error as e:
                logger.error(f"åŒæ­¥å…¨å±€é»‘è¯å¤±è´¥: {e}", exc_info=True)
                return {
                    'success': False,
                    'error': str(e),
                    'synced_count': 0,
                    'skipped_count': 0
                }
            finally:
                await cursor.close()

    async def batch_set_jargon_global(self, jargon_ids: List[int], is_global: bool) -> Dict[str, Any]:
        """
        æ‰¹é‡è®¾ç½®é»‘è¯çš„å…¨å±€å…±äº«çŠ¶æ€

        Args:
            jargon_ids: é»‘è¯è®°å½•IDåˆ—è¡¨
            is_global: æ˜¯å¦å…¨å±€å…±äº«

        Returns:
            æ“ä½œç»“æœç»Ÿè®¡
        """
        async with self.get_db_connection() as conn:
            cursor = await conn.cursor()

            try:
                success_count = 0
                failed_count = 0

                for jid in jargon_ids:
                    try:
                        await cursor.execute('''
                            UPDATE jargon
                            SET is_global = ?, updated_at = CURRENT_TIMESTAMP
                            WHERE id = ? AND is_jargon = 1
                        ''', (is_global, jid))
                        if cursor.rowcount > 0:
                            success_count += 1
                        else:
                            failed_count += 1
                    except Exception:
                        failed_count += 1

                await conn.commit()

                logger.info(f"æ‰¹é‡æ›´æ–°é»‘è¯å…¨å±€çŠ¶æ€: æˆåŠŸ {success_count}, å¤±è´¥ {failed_count}")

                return {
                    'success': True,
                    'success_count': success_count,
                    'failed_count': failed_count
                }

            except aiosqlite.Error as e:
                logger.error(f"æ‰¹é‡æ›´æ–°é»‘è¯å…¨å±€çŠ¶æ€å¤±è´¥: {e}", exc_info=True)
                return {
                    'success': False,
                    'error': str(e),
                    'success_count': 0,
                    'failed_count': len(jargon_ids)
                }
            finally:
                await cursor.close()

    # ========================================================================
    # ORM Repository æ–¹æ³•ï¼ˆæ–°ï¼‰
    # ========================================================================

    async def save_learning_batch(
        self,
        batch_id: str,
        batch_name: str,
        group_id: str,
        start_time: float,
        end_time: Optional[float] = None,
        quality_score: Optional[float] = None,
        processed_messages: int = 0,
        message_count: int = 0,
        filtered_count: int = 0,
        success: bool = True,
        error_message: Optional[str] = None,
        status: str = 'pending'
    ) -> bool:
        """
        ä¿å­˜å­¦ä¹ æ‰¹æ¬¡ï¼ˆä½¿ç”¨ ORMï¼‰

        Args:
            batch_id: æ‰¹æ¬¡ ID
            batch_name: æ‰¹æ¬¡åç§°
            group_id: ç¾¤ç»„ ID
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´
            quality_score: è´¨é‡åˆ†æ•°
            processed_messages: å·²å¤„ç†æ¶ˆæ¯æ•°
            message_count: æ€»æ¶ˆæ¯æ•°
            filtered_count: è¿‡æ»¤æ‰çš„æ¶ˆæ¯æ•°
            success: æ˜¯å¦æˆåŠŸ
            error_message: é”™è¯¯ä¿¡æ¯
            status: çŠ¶æ€

        Returns:
            bool: æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        if not self.db_engine:
            self._logger.warning("DatabaseEngine æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä¿å­˜å­¦ä¹ æ‰¹æ¬¡")
            return False

        try:
            async with self.db_engine.get_session() as session:
                repo = LearningBatchRepository(session)
                batch = await repo.save_learning_batch(
                    batch_id=batch_id,
                    batch_name=batch_name,
                    group_id=group_id,
                    start_time=start_time,
                    end_time=end_time,
                    quality_score=quality_score,
                    processed_messages=processed_messages,
                    message_count=message_count,
                    filtered_count=filtered_count,
                    success=success,
                    error_message=error_message,
                    status=status
                )
                await session.commit()
                return batch is not None

        except Exception as e:
            self._logger.error(f"ä¿å­˜å­¦ä¹ æ‰¹æ¬¡å¤±è´¥: {e}", exc_info=True)
            return False

    async def get_learning_batches(
        self,
        group_id: str,
        limit: int = 50,
        offset: int = 0,
        status_filter: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        è·å–å­¦ä¹ æ‰¹æ¬¡åˆ—è¡¨ï¼ˆä½¿ç”¨ ORMï¼‰

        Args:
            group_id: ç¾¤ç»„ ID
            limit: æœ€å¤§è¿”å›æ•°é‡
            offset: åç§»é‡
            status_filter: çŠ¶æ€è¿‡æ»¤

        Returns:
            List[Dict]: æ‰¹æ¬¡åˆ—è¡¨
        """
        if not self.db_engine:
            self._logger.warning("DatabaseEngine æœªåˆå§‹åŒ–ï¼Œè¿”å›ç©ºåˆ—è¡¨")
            return []

        try:
            async with self.db_engine.get_session() as session:
                repo = LearningBatchRepository(session)
                batches = await repo.get_learning_batches(
                    group_id=group_id,
                    limit=limit,
                    offset=offset,
                    status_filter=status_filter
                )
                return [batch.to_dict() for batch in batches]

        except Exception as e:
            self._logger.error(f"è·å–å­¦ä¹ æ‰¹æ¬¡åˆ—è¡¨å¤±è´¥: {e}", exc_info=True)
            return []

    async def get_learning_batch_by_id(self, batch_id: str) -> Optional[Dict[str, Any]]:
        """
        æ ¹æ® batch_id è·å–å­¦ä¹ æ‰¹æ¬¡ï¼ˆä½¿ç”¨ ORMï¼‰

        Args:
            batch_id: æ‰¹æ¬¡ ID

        Returns:
            Optional[Dict]: æ‰¹æ¬¡è®°å½•
        """
        if not self.db_engine:
            self._logger.warning("DatabaseEngine æœªåˆå§‹åŒ–ï¼Œè¿”å› None")
            return None

        try:
            async with self.db_engine.get_session() as session:
                repo = LearningBatchRepository(session)
                batch = await repo.get_learning_batch_by_id(batch_id)
                return batch.to_dict() if batch else None

        except Exception as e:
            self._logger.error(f"è·å–å­¦ä¹ æ‰¹æ¬¡å¤±è´¥: {e}", exc_info=True)
            return None

    async def save_learning_session(
        self,
        session_id: str,
        group_id: str,
        batch_id: Optional[str] = None,
        start_time: Optional[float] = None,
        end_time: Optional[float] = None,
        metrics: Optional[str] = None
    ) -> bool:
        """
        ä¿å­˜å­¦ä¹ ä¼šè¯ï¼ˆä½¿ç”¨ ORMï¼‰

        Args:
            session_id: ä¼šè¯ ID
            group_id: ç¾¤ç»„ ID
            batch_id: æ‰¹æ¬¡ ID
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´
            metrics: æŒ‡æ ‡æ•°æ®ï¼ˆJSONå­—ç¬¦ä¸²ï¼‰

        Returns:
            bool: æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        if not self.db_engine:
            self._logger.warning("DatabaseEngine æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä¿å­˜å­¦ä¹ ä¼šè¯")
            return False

        try:
            async with self.db_engine.get_session() as session:
                repo = LearningSessionRepository(session)
                learning_session = await repo.save_learning_session(
                    session_id=session_id,
                    group_id=group_id,
                    batch_id=batch_id,
                    start_time=start_time,
                    end_time=end_time,
                    metrics=metrics
                )
                await session.commit()
                return learning_session is not None

        except Exception as e:
            self._logger.error(f"ä¿å­˜å­¦ä¹ ä¼šè¯å¤±è´¥: {e}", exc_info=True)
            return False

    async def get_learning_sessions(
        self,
        group_id: str,
        batch_id: Optional[str] = None,
        limit: int = 50,
        offset: int = 0
    ) -> List[Dict[str, Any]]:
        """
        è·å–å­¦ä¹ ä¼šè¯åˆ—è¡¨ï¼ˆä½¿ç”¨ ORMï¼‰

        Args:
            group_id: ç¾¤ç»„ ID
            batch_id: æ‰¹æ¬¡ IDï¼ˆå¯é€‰ï¼‰
            limit: æœ€å¤§è¿”å›æ•°é‡
            offset: åç§»é‡

        Returns:
            List[Dict]: ä¼šè¯åˆ—è¡¨
        """
        if not self.db_engine:
            self._logger.warning("DatabaseEngine æœªåˆå§‹åŒ–ï¼Œè¿”å›ç©ºåˆ—è¡¨")
            return []

        try:
            async with self.db_engine.get_session() as session:
                repo = LearningSessionRepository(session)
                sessions = await repo.get_learning_sessions(
                    group_id=group_id,
                    batch_id=batch_id,
                    limit=limit,
                    offset=offset
                )
                return [sess.to_dict() for sess in sessions]

        except Exception as e:
            self._logger.error(f"è·å–å­¦ä¹ ä¼šè¯åˆ—è¡¨å¤±è´¥: {e}", exc_info=True)
            return []

    # ==================== å¯¹è¯ä¸ä¸Šä¸‹æ–‡ç³»ç»Ÿ ORM æ–¹æ³• ====================

    async def save_conversation_context(
        self,
        group_id: str,
        user_id: str,
        context_window: str,
        topic: Optional[str] = None,
        sentiment: Optional[str] = None,
        context_embedding: Optional[bytes] = None,
        last_updated: Optional[float] = None
    ) -> bool:
        """
        ä¿å­˜å¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆä½¿ç”¨ ORMï¼‰

        Args:
            group_id: ç¾¤ç»„ ID
            user_id: ç”¨æˆ· ID
            context_window: ä¸Šä¸‹æ–‡çª—å£ï¼ˆJSONå­—ç¬¦ä¸²ï¼‰
            topic: å½“å‰è¯é¢˜
            sentiment: æƒ…æ„Ÿå€¾å‘
            context_embedding: ä¸Šä¸‹æ–‡å‘é‡åµŒå…¥
            last_updated: æœ€åæ›´æ–°æ—¶é—´æˆ³

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        if not self.db_engine:
            self._logger.warning("DatabaseEngine æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä¿å­˜å¯¹è¯ä¸Šä¸‹æ–‡")
            return False

        try:
            async with self.db_engine.get_session() as session:
                repo = ConversationContextRepository(session)
                context = await repo.save_context(
                    group_id=group_id,
                    user_id=user_id,
                    context_window=context_window,
                    topic=topic,
                    sentiment=sentiment,
                    context_embedding=context_embedding,
                    last_updated=last_updated
                )
                await session.commit()
                return context is not None

        except Exception as e:
            self._logger.error(f"ä¿å­˜å¯¹è¯ä¸Šä¸‹æ–‡å¤±è´¥: {e}", exc_info=True)
            return False

    async def get_latest_conversation_context(
        self,
        group_id: str,
        user_id: str
    ) -> Optional[Dict[str, Any]]:
        """
        è·å–æœ€æ–°çš„å¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆä½¿ç”¨ ORMï¼‰

        Args:
            group_id: ç¾¤ç»„ ID
            user_id: ç”¨æˆ· ID

        Returns:
            Optional[Dict]: ä¸Šä¸‹æ–‡è®°å½•
        """
        if not self.db_engine:
            self._logger.warning("DatabaseEngine æœªåˆå§‹åŒ–ï¼Œè¿”å› None")
            return None

        try:
            async with self.db_engine.get_session() as session:
                repo = ConversationContextRepository(session)
                context = await repo.get_latest_context(
                    group_id=group_id,
                    user_id=user_id
                )
                return context.to_dict() if context else None

        except Exception as e:
            self._logger.error(f"è·å–æœ€æ–°å¯¹è¯ä¸Šä¸‹æ–‡å¤±è´¥: {e}", exc_info=True)
            return None

    async def save_topic_cluster(
        self,
        group_id: str,
        cluster_id: str,
        topic_keywords: str,
        message_count: int = 0,
        representative_messages: Optional[str] = None,
        cluster_center: Optional[bytes] = None
    ) -> bool:
        """
        ä¿å­˜ä¸»é¢˜èšç±»ï¼ˆä½¿ç”¨ ORMï¼‰

        Args:
            group_id: ç¾¤ç»„ ID
            cluster_id: èšç±» ID
            topic_keywords: ä¸»é¢˜å…³é”®è¯ï¼ˆJSONå­—ç¬¦ä¸²ï¼‰
            message_count: æ¶ˆæ¯æ•°é‡
            representative_messages: ä»£è¡¨æ€§æ¶ˆæ¯ï¼ˆJSONå­—ç¬¦ä¸²ï¼‰
            cluster_center: èšç±»ä¸­å¿ƒå‘é‡

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        if not self.db_engine:
            self._logger.warning("DatabaseEngine æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä¿å­˜ä¸»é¢˜èšç±»")
            return False

        try:
            async with self.db_engine.get_session() as session:
                repo = ConversationTopicClusteringRepository(session)
                cluster = await repo.save_cluster(
                    group_id=group_id,
                    cluster_id=cluster_id,
                    topic_keywords=topic_keywords,
                    message_count=message_count,
                    representative_messages=representative_messages,
                    cluster_center=cluster_center
                )
                await session.commit()
                return cluster is not None

        except Exception as e:
            self._logger.error(f"ä¿å­˜ä¸»é¢˜èšç±»å¤±è´¥: {e}", exc_info=True)
            return False

    async def get_all_topic_clusters(
        self,
        group_id: str,
        order_by_message_count: bool = True,
        limit: int = 100
    ) -> List[Dict[str, Any]]:
        """
        è·å–æ‰€æœ‰ä¸»é¢˜èšç±»ï¼ˆä½¿ç”¨ ORMï¼‰

        Args:
            group_id: ç¾¤ç»„ ID
            order_by_message_count: æ˜¯å¦æŒ‰æ¶ˆæ¯æ•°é‡æ’åº
            limit: æœ€å¤§è¿”å›æ•°é‡

        Returns:
            List[Dict]: èšç±»åˆ—è¡¨
        """
        if not self.db_engine:
            self._logger.warning("DatabaseEngine æœªåˆå§‹åŒ–ï¼Œè¿”å›ç©ºåˆ—è¡¨")
            return []

        try:
            async with self.db_engine.get_session() as session:
                repo = ConversationTopicClusteringRepository(session)
                clusters = await repo.get_all_clusters(
                    group_id=group_id,
                    order_by_message_count=order_by_message_count,
                    limit=limit
                )
                return [cluster.to_dict() for cluster in clusters]

        except Exception as e:
            self._logger.error(f"è·å–ä¸»é¢˜èšç±»åˆ—è¡¨å¤±è´¥: {e}", exc_info=True)
            return []

    async def save_quality_metrics(
        self,
        group_id: str,
        message_id: int,
        coherence_score: Optional[float] = None,
        relevance_score: Optional[float] = None,
        engagement_score: Optional[float] = None,
        sentiment_alignment: Optional[float] = None,
        calculated_at: Optional[float] = None
    ) -> bool:
        """
        ä¿å­˜å¯¹è¯è´¨é‡æŒ‡æ ‡ï¼ˆä½¿ç”¨ ORMï¼‰

        Args:
            group_id: ç¾¤ç»„ ID
            message_id: æ¶ˆæ¯ ID
            coherence_score: è¿è´¯æ€§åˆ†æ•°
            relevance_score: ç›¸å…³æ€§åˆ†æ•°
            engagement_score: äº’åŠ¨åº¦åˆ†æ•°
            sentiment_alignment: æƒ…æ„Ÿä¸€è‡´æ€§åˆ†æ•°
            calculated_at: è®¡ç®—æ—¶é—´æˆ³

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        if not self.db_engine:
            self._logger.warning("DatabaseEngine æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä¿å­˜è´¨é‡æŒ‡æ ‡")
            return False

        try:
            async with self.db_engine.get_session() as session:
                repo = ConversationQualityMetricsRepository(session)
                metrics = await repo.save_quality_metrics(
                    group_id=group_id,
                    message_id=message_id,
                    coherence_score=coherence_score,
                    relevance_score=relevance_score,
                    engagement_score=engagement_score,
                    sentiment_alignment=sentiment_alignment,
                    calculated_at=calculated_at
                )
                await session.commit()
                return metrics is not None

        except Exception as e:
            self._logger.error(f"ä¿å­˜è´¨é‡æŒ‡æ ‡å¤±è´¥: {e}", exc_info=True)
            return False

    async def get_average_quality_scores(
        self,
        group_id: str,
        start_time: Optional[float] = None,
        end_time: Optional[float] = None
    ) -> Dict[str, float]:
        """
        è·å–å¹³å‡è´¨é‡åˆ†æ•°ï¼ˆä½¿ç”¨ ORMï¼‰

        Args:
            group_id: ç¾¤ç»„ ID
            start_time: å¼€å§‹æ—¶é—´æˆ³ï¼ˆå¯é€‰ï¼‰
            end_time: ç»“æŸæ—¶é—´æˆ³ï¼ˆå¯é€‰ï¼‰

        Returns:
            Dict[str, float]: å„æŒ‡æ ‡çš„å¹³å‡åˆ†æ•°
        """
        if not self.db_engine:
            self._logger.warning("DatabaseEngine æœªåˆå§‹åŒ–ï¼Œè¿”å›é»˜è®¤å€¼")
            return {
                "avg_coherence_score": 0.0,
                "avg_relevance_score": 0.0,
                "avg_engagement_score": 0.0,
                "avg_sentiment_alignment": 0.0
            }

        try:
            async with self.db_engine.get_session() as session:
                repo = ConversationQualityMetricsRepository(session)
                return await repo.get_average_scores(
                    group_id=group_id,
                    start_time=start_time,
                    end_time=end_time
                )

        except Exception as e:
            self._logger.error(f"è·å–å¹³å‡è´¨é‡åˆ†æ•°å¤±è´¥: {e}", exc_info=True)
            return {
                "avg_coherence_score": 0.0,
                "avg_relevance_score": 0.0,
                "avg_engagement_score": 0.0,
                "avg_sentiment_alignment": 0.0
            }

    async def save_context_similarity(
        self,
        context_hash_1: str,
        context_hash_2: str,
        similarity_score: float,
        calculation_method: Optional[str] = None,
        cached_at: Optional[float] = None
    ) -> bool:
        """
        ä¿å­˜ä¸Šä¸‹æ–‡ç›¸ä¼¼åº¦ç¼“å­˜ï¼ˆä½¿ç”¨ ORMï¼‰

        Args:
            context_hash_1: ä¸Šä¸‹æ–‡1çš„å“ˆå¸Œå€¼
            context_hash_2: ä¸Šä¸‹æ–‡2çš„å“ˆå¸Œå€¼
            similarity_score: ç›¸ä¼¼åº¦åˆ†æ•°
            calculation_method: è®¡ç®—æ–¹æ³•
            cached_at: ç¼“å­˜æ—¶é—´æˆ³

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        if not self.db_engine:
            self._logger.warning("DatabaseEngine æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä¿å­˜ç›¸ä¼¼åº¦ç¼“å­˜")
            return False

        try:
            async with self.db_engine.get_session() as session:
                repo = ContextSimilarityCacheRepository(session)
                cache = await repo.save_similarity(
                    context_hash_1=context_hash_1,
                    context_hash_2=context_hash_2,
                    similarity_score=similarity_score,
                    calculation_method=calculation_method,
                    cached_at=cached_at
                )
                await session.commit()
                return cache is not None

        except Exception as e:
            self._logger.error(f"ä¿å­˜ç›¸ä¼¼åº¦ç¼“å­˜å¤±è´¥: {e}", exc_info=True)
            return False

    async def get_context_similarity(
        self,
        context_hash_1: str,
        context_hash_2: str
    ) -> Optional[float]:
        """
        è·å–ä¸Šä¸‹æ–‡ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨ ORMï¼Œæ”¯æŒåŒå‘æŸ¥æ‰¾ï¼‰

        Args:
            context_hash_1: ä¸Šä¸‹æ–‡1çš„å“ˆå¸Œå€¼
            context_hash_2: ä¸Šä¸‹æ–‡2çš„å“ˆå¸Œå€¼

        Returns:
            Optional[float]: ç›¸ä¼¼åº¦åˆ†æ•°
        """
        if not self.db_engine:
            self._logger.warning("DatabaseEngine æœªåˆå§‹åŒ–ï¼Œè¿”å› None")
            return None

        try:
            async with self.db_engine.get_session() as session:
                repo = ContextSimilarityCacheRepository(session)
                cache = await repo.get_similarity(
                    context_hash_1=context_hash_1,
                    context_hash_2=context_hash_2
                )
                return cache.similarity_score if cache else None

        except Exception as e:
            self._logger.error(f"è·å–ç›¸ä¼¼åº¦ç¼“å­˜å¤±è´¥: {e}", exc_info=True)
            return None

    # ==================== é»‘è¯ç³»ç»Ÿ ORM æ–¹æ³• ====================

    async def get_recent_jargon_list_orm(
        self,
        chat_id: Optional[str] = None,
        limit: int = 20,
        only_confirmed: bool = True
    ) -> List[Dict[str, Any]]:
        """
        è·å–æœ€è¿‘å­¦ä¹ åˆ°çš„é»‘è¯åˆ—è¡¨ï¼ˆä½¿ç”¨ ORMï¼‰

        Args:
            chat_id: ç¾¤ç»„ID (Noneè¡¨ç¤ºè·å–æ‰€æœ‰)
            limit: è¿”å›æ•°é‡é™åˆ¶
            only_confirmed: æ˜¯å¦åªè¿”å›å·²ç¡®è®¤çš„é»‘è¯

        Returns:
            List[Dict]: é»‘è¯åˆ—è¡¨
        """
        if not self.db_engine:
            self._logger.warning("DatabaseEngine æœªåˆå§‹åŒ–ï¼Œè¿”å›ç©ºåˆ—è¡¨")
            return []

        try:
            async with self.db_engine.get_session() as session:
                repo = JargonRepository(session)
                jargons = await repo.get_recent_jargon_list(
                    chat_id=chat_id,
                    limit=limit,
                    only_confirmed=only_confirmed
                )
                return [jargon.to_dict() for jargon in jargons]

        except Exception as e:
            self._logger.error(f"è·å–é»‘è¯åˆ—è¡¨å¤±è´¥ï¼ˆORMï¼‰: {e}", exc_info=True)
            return []

    async def get_jargon_statistics_orm(
        self,
        chat_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        è·å–é»‘è¯å­¦ä¹ ç»Ÿè®¡ä¿¡æ¯ï¼ˆä½¿ç”¨ ORMï¼‰

        Args:
            chat_id: ç¾¤ç»„ID (Noneè¡¨ç¤ºè·å–å…¨å±€ç»Ÿè®¡)

        Returns:
            Dict[str, Any]: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        if not self.db_engine:
            self._logger.warning("DatabaseEngine æœªåˆå§‹åŒ–ï¼Œè¿”å›é»˜è®¤å€¼")
            return {
                'total_candidates': 0,
                'confirmed_jargon': 0,
                'completed_inference': 0,
                'total_occurrences': 0,
                'average_count': 0.0,
                'active_groups': 0
            }

        try:
            async with self.db_engine.get_session() as session:
                repo = JargonRepository(session)
                return await repo.get_jargon_statistics(chat_id=chat_id)

        except Exception as e:
            self._logger.error(f"è·å–é»‘è¯ç»Ÿè®¡å¤±è´¥ï¼ˆORMï¼‰: {e}", exc_info=True)
            return {
                'total_candidates': 0,
                'confirmed_jargon': 0,
                'completed_inference': 0,
                'total_occurrences': 0,
                'average_count': 0.0,
                'active_groups': 0
            }

    async def get_jargon_by_id_orm(
        self,
        jargon_id: int
    ) -> Optional[Dict[str, Any]]:
        """
        æ ¹æ®IDè·å–é»‘è¯è®°å½•ï¼ˆä½¿ç”¨ ORMï¼‰

        Args:
            jargon_id: é»‘è¯è®°å½•ID

        Returns:
            Optional[Dict]: é»‘è¯è®°å½•æˆ–None
        """
        if not self.db_engine:
            self._logger.warning("DatabaseEngine æœªåˆå§‹åŒ–ï¼Œè¿”å› None")
            return None

        try:
            async with self.db_engine.get_session() as session:
                repo = JargonRepository(session)
                jargon = await repo.get_by_id(jargon_id)
                return jargon.to_dict() if jargon else None

        except Exception as e:
            self._logger.error(f"æ ¹æ®IDè·å–é»‘è¯å¤±è´¥ï¼ˆORMï¼‰: {e}", exc_info=True)
            return None

    async def update_jargon_status_orm(
        self,
        jargon_id: int,
        is_jargon: Optional[bool] = None,
        is_complete: Optional[bool] = None,
        meaning: Optional[str] = None
    ) -> bool:
        """
        æ›´æ–°é»‘è¯çŠ¶æ€ï¼ˆä½¿ç”¨ ORMï¼‰

        Args:
            jargon_id: é»‘è¯ID
            is_jargon: æ˜¯å¦ä¸ºé»‘è¯
            is_complete: æ˜¯å¦å®Œæˆæ¨ç†
            meaning: å«ä¹‰

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        if not self.db_engine:
            self._logger.warning("DatabaseEngine æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ›´æ–°é»‘è¯çŠ¶æ€")
            return False

        try:
            async with self.db_engine.get_session() as session:
                repo = JargonRepository(session)
                success = await repo.update_jargon_status(
                    jargon_id=jargon_id,
                    is_jargon=is_jargon,
                    is_complete=is_complete,
                    meaning=meaning
                )
                await session.commit()
                return success

        except Exception as e:
            self._logger.error(f"æ›´æ–°é»‘è¯çŠ¶æ€å¤±è´¥ï¼ˆORMï¼‰: {e}", exc_info=True)
            return False

    # ==================== å­¦ä¹ ç³»ç»Ÿ ORM æ–¹æ³• ====================

    async def get_pending_style_reviews_orm(self, limit: int = 50) -> List[Dict[str, Any]]:
        """è·å–å¾…å®¡æŸ¥çš„é£æ ¼å­¦ä¹ è®°å½•ï¼ˆä½¿ç”¨ ORMï¼‰"""
        if not self.db_engine:
            self._logger.warning("DatabaseEngine æœªåˆå§‹åŒ–ï¼Œè¿”å›ç©ºåˆ—è¡¨")
            return []

        try:
            async with self.db_engine.get_session() as session:
                repo = StyleLearningReviewRepository(session)
                reviews = await repo.get_by_status(status='pending', limit=limit)

                # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼Œä¿æŒä¸ä¼ ç»Ÿæ–¹æ³•ç›¸åŒçš„æ ¼å¼
                result = []
                for review in reviews:
                    review_dict = review.to_dict()

                    # è§£æ learned_patterns JSON å­—ç¬¦ä¸²
                    learned_patterns = []
                    try:
                        if review_dict.get('learned_patterns'):
                            import json
                            learned_patterns = json.loads(review_dict['learned_patterns'])
                    except json.JSONDecodeError:
                        pass

                    result.append({
                        'id': review_dict['id'],
                        'type': review_dict['type'],
                        'group_id': review_dict['group_id'],
                        'timestamp': review_dict['timestamp'],
                        'learned_patterns': learned_patterns,
                        'few_shots_content': review_dict['few_shots_content'],
                        'status': review_dict['status'],
                        'description': review_dict['description'],
                        'created_at': review_dict['created_at']
                    })

                return result

        except Exception as e:
            self._logger.error(f"è·å–å¾…å®¡æŸ¥é£æ ¼å­¦ä¹ è®°å½•å¤±è´¥ï¼ˆORMï¼‰: {e}", exc_info=True)
            return []

    async def update_style_review_status_orm(
        self,
        review_id: int,
        status: str,
        group_id: str = None
    ) -> bool:
        """æ›´æ–°é£æ ¼å­¦ä¹ å®¡æŸ¥çŠ¶æ€ï¼ˆä½¿ç”¨ ORMï¼‰"""
        if not self.db_engine:
            self._logger.warning("DatabaseEngine æœªåˆå§‹åŒ–")
            return False

        try:
            async with self.db_engine.get_session() as session:
                repo = StyleLearningReviewRepository(session)

                import time
                success = await repo.update(
                    review_id,
                    status=status,
                    updated_at=time.time()
                )

                await session.commit()

                if success:
                    self._logger.info(f"æ›´æ–°é£æ ¼å­¦ä¹ å®¡æŸ¥çŠ¶æ€æˆåŠŸï¼ˆORMï¼‰: ID={review_id}, çŠ¶æ€={status}")
                else:
                    self._logger.warning(f"æ›´æ–°é£æ ¼å­¦ä¹ å®¡æŸ¥çŠ¶æ€å¤±è´¥ï¼ˆORMï¼‰: æœªæ‰¾åˆ°ID={review_id}çš„è®°å½•")

                return success

        except Exception as e:
            self._logger.error(f"æ›´æ–°é£æ ¼å­¦ä¹ å®¡æŸ¥çŠ¶æ€å¤±è´¥ï¼ˆORMï¼‰: {e}", exc_info=True)
            return False

    async def get_style_progress_data_orm(self) -> List[Dict[str, Any]]:
        """è·å–é£æ ¼è¿›åº¦æ•°æ®ï¼ˆä½¿ç”¨ ORMï¼‰"""
        if not self.db_engine:
            self._logger.warning("DatabaseEngine æœªåˆå§‹åŒ–ï¼Œè¿”å›ç©ºåˆ—è¡¨")
            return []

        try:
            async with self.db_engine.get_session() as session:
                repo = LearningBatchRepository(session)

                # è·å–æœ€è¿‘30æ¡æœ‰è´¨é‡åˆ†æ•°å’Œæ¶ˆæ¯çš„å­¦ä¹ æ‰¹æ¬¡
                from sqlalchemy import select, and_
                from ..models.orm import LearningBatch

                stmt = select(LearningBatch).where(
                    and_(
                        LearningBatch.quality_score.isnot(None),
                        LearningBatch.processed_messages > 0
                    )
                ).order_by(LearningBatch.start_time.desc()).limit(30)

                result = await session.execute(stmt)
                batches = list(result.scalars().all())

                self._logger.debug(f"get_style_progress_data_orm è·å–åˆ° {len(batches)} è¡Œæ•°æ®")

                progress_data = []
                for batch in batches:
                    try:
                        progress_item = {
                            'group_id': batch.group_id,
                            'timestamp': float(batch.start_time) if batch.start_time else 0,
                            'quality_score': float(batch.quality_score) if batch.quality_score else 0,
                            'success': bool(batch.success)
                        }

                        # æ·»åŠ æ¶ˆæ¯æ•°é‡ä¿¡æ¯
                        if batch.processed_messages is not None:
                            progress_item['processed_messages'] = int(batch.processed_messages)
                        if batch.filtered_count is not None:
                            progress_item['filtered_count'] = int(batch.filtered_count)
                        if batch.batch_name:
                            progress_item['batch_name'] = batch.batch_name
                        else:
                            progress_item['batch_name'] = 'æœªå‘½å'

                        progress_data.append(progress_item)

                    except Exception as row_error:
                        self._logger.warning(f"å¤„ç†å­¦ä¹ æ‰¹æ¬¡è¿›åº¦æ•°æ®è¡Œæ—¶å‡ºé”™ï¼ˆORMï¼‰ï¼Œè·³è¿‡: {row_error}")

                return progress_data

        except Exception as e:
            self._logger.error(f"ä»learning_batchesè¡¨è·å–è¿›åº¦æ•°æ®å¤±è´¥ï¼ˆORMï¼‰: {e}", exc_info=True)
            return []

    # ==================== äººæ ¼å­¦ä¹ å®¡æŸ¥ç³»ç»Ÿ ORM æ–¹æ³• ====================

    async def get_pending_persona_learning_reviews_orm(self, limit: int = 50) -> List[Dict[str, Any]]:
        """è·å–å¾…å®¡æŸ¥çš„äººæ ¼å­¦ä¹ è®°å½•ï¼ˆä½¿ç”¨ ORMï¼‰"""
        if not self.db_engine:
            self._logger.warning("DatabaseEngine æœªåˆå§‹åŒ–ï¼Œè¿”å›ç©ºåˆ—è¡¨")
            return []

        try:
            async with self.db_engine.get_session() as session:
                repo = PersonaLearningReviewRepository(session)
                reviews = await repo.get_pending_reviews(limit=limit)

                # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼Œä¿æŒä¸ä¼ ç»Ÿæ–¹æ³•ç›¸åŒçš„æ ¼å¼
                result = []
                for review in reviews:
                    review_dict = review.to_dict()

                    # è§£æ metadata JSON å­—ç¬¦ä¸²
                    metadata = {}
                    try:
                        if review_dict.get('metadata'):
                            import json
                            metadata = json.loads(review_dict['metadata'])
                    except json.JSONDecodeError:
                        pass

                    # ç¡®ä¿æœ‰proposed_contentå­—æ®µï¼Œå¦‚æœä¸ºç©ºåˆ™ä½¿ç”¨new_content
                    proposed_content = review_dict.get('proposed_content') or review_dict.get('new_content')
                    confidence_score = review_dict.get('confidence_score') if review_dict.get('confidence_score') is not None else 0.5

                    result.append({
                        'id': review_dict['id'],
                        'timestamp': review_dict['timestamp'],
                        'group_id': review_dict['group_id'],
                        'update_type': review_dict['update_type'],
                        'original_content': review_dict['original_content'],
                        'new_content': review_dict['new_content'],
                        'proposed_content': proposed_content,
                        'confidence_score': confidence_score,
                        'reason': review_dict['reason'],
                        'status': review_dict['status'],
                        'reviewer_comment': review_dict['reviewer_comment'],
                        'review_time': review_dict['review_time'],
                        'metadata': metadata
                    })

                return result

        except Exception as e:
            self._logger.error(f"è·å–å¾…å®¡æŸ¥äººæ ¼å­¦ä¹ è®°å½•å¤±è´¥ï¼ˆORMï¼‰: {e}", exc_info=True)
            return []

    async def update_persona_learning_review_status_orm(
        self,
        review_id: int,
        status: str,
        comment: str = None,
        modified_content: str = None
    ) -> bool:
        """æ›´æ–°äººæ ¼å­¦ä¹ å®¡æŸ¥çŠ¶æ€ï¼ˆä½¿ç”¨ ORMï¼‰"""
        if not self.db_engine:
            self._logger.warning("DatabaseEngine æœªåˆå§‹åŒ–")
            return False

        try:
            async with self.db_engine.get_session() as session:
                repo = PersonaLearningReviewRepository(session)

                import time
                update_data = {
                    'status': status,
                    'review_time': time.time()
                }

                if comment:
                    update_data['reviewer_comment'] = comment

                # å¦‚æœæœ‰ä¿®æ”¹åçš„å†…å®¹ï¼Œä¹Ÿè¦æ›´æ–°proposed_contentå’Œnew_contentå­—æ®µ
                if modified_content:
                    update_data['proposed_content'] = modified_content
                    update_data['new_content'] = modified_content

                success = await repo.update(review_id, **update_data)
                await session.commit()

                if success:
                    self._logger.info(f"æ›´æ–°äººæ ¼å­¦ä¹ å®¡æŸ¥çŠ¶æ€æˆåŠŸï¼ˆORMï¼‰: ID={review_id}, çŠ¶æ€={status}")
                else:
                    self._logger.warning(f"æ›´æ–°äººæ ¼å­¦ä¹ å®¡æŸ¥çŠ¶æ€å¤±è´¥ï¼ˆORMï¼‰: æœªæ‰¾åˆ°ID={review_id}çš„è®°å½•")

                return success

        except Exception as e:
            self._logger.error(f"æ›´æ–°äººæ ¼å­¦ä¹ å®¡æŸ¥çŠ¶æ€å¤±è´¥ï¼ˆORMï¼‰: {e}", exc_info=True)
            return False

    async def get_persona_learning_review_by_id_orm(self, review_id: int) -> Optional[Dict[str, Any]]:
        """æ ¹æ®IDè·å–äººæ ¼å­¦ä¹ å®¡æŸ¥è®°å½•ï¼ˆä½¿ç”¨ ORMï¼‰"""
        if not self.db_engine:
            self._logger.warning("DatabaseEngine æœªåˆå§‹åŒ–")
            return None

        try:
            async with self.db_engine.get_session() as session:
                repo = PersonaLearningReviewRepository(session)
                review = await repo.get_by_id(review_id)

                if not review:
                    return None

                review_dict = review.to_dict()

                # è§£æ metadata JSON å­—ç¬¦ä¸²
                metadata = {}
                try:
                    if review_dict.get('metadata'):
                        import json
                        metadata = json.loads(review_dict['metadata'])
                except json.JSONDecodeError:
                    pass

                # ç¡®ä¿æœ‰proposed_contentå­—æ®µ
                proposed_content = review_dict.get('proposed_content') or review_dict.get('new_content')
                confidence_score = review_dict.get('confidence_score') if review_dict.get('confidence_score') is not None else 0.5

                return {
                    'id': review_dict['id'],
                    'timestamp': review_dict['timestamp'],
                    'group_id': review_dict['group_id'],
                    'update_type': review_dict['update_type'],
                    'original_content': review_dict['original_content'],
                    'new_content': review_dict['new_content'],
                    'proposed_content': proposed_content,
                    'confidence_score': confidence_score,
                    'reason': review_dict['reason'],
                    'status': review_dict['status'],
                    'reviewer_comment': review_dict['reviewer_comment'],
                    'review_time': review_dict['review_time'],
                    'metadata': metadata
                }

        except Exception as e:
            self._logger.error(f"æ ¹æ®IDè·å–äººæ ¼å­¦ä¹ å®¡æŸ¥è®°å½•å¤±è´¥ï¼ˆORMï¼‰: {e}", exc_info=True)
            return None

    async def delete_persona_learning_review_by_id_orm(self, review_id: int) -> bool:
        """åˆ é™¤æŒ‡å®šIDçš„äººæ ¼å­¦ä¹ å®¡æŸ¥è®°å½•ï¼ˆä½¿ç”¨ ORMï¼‰"""
        if not self.db_engine:
            self._logger.warning("DatabaseEngine æœªåˆå§‹åŒ–")
            return False

        try:
            async with self.db_engine.get_session() as session:
                repo = PersonaLearningReviewRepository(session)
                success = await repo.delete(review_id)
                await session.commit()

                if success:
                    self._logger.info(f"åˆ é™¤äººæ ¼å­¦ä¹ å®¡æŸ¥è®°å½•æˆåŠŸï¼ˆORMï¼‰: ID={review_id}")
                else:
                    self._logger.warning(f"åˆ é™¤äººæ ¼å­¦ä¹ å®¡æŸ¥è®°å½•å¤±è´¥ï¼ˆORMï¼‰: æœªæ‰¾åˆ°ID={review_id}çš„è®°å½•")

                return success

        except Exception as e:
            self._logger.error(f"åˆ é™¤äººæ ¼å­¦ä¹ å®¡æŸ¥è®°å½•å¤±è´¥ï¼ˆORMï¼‰: {e}", exc_info=True)
            return False

    async def get_reviewed_persona_learning_updates_orm(
        self,
        limit: int = 50,
        offset: int = 0,
        status_filter: str = None
    ) -> List[Dict[str, Any]]:
        """è·å–å·²å®¡æŸ¥çš„äººæ ¼å­¦ä¹ æ›´æ–°è®°å½•ï¼ˆä½¿ç”¨ ORMï¼‰"""
        if not self.db_engine:
            self._logger.warning("DatabaseEngine æœªåˆå§‹åŒ–ï¼Œè¿”å›ç©ºåˆ—è¡¨")
            return []

        try:
            async with self.db_engine.get_session() as session:
                repo = PersonaLearningReviewRepository(session)
                reviews = await repo.get_reviewed_updates(
                    limit=limit,
                    offset=offset,
                    status_filter=status_filter
                )

                # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                result = []
                for review in reviews:
                    review_dict = review.to_dict()

                    # è§£æ metadata JSON å­—ç¬¦ä¸²
                    metadata = {}
                    try:
                        if review_dict.get('metadata'):
                            import json
                            metadata = json.loads(review_dict['metadata'])
                    except json.JSONDecodeError:
                        pass

                    # ç¡®ä¿æœ‰proposed_contentå­—æ®µ
                    proposed_content = review_dict.get('proposed_content') or review_dict.get('new_content')
                    confidence_score = review_dict.get('confidence_score') if review_dict.get('confidence_score') is not None else 0.5

                    result.append({
                        'id': review_dict['id'],
                        'timestamp': review_dict['timestamp'],
                        'group_id': review_dict['group_id'],
                        'update_type': review_dict['update_type'],
                        'original_content': review_dict['original_content'],
                        'new_content': review_dict['new_content'],
                        'proposed_content': proposed_content,
                        'confidence_score': confidence_score,
                        'reason': review_dict['reason'],
                        'status': review_dict['status'],
                        'reviewer_comment': review_dict['reviewer_comment'],
                        'review_time': review_dict['review_time'],
                        'metadata': metadata
                    })

                return result

        except Exception as e:
            self._logger.error(f"è·å–å·²å®¡æŸ¥äººæ ¼å­¦ä¹ æ›´æ–°è®°å½•å¤±è´¥ï¼ˆORMï¼‰: {e}", exc_info=True)
            return []

    async def get_reviewed_style_learning_updates_orm(
        self,
        limit: int = 50,
        offset: int = 0,
        status_filter: str = None
    ) -> List[Dict[str, Any]]:
        """è·å–å·²å®¡æŸ¥çš„é£æ ¼å­¦ä¹ æ›´æ–°è®°å½•ï¼ˆä½¿ç”¨ ORMï¼‰"""
        if not self.db_engine:
            self._logger.warning("DatabaseEngine æœªåˆå§‹åŒ–ï¼Œè¿”å›ç©ºåˆ—è¡¨")
            return []

        try:
            async with self.db_engine.get_session() as session:
                from sqlalchemy import select, or_, func as sql_func, case
                from ..models.orm import StyleLearningReview

                # æ„å»ºæŸ¥è¯¢
                stmt = select(StyleLearningReview)

                # çŠ¶æ€è¿‡æ»¤
                if status_filter:
                    stmt = stmt.where(StyleLearningReview.status == status_filter)
                else:
                    stmt = stmt.where(
                        or_(
                            StyleLearningReview.status == 'approved',
                            StyleLearningReview.status == 'rejected'
                        )
                    )

                # æ’åºï¼šä½¿ç”¨updated_atï¼Œå¦‚æœä¸ºNULLåˆ™ä½¿ç”¨timestamp
                stmt = stmt.order_by(
                    sql_func.coalesce(StyleLearningReview.updated_at, StyleLearningReview.timestamp).desc()
                ).offset(offset).limit(limit)

                result = await session.execute(stmt)
                reviews = list(result.scalars().all())

                # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                updates = []
                for review in reviews:
                    review_dict = review.to_dict()

                    # å°è¯•è§£ælearned_patternsä»¥è·å–æ›´å¤šä¿¡æ¯
                    try:
                        import json
                        learned_patterns = json.loads(review_dict['learned_patterns']) if review_dict.get('learned_patterns') else {}
                        reason = learned_patterns.get('reason', 'é£æ ¼å­¦ä¹ æ›´æ–°')
                        original_content = learned_patterns.get('original_content', 'åŸå§‹é£æ ¼ç‰¹å¾')
                        proposed_content = learned_patterns.get('proposed_content', review_dict.get('learned_patterns', ''))
                        confidence_score = learned_patterns.get('confidence_score', 0.8)
                    except (json.JSONDecodeError, AttributeError):
                        reason = review_dict.get('description', 'é£æ ¼å­¦ä¹ æ›´æ–°')
                        original_content = 'åŸå§‹é£æ ¼ç‰¹å¾'
                        proposed_content = review_dict.get('learned_patterns', 'æ— å†…å®¹')
                        confidence_score = 0.8

                    updates.append({
                        'id': review_dict['id'],
                        'group_id': review_dict['group_id'],
                        'original_content': original_content,
                        'proposed_content': proposed_content,
                        'confidence_score': confidence_score,
                        'reason': reason,
                        'update_type': review_dict.get('type', 'style'),
                        'timestamp': review_dict['timestamp'],
                        'status': review_dict['status'],
                        'reviewer_comment': None,
                        'review_time': review_dict.get('updated_at', review_dict['timestamp'])
                    })

                return updates

        except Exception as e:
            self._logger.error(f"è·å–å·²å®¡æŸ¥é£æ ¼å­¦ä¹ æ›´æ–°è®°å½•å¤±è´¥ï¼ˆORMï¼‰: {e}", exc_info=True)
            return []

    async def delete_style_review_by_id_orm(self, review_id: int) -> bool:
        """åˆ é™¤æŒ‡å®šIDçš„é£æ ¼å­¦ä¹ å®¡æŸ¥è®°å½•ï¼ˆä½¿ç”¨ ORMï¼‰"""
        if not self.db_engine:
            self._logger.warning("DatabaseEngine æœªåˆå§‹åŒ–")
            return False

        try:
            async with self.db_engine.get_session() as session:
                repo = StyleLearningReviewRepository(session)
                success = await repo.delete(review_id)
                await session.commit()

                if success:
                    self._logger.info(f"æˆåŠŸåˆ é™¤é£æ ¼å­¦ä¹ å®¡æŸ¥è®°å½•ï¼ˆORMï¼‰ï¼ŒID: {review_id}")
                else:
                    self._logger.warning(f"æœªæ‰¾åˆ°è¦åˆ é™¤çš„é£æ ¼å­¦ä¹ å®¡æŸ¥è®°å½•ï¼ˆORMï¼‰ï¼ŒID: {review_id}")

                return success

        except Exception as e:
            self._logger.error(f"åˆ é™¤é£æ ¼å­¦ä¹ å®¡æŸ¥è®°å½•å¤±è´¥ï¼ˆORMï¼‰: {e}", exc_info=True)
            return False

    async def search_jargon_orm(
        self,
        keyword: str,
        chat_id: Optional[str] = None,
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """æœç´¢é»‘è¯ï¼ˆä½¿ç”¨ ORMï¼‰"""
        if not self.db_engine:
            self._logger.warning("DatabaseEngine æœªåˆå§‹åŒ–ï¼Œè¿”å›ç©ºåˆ—è¡¨")
            return []

        try:
            async with self.db_engine.get_session() as session:
                from sqlalchemy import select, and_, or_, desc
                from ..models.orm import Jargon

                # æ„å»ºæŸ¥è¯¢
                stmt = select(Jargon)

                if chat_id:
                    # æœç´¢æŒ‡å®šç¾¤ç»„çš„é»‘è¯
                    stmt = stmt.where(
                        and_(
                            Jargon.chat_id == chat_id,
                            Jargon.content.like(f'%{keyword}%'),
                            Jargon.is_jargon == True
                        )
                    )
                else:
                    # æœç´¢å…¨å±€é»‘è¯
                    stmt = stmt.where(
                        and_(
                            Jargon.content.like(f'%{keyword}%'),
                            Jargon.is_jargon == True,
                            Jargon.is_global == True
                        )
                    )

                stmt = stmt.order_by(
                    desc(Jargon.count),
                    desc(Jargon.updated_at)
                ).limit(limit)

                result = await session.execute(stmt)
                jargons = list(result.scalars().all())

                # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                results = []
                for jargon in jargons:
                    results.append({
                        'id': jargon.id,
                        'content': jargon.content,
                        'meaning': jargon.meaning,
                        'is_jargon': bool(jargon.is_jargon),
                        'count': jargon.count,
                        'is_complete': bool(jargon.is_complete)
                    })

                return results

        except Exception as e:
            self._logger.error(f"æœç´¢é»‘è¯å¤±è´¥ï¼ˆORMï¼‰: {e}", exc_info=True)
            return []

    async def delete_jargon_by_id_orm(self, jargon_id: int) -> bool:
        """æ ¹æ®IDåˆ é™¤é»‘è¯è®°å½•ï¼ˆä½¿ç”¨ ORMï¼‰"""
        if not self.db_engine:
            self._logger.warning("DatabaseEngine æœªåˆå§‹åŒ–")
            return False

        try:
            async with self.db_engine.get_session() as session:
                repo = JargonRepository(session)
                success = await repo.delete(jargon_id)
                await session.commit()

                if success:
                    self._logger.debug(f"åˆ é™¤é»‘è¯è®°å½•æˆåŠŸï¼ˆORMï¼‰, ID: {jargon_id}")

                return success

        except Exception as e:
            self._logger.error(f"åˆ é™¤é»‘è¯è®°å½•å¤±è´¥ï¼ˆORMï¼‰: {e}", exc_info=True)
            return False


