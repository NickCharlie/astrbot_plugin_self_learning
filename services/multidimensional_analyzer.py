"""
å¤šç»´åº¦å­¦ä¹ å¼•æ“ - å…¨æ–¹ä½åˆ†æç”¨æˆ·ç‰¹å¾å’Œç¤¾äº¤å…³ç³» - ç”¨æˆ·ç”»åƒ
"""
import re
import json
import time
import asyncio # ç¡®ä¿ asyncio å¯¼å…¥
from typing import Dict, List, Optional, Any, Set
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from collections import defaultdict, Counter
import emoji # å¯¼å…¥ emoji åº“

from astrbot.api import logger
from astrbot.api.event import AstrMessageEvent

from ..config import PluginConfig

from ..exceptions import StyleAnalysisError

from ..core.framework_llm_adapter import FrameworkLLMAdapter # å¯¼å…¥æ¡†æ¶é€‚é…å™¨

from .database_manager import DatabaseManager

from ..utils.json_utils import safe_parse_llm_json


@dataclass
class UserProfile:
    """ç”¨æˆ·ç”»åƒ"""
    qq_id: str
    qq_name: str
    nicknames: List[str] = None
    activity_pattern: Dict[str, Any] = None
    communication_style: Dict[str, float] = None
    social_connections: List[str] = None
    topic_preferences: Dict[str, float] = None
    emotional_tendency: Dict[str, float] = None
    last_active: float = None  # æ·»åŠ ç¼ºå¤±çš„å­—æ®µ
    
    def __post_init__(self):
        if self.nicknames is None:
            self.nicknames = []
        if self.activity_pattern is None:
            self.activity_pattern = {}
        if self.communication_style is None:
            self.communication_style = {}
        if self.social_connections is None:
            self.social_connections = []
        if self.topic_preferences is None:
            self.topic_preferences = {}
        if self.emotional_tendency is None:
            self.emotional_tendency = {}
        if self.last_active is None:
            self.last_active = time.time()


@dataclass
class SocialRelation:
    """ç¤¾äº¤å…³ç³»"""
    from_user: str
    to_user: str
    relation_type: str  # mention, reply, frequent_interaction
    strength: float  # å…³ç³»å¼ºåº¦ 0-1
    frequency: int   # äº¤äº’é¢‘æ¬¡
    last_interaction: str


@dataclass
class ContextualPattern:
    """æƒ…å¢ƒæ¨¡å¼"""
    context_type: str  # time_based, topic_based, social_based
    pattern_name: str
    triggers: List[str]
    characteristics: Dict[str, Any]
    confidence: float


class MultidimensionalAnalyzer:
    """å¤šç»´åº¦åˆ†æå™¨"""
    
    def __init__(self, config: PluginConfig, db_manager: DatabaseManager, context=None,
                 llm_adapter: Optional[FrameworkLLMAdapter] = None,
                 prompts: Any = None, temporary_persona_updater = None): # æ·»åŠ  prompts å‚æ•°å’Œä¸´æ—¶äººæ ¼æ›´æ–°å™¨
        self.config = config
        self.context = context
        self.db_manager: DatabaseManager = db_manager # ç›´æ¥ä¼ å…¥ DatabaseManager å®ä¾‹
        self.prompts = prompts # ä¿å­˜ prompts
        self.temporary_persona_updater = temporary_persona_updater # ä¿å­˜ä¸´æ—¶äººæ ¼æ›´æ–°å™¨å¼•ç”¨

        # ä½¿ç”¨æ¡†æ¶é€‚é…å™¨
        self.llm_adapter = llm_adapter

        # å‹å¥½çš„é…ç½®çŠ¶æ€æç¤º
        if self.llm_adapter:
            if not self.llm_adapter.has_filter_provider():
                logger.info("ğŸ’¡ ç­›é€‰æ¨¡å‹æœªé…ç½®ï¼Œå°†ä½¿ç”¨ç®€åŒ–ç®—æ³•è¿›è¡Œæ¶ˆæ¯ç­›é€‰")
            if not self.llm_adapter.has_refine_provider():
                logger.info("ğŸ’¡ æç‚¼æ¨¡å‹æœªé…ç½®ï¼Œå°†ä½¿ç”¨ç®€åŒ–ç®—æ³•è¿›è¡Œæ·±åº¦åˆ†æ")
            if not self.llm_adapter.has_reinforce_provider():
                logger.info("ğŸ’¡ å¼ºåŒ–æ¨¡å‹æœªé…ç½®ï¼Œå°†è·³è¿‡å¼ºåŒ–å­¦ä¹ åŠŸèƒ½")
        else:
            logger.info("ğŸ’¡ æ¡†æ¶LLMé€‚é…å™¨æœªé…ç½®ï¼Œå°†ä½¿ç”¨ç®€åŒ–ç®—æ³•è¿›è¡Œåˆ†æ")
        
        # ç”¨æˆ·ç”»åƒå­˜å‚¨
        self.user_profiles: Dict[str, UserProfile] = {}
        
        # ç¤¾äº¤å…³ç³»å›¾è°±
        self.social_graph: Dict[str, List[SocialRelation]] = defaultdict(list)
        
        # æ˜µç§°æ˜ å°„è¡¨
        self.nickname_mapping: Dict[str, str] = {}  # nickname -> qq_id
        
        # æƒ…å¢ƒæ¨¡å¼åº“
        self.contextual_patterns: List[ContextualPattern] = []
        
        # è¯é¢˜åˆ†ç±»å™¨
        self.topic_keywords = {
            'æ—¥å¸¸èŠå¤©': ['åƒé¥­', 'ç¡è§‰', 'ä¸Šç­', 'ä¸‹ç­', 'ä¼‘æ¯', 'å¿™'],
            'æ¸¸æˆå¨±ä¹': ['æ¸¸æˆ', 'ç”µå½±', 'éŸ³ä¹', 'å°è¯´', 'åŠ¨æ¼«', 'ç»¼è‰º'],
            'å­¦ä¹ å·¥ä½œ': ['å­¦ä¹ ', 'å·¥ä½œ', 'é¡¹ç›®', 'è€ƒè¯•', 'ä¼šè®®', 'ä»»åŠ¡'],
            'æƒ…æ„Ÿäº¤æµ': ['å¼€å¿ƒ', 'éš¾è¿‡', 'ç”Ÿæ°”', 'æ‹…å¿ƒ', 'å…´å¥‹', 'æ— èŠ'],
            'æŠ€æœ¯è®¨è®º': ['ä»£ç ', 'ç¨‹åº', 'ç®—æ³•', 'æŠ€æœ¯', 'å¼€å‘', 'ç¼–ç¨‹'],
            'ç”Ÿæ´»åˆ†äº«': ['æ—…æ¸¸', 'ç¾é£Ÿ', 'è´­ç‰©', 'å¥èº«', 'å® ç‰©', 'å®¶åº­']
        }
        
        logger.info("å¤šç»´åº¦å­¦ä¹ å¼•æ“åˆå§‹åŒ–å®Œæˆ")

    async def start(self):
        """æœåŠ¡å¯åŠ¨æ—¶åŠ è½½ç”¨æˆ·ç”»åƒå’Œç¤¾äº¤å…³ç³»"""
        try:
            logger.info("å¤šç»´åº¦åˆ†æå™¨å¯åŠ¨ä¸­...")
            
            # åˆå§‹åŒ–ç”¨æˆ·ç”»åƒå­˜å‚¨
            self.user_profiles = {}
            self.social_graph = {}
            
            # ä»æ•°æ®åº“åŠ è½½å·²æœ‰çš„ç”¨æˆ·ç”»åƒæ•°æ®
            try:
                await self._load_user_profiles_from_db()
            except Exception as e:
                logger.warning(f"ä»æ•°æ®åº“åŠ è½½ç”¨æˆ·ç”»åƒå¤±è´¥: {e}")
            
            # ä»æ•°æ®åº“åŠ è½½ç¤¾äº¤å…³ç³»æ•°æ®
            try:
                await self._load_social_relations_from_db()
            except Exception as e:
                logger.warning(f"ä»æ•°æ®åº“åŠ è½½ç¤¾äº¤å…³ç³»å¤±è´¥: {e}")
            
            # åˆå§‹åŒ–åˆ†æç¼“å­˜
            self._analysis_cache = {}
            self._cache_timeout = 3600  # 1å°æ—¶ç¼“å­˜
            
            # å¯åŠ¨å®šæœŸæ¸…ç†ä»»åŠ¡
            self._cleanup_task = asyncio.create_task(self._periodic_cleanup())
            
            logger.info(f"å¤šç»´åº¦åˆ†æå™¨å¯åŠ¨å®Œæˆï¼Œå·²åŠ è½½ {len(self.user_profiles)} ä¸ªç”¨æˆ·ç”»åƒï¼Œ{len(self.social_graph)} ä¸ªç¤¾äº¤å…³ç³»")
            
        except Exception as e:
            logger.error(f"å¤šç»´åº¦åˆ†æå™¨å¯åŠ¨å¤±è´¥: {e}")
            raise
    
    async def _load_user_profiles_from_db(self):
        """ä»æ•°æ®åº“åŠ è½½ç”¨æˆ·ç”»åƒ"""
        try:
            # è·å–æ‰€æœ‰æ´»è·ƒç¾¤ç»„
            async with self.db_manager.get_db_connection() as conn:
                cursor = await conn.cursor()
                
                # æŸ¥è¯¢æœ€è¿‘æ´»è·ƒçš„ç”¨æˆ·
                await cursor.execute('''
                    SELECT group_id, sender_id, MAX(sender_name) as sender_name, COUNT(*) as msg_count
                    FROM raw_messages
                    WHERE timestamp > ?
                    GROUP BY group_id, sender_id
                    HAVING msg_count >= 5
                    ORDER BY msg_count DESC
                    LIMIT 500
                ''', (time.time() - 7 * 24 * 3600,))  # æœ€è¿‘7å¤©
                
                users = await cursor.fetchall()
                
                for group_id, sender_id, sender_name, msg_count in users:
                    if group_id and sender_id:
                        user_key = f"{group_id}:{sender_id}"
                        self.user_profiles[user_key] = {
                            'user_id': sender_id,
                            'name': sender_name or f"ç”¨æˆ·{sender_id}",
                            'group_id': group_id,
                            'message_count': msg_count,
                            'topics': [],
                            'communication_style': {},
                            'last_activity': time.time(),
                            'created_at': time.time()
                        }
                
                await cursor.close()
            
            logger.info(f"ä»æ•°æ®åº“åŠ è½½äº† {len(self.user_profiles)} ä¸ªç”¨æˆ·ç”»åƒ")
            
        except Exception as e:
            logger.error(f"ä»æ•°æ®åº“åŠ è½½ç”¨æˆ·ç”»åƒå¤±è´¥: {e}")
    
    async def _load_social_relations_from_db(self):
        """ä»æ•°æ®åº“åŠ è½½ç¤¾äº¤å…³ç³»"""
        try:
            # åˆå§‹åŒ–ç¤¾äº¤å›¾è°±
            self.social_graph = {}
            
            # åˆ†æç”¨æˆ·é—´çš„äº¤äº’å…³ç³»
            async with self.db_manager.get_db_connection() as conn:
                cursor = await conn.cursor()
                
                # æŸ¥è¯¢ç”¨æˆ·åœ¨åŒä¸€ç¾¤ç»„ä¸­çš„äº¤äº’
                await cursor.execute('''
                    SELECT group_id, sender_id, COUNT(*) as interaction_count
                    FROM raw_messages 
                    WHERE timestamp > ? AND group_id IS NOT NULL
                    GROUP BY group_id, sender_id
                    HAVING interaction_count >= 3
                ''', (time.time() - 7 * 24 * 3600,))
                
                interactions = await cursor.fetchall()
                
                # æ„å»ºåŸºç¡€ç¤¾äº¤å…³ç³»
                for group_id, sender_id, count in interactions:
                    if sender_id not in self.social_graph:
                        self.social_graph[sender_id] = []
                    
                    # ä¸ºç®€åŒ–ï¼Œæš‚æ—¶è®°å½•ç”¨æˆ·åœ¨å„ç¾¤ç»„çš„æ´»è·ƒåº¦
                    relation_info = {
                        'target_user': group_id,
                        'relation_type': 'group_member',
                        'strength': min(1.0, count / 100.0),  # åŸºäºæ¶ˆæ¯æ•°é‡è®¡ç®—å…³ç³»å¼ºåº¦
                        'last_interaction': time.time()
                    }
                    self.social_graph[sender_id].append(relation_info)
                
                await cursor.close()
            
            logger.info(f"æ„å»ºäº† {len(self.social_graph)} ä¸ªç”¨æˆ·çš„ç¤¾äº¤å…³ç³»")
            
        except Exception as e:
            logger.error(f"åŠ è½½ç¤¾äº¤å…³ç³»å¤±è´¥: {e}")
    
    async def _periodic_cleanup(self):
        """å®šæœŸæ¸…ç†è¿‡æœŸç¼“å­˜å’Œæ•°æ®"""
        try:
            while True:
                await asyncio.sleep(3600)  # æ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡
                
                current_time = time.time()
                
                # æ¸…ç†åˆ†æç¼“å­˜
                if hasattr(self, '_analysis_cache'):
                    expired_keys = [
                        k for k, v in self._analysis_cache.items()
                        if current_time - v.get('timestamp', 0) > self._cache_timeout
                    ]
                    for key in expired_keys:
                        del self._analysis_cache[key]
                    
                    if expired_keys:
                        logger.debug(f"æ¸…ç†äº† {len(expired_keys)} ä¸ªè¿‡æœŸçš„åˆ†æç¼“å­˜")
                
                # æ¸…ç†è¿‡æœŸçš„ç”¨æˆ·æ´»åŠ¨è®°å½•
                cutoff_time = current_time - 30 * 24 * 3600  # 30å¤©å‰
                expired_users = [
                    k for k, v in self.user_profiles.items()
                    if v.get('last_activity', 0) < cutoff_time
                ]
                
                for user_key in expired_users:
                    del self.user_profiles[user_key]
                    
                if expired_users:
                    logger.info(f"æ¸…ç†äº† {len(expired_users)} ä¸ªè¿‡æœŸçš„ç”¨æˆ·ç”»åƒ")
                    
        except asyncio.CancelledError:
            logger.info("å®šæœŸæ¸…ç†ä»»åŠ¡å·²å–æ¶ˆ")
        except Exception as e:
            logger.error(f"å®šæœŸæ¸…ç†ä»»åŠ¡å¼‚å¸¸: {e}")

    async def filter_message_with_llm(self, message_text: str, current_persona_description: str) -> bool:
        """
        ä½¿ç”¨ LLM å¯¹æ¶ˆæ¯è¿›è¡Œæ™ºèƒ½ç­›é€‰ï¼Œåˆ¤æ–­å…¶æ˜¯å¦ä¸å½“å‰äººæ ¼åŒ¹é…ã€ç‰¹å¾é²œæ˜ä¸”æœ‰å­¦ä¹ æ„ä¹‰ã€‚
        è¿”å› True è¡¨ç¤ºæ¶ˆæ¯é€šè¿‡ç­›é€‰ï¼ŒFalse è¡¨ç¤ºä¸é€šè¿‡ã€‚
        """
        # ä½¿ç”¨æ¡†æ¶é€‚é…å™¨
        if self.llm_adapter and self.llm_adapter.has_filter_provider() and self.llm_adapter.providers_configured > 0:
            prompt = self.prompts.MULTIDIMENSIONAL_ANALYZER_FILTER_MESSAGE_PROMPT.format(
                current_persona_description=current_persona_description,
                message_text=message_text
            )
            try:
                response = await self.llm_adapter.filter_chat_completion(
                    prompt=prompt,
                    temperature=0.1
                )
                if response:
                    # è§£æç½®ä¿¡åº¦
                    numbers = re.findall(r'0\.\d+|1\.0|0', response.strip())
                    if numbers:
                        confidence = min(float(numbers[0]), 1.0)
                        logger.debug(f"æ¶ˆæ¯ç­›é€‰ç½®ä¿¡åº¦: {confidence} (é˜ˆå€¼: {self.config.confidence_threshold})")
                        return confidence >= self.config.confidence_threshold
                logger.warning(f"æ¡†æ¶é€‚é…å™¨ç­›é€‰æœªè¿”å›æœ‰æ•ˆç½®ä¿¡åº¦ï¼Œæ¶ˆæ¯é»˜è®¤ä¸é€šè¿‡ç­›é€‰ã€‚")
                return False
            except Exception as e:
                logger.error(f"LLMæ¶ˆæ¯ç­›é€‰å¤±è´¥: {e}")
                return False
        else:
            logger.warning("ç­›é€‰æ¨¡å‹æœªé…ç½®ï¼Œæ— æ³•è¿›è¡ŒLLMæ¶ˆæ¯ç­›é€‰ï¼Œè·³è¿‡æ­¤æ­¥éª¤")
            return True

    async def evaluate_message_quality_with_llm(self, message_text: str, current_persona_description: str) -> Dict[str, float]:
        """
        ä½¿ç”¨ LLM å¯¹æ¶ˆæ¯è¿›è¡Œå¤šç»´åº¦é‡åŒ–è¯„åˆ†ã€‚
        è¯„åˆ†ç»´åº¦åŒ…æ‹¬ï¼šå†…å®¹è´¨é‡ã€ç›¸å…³æ€§ã€æƒ…æ„Ÿç§¯ææ€§ã€äº’åŠ¨æ€§ã€å­¦ä¹ ä»·å€¼ã€‚
        è¿”å›ä¸€ä¸ªåŒ…å«å„ç»´åº¦è¯„åˆ†çš„å­—å…¸ã€‚
        """
        default_scores = {
            "content_quality": 0.5,
            "relevance": 0.5,
            "emotional_positivity": 0.5,
            "interactivity": 0.5,
            "learning_value": 0.5
        }

        # ä¼˜å…ˆä½¿ç”¨æ¡†æ¶é€‚é…å™¨
        if self.llm_adapter and self.llm_adapter.has_refine_provider() and self.llm_adapter.providers_configured >= 2:
            prompt = self.prompts.JSON_ONLY_SYSTEM_PROMPT + "\n\n" + self.prompts.MULTIDIMENSIONAL_ANALYZER_EVALUATE_MESSAGE_QUALITY_PROMPT.format(
                current_persona_description=current_persona_description,
                message_text=message_text
            )
            try:
                response = await self.llm_adapter.refine_chat_completion(prompt=prompt)
                if response:
                    scores = safe_parse_llm_json(response, fallback_result=default_scores)
                    
                    if scores and isinstance(scores, dict):
                        # ç¡®ä¿æ‰€æœ‰åˆ†æ•°éƒ½åœ¨0-1ä¹‹é—´
                        for key, value in scores.items():
                            scores[key] = max(0.0, min(float(value), 1.0))
                        logger.debug(f"æ¶ˆæ¯å¤šç»´åº¦è¯„åˆ†: {scores}")
                        return scores
                    else:
                        return default_scores
                logger.warning(f"LLMå¤šç»´åº¦è¯„åˆ†æ¨¡å‹æœªè¿”å›æœ‰æ•ˆå“åº”ï¼Œè¿”å›é»˜è®¤è¯„åˆ†ã€‚")
                return default_scores
            except Exception as e:
                logger.error(f"LLMå¤šç»´åº¦è¯„åˆ†å¤±è´¥: {e}")
                return default_scores
        else:
            logger.warning("æç‚¼æ¨¡å‹æœªé…ç½®ï¼Œæ— æ³•è¿›è¡Œæ¶ˆæ¯è´¨é‡è¯„åˆ†ï¼Œè¿”å›é»˜è®¤è¯„åˆ†")
            return default_scores

    def _debug_dict_keys(self, data: Dict, context_name: str = "") -> Dict:
        """è°ƒè¯•å­—å…¸é”®ï¼Œç¡®ä¿æ²¡æœ‰tupleé”®"""
        try:
            import json
            from collections import Counter
            
            # é€’å½’æ¸…ç†æ•°æ®ç»“æ„
            def clean_data(obj):
                if isinstance(obj, Counter):
                    # Counterè½¬æ¢ä¸ºæ™®é€šå­—å…¸
                    return dict(obj)
                elif isinstance(obj, dict):
                    # æ¸…ç†å­—å…¸çš„é”®å’Œå€¼
                    cleaned = {}
                    for key, value in obj.items():
                        # ç¡®ä¿é”®æ˜¯å¯åºåˆ—åŒ–çš„åŸºæœ¬ç±»å‹
                        if isinstance(key, (tuple, list, set)):
                            clean_key = str(key)
                        elif key is None:
                            clean_key = "null"
                        else:
                            clean_key = key
                        
                        cleaned[clean_key] = clean_data(value)
                    return cleaned
                elif isinstance(obj, (list, tuple)):
                    return [clean_data(item) for item in obj]
                elif hasattr(obj, '__dict__'):
                    # å¤„ç†è‡ªå®šä¹‰å¯¹è±¡
                    return clean_data(obj.__dict__)
                else:
                    return obj
            
            cleaned_data = clean_data(data)
            
            # å°è¯•åºåˆ—åŒ–ä»¥ç¡®è®¤æ²¡æœ‰é—®é¢˜
            json.dumps(cleaned_data)
            return cleaned_data
            
        except TypeError as e:
            logger.error(f"JSONåºåˆ—åŒ–é”™è¯¯åœ¨ {context_name}: {e}")
            logger.error(f"é—®é¢˜æ•°æ®ç±»å‹: {type(data)}")
            # è¿”å›ç©ºå­—å…¸é¿å…å´©æºƒ
            return {}

    def _clean_user_profiles(self):
        """æ¸…ç†user_profilesä¸­çš„é—®é¢˜æ•°æ®"""
        try:
            from collections import Counter
            
            profiles_to_fix = []
            for user_key, profile in self.user_profiles.items():
                if isinstance(profile, dict):
                    # æ£€æŸ¥å¹¶ä¿®å¤activity_patternä¸­çš„Counter
                    if 'activity_pattern' in profile and isinstance(profile['activity_pattern'], dict):
                        activity_pattern = profile['activity_pattern']
                        if 'activity_hours' in activity_pattern and isinstance(activity_pattern['activity_hours'], Counter):
                            profiles_to_fix.append((user_key, 'activity_hours'))
            
            # ä¿®å¤å‘ç°çš„é—®é¢˜
            for user_key, issue_type in profiles_to_fix:
                if issue_type == 'activity_hours':
                    counter_obj = self.user_profiles[user_key]['activity_pattern']['activity_hours']
                    self.user_profiles[user_key]['activity_pattern']['activity_hours'] = dict(counter_obj)
                    logger.debug(f"ä¿®å¤äº†ç”¨æˆ· {user_key} çš„Counterå¯¹è±¡")
                    
        except Exception as e:
            logger.error(f"æ¸…ç†user_profileså¤±è´¥: {e}")

    def _clean_profile_for_serialization(self, profile_dict: Dict[str, Any]) -> Dict[str, Any]:
        """æ¸…ç†ç”¨æˆ·æ¡£æ¡ˆæ•°æ®ï¼Œç¡®ä¿å¯ä»¥å®‰å…¨è¿›è¡ŒJSONåºåˆ—åŒ–"""
        try:
            from collections import Counter
            
            def clean_data_recursive(obj):
                if isinstance(obj, Counter):
                    # Counterè½¬æ¢ä¸ºæ™®é€šå­—å…¸ï¼Œç¡®ä¿é”®æ˜¯åŸºæœ¬ç±»å‹
                    return {str(k) if isinstance(k, (tuple, list, set)) else k: v for k, v in obj.items()}
                elif isinstance(obj, dict):
                    cleaned = {}
                    for key, value in obj.items():
                        # ç¡®ä¿é”®æ˜¯å¯åºåˆ—åŒ–çš„
                        clean_key = str(key) if isinstance(key, (tuple, list, set)) else key
                        if clean_key is None:
                            clean_key = "null"
                        cleaned[clean_key] = clean_data_recursive(value)
                    return cleaned
                elif isinstance(obj, (list, tuple)):
                    return [clean_data_recursive(item) for item in obj]
                else:
                    return obj
            
            cleaned_profile = clean_data_recursive(profile_dict)
            
            # å°è¯•JSONåºåˆ—åŒ–æµ‹è¯•
            import json
            json.dumps(cleaned_profile)
            
            return cleaned_profile
            
        except Exception as e:
            logger.error(f"æ¸…ç†ç”¨æˆ·æ¡£æ¡ˆæ•°æ®å¤±è´¥: {e}")
            # è¿”å›åŸºç¡€çš„ç©ºæ¡£æ¡ˆï¼Œç¡®ä¿ä¸ä¼šå´©æºƒ
            return {
                'qq_id': profile_dict.get('qq_id', ''),
                'qq_name': profile_dict.get('qq_name', ''),
                'nicknames': [],
                'activity_pattern': {},
                'communication_style': {},
                'topic_preferences': {},
                'emotional_tendency': {}
            }

    async def analyze_message_context(self, event: AstrMessageEvent, message_text: str) -> Dict[str, Any]:
        """åˆ†ææ¶ˆæ¯çš„å¤šç»´åº¦ä¸Šä¸‹æ–‡"""
        try:
            # æ£€æŸ¥eventæ˜¯å¦ä¸ºNone
            if event is None:
                logger.info("ä½¿ç”¨ç®€åŒ–åˆ†ææ–¹å¼ï¼ˆæ— eventå¯¹è±¡ï¼‰")
                return await self._analyze_message_context_without_event(message_text)
            
            sender_id = event.get_sender_id()
            sender_name = event.get_sender_name()
            group_id = event.get_group_id() or event.get_sender_id()  # ç§èŠæ—¶ä½¿ç”¨ sender_id ä½œä¸ºä¼šè¯ ID
            
            # é¢„å…ˆæ¸…ç†user_profilesä¸­çš„ä»»ä½•é—®é¢˜æ•°æ®
            self._clean_user_profiles()
            
            # æ›´æ–°ç”¨æˆ·ç”»åƒ
            await self._update_user_profile(group_id, sender_id, sender_name, message_text, event) # ä¼ å…¥ group_id
            
            # åˆ†æç¤¾äº¤å…³ç³»
            social_context = await self._analyze_social_context(event, message_text)
            
            # åˆ†æè¯é¢˜åå¥½
            topic_context = await self._analyze_topic_context(message_text)
            
            # åˆ†ææƒ…æ„Ÿå€¾å‘
            emotional_context = await self._analyze_emotional_context(message_text)
            
            # åˆ†ææ—¶é—´æ¨¡å¼
            temporal_context = await self._analyze_temporal_context(event)
            
            # åˆ†ææ²Ÿé€šé£æ ¼
            style_context = await self._analyze_communication_style(message_text)
            
            # æ„å»ºåˆ†æç»“æœï¼Œä½¿ç”¨ç®€åŒ–çš„æ¸…ç†æ–¹æ³•
            user_profile_data = self._get_user_profile_activity(group_id, sender_id)
            
            analysis_result = {
                'user_profile': user_profile_data,
                'social_context': social_context or {},
                'topic_context': topic_context or {},
                'emotional_context': emotional_context or {},
                'temporal_context': temporal_context or {},
                'style_context': style_context or {},
                'contextual_relevance': await self._calculate_contextual_relevance(
                    sender_id, message_text, event
                )
            }
            
            # ä½¿ç”¨æ¸…ç†æ–¹æ³•ç¡®ä¿æ²¡æœ‰åºåˆ—åŒ–é—®é¢˜
            analysis_result = self._debug_dict_keys(analysis_result, 'final_analysis_result')
            
            # å°è¯•å°†åˆ†æç»“æœé›†æˆåˆ°system_prompt
            if self.temporary_persona_updater:
                try:
                    # å‡†å¤‡å¤šç»´åº¦æ›´æ–°æ•°æ®
                    update_data = {}
                    
                    # ç”¨æˆ·æ¡£æ¡ˆæ›´æ–°
                    user_key = f"{group_id}:{sender_id}"
                    if user_key in self.user_profiles:
                        profile = self.user_profiles[user_key]
                        update_data['user_profile'] = {
                            'preferences': self._safe_get_profile_attr(profile, 'activity_pattern', {}).get('frequency', 'æ­£å¸¸'),
                            'communication_style': style_context.get('style_summary', ''),
                            'personality_traits': f"æƒ…æ„Ÿå€¾å‘{emotional_context.get('sentiment', 'ä¸­æ€§')}"
                        }
                    
                    # ç¤¾äº¤å…³ç³»æ›´æ–°
                    if social_context:
                        update_data['social_relationship'] = {
                            'user_relationships': social_context.get('relationship_level', ''),
                            'group_atmosphere': social_context.get('group_dynamics', ''),
                            'interaction_style': social_context.get('interaction_pattern', '')
                        }
                    
                    # ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ›´æ–°
                    if topic_context:
                        update_data['context_awareness'] = {
                            'current_topic': topic_context.get('main_topics', [''])[0] if topic_context.get('main_topics') else '',
                            'recent_focus': topic_context.get('topic_shift', ''),
                            'dialogue_flow': f"è¯é¢˜ç›¸å…³åº¦: {topic_context.get('relevance_score', 0)}"
                        }
                    
                    # åº”ç”¨ç»¼åˆæ›´æ–°åˆ°system_prompt
                    if update_data:
                        await self.temporary_persona_updater.apply_comprehensive_update_to_system_prompt(
                            group_id, update_data
                        )
                        logger.info(f"æˆåŠŸå°†å¤šç»´åº¦åˆ†æç»“æœé›†æˆåˆ°system_prompt: {group_id}")
                    
                except Exception as e:
                    logger.error(f"é›†æˆåˆ†æç»“æœåˆ°system_promptå¤±è´¥: {e}")
                    # è¿™é‡Œä¸æŠ›å‡ºå¼‚å¸¸ï¼Œè®©åˆ†æç»“æœæ­£å¸¸è¿”å›
            
            return analysis_result
            
        except Exception as e:
            logger.error(f"å¤šç»´åº¦ä¸Šä¸‹æ–‡åˆ†æå¤±è´¥: {e}")
            return {}

    async def analyze_message_batch(self, 
                                   message_text: str,
                                   sender_id: str = '',
                                   sender_name: str = '',
                                   group_id: str = '',
                                   timestamp: float = None) -> Dict[str, Any]:
        """
        æ‰¹é‡åˆ†ææ¶ˆæ¯ä¸Šä¸‹æ–‡ï¼ˆç”¨äºå­¦ä¹ æµç¨‹ä¸­çš„æ‰¹é‡å¤„ç†ï¼‰
        
        Args:
            message_text: æ¶ˆæ¯æ–‡æœ¬
            sender_id: å‘é€è€…ID
            sender_name: å‘é€è€…åç§°
            group_id: ç¾¤ç»„ID
            timestamp: æ—¶é—´æˆ³
            
        Returns:
            Dict[str, Any]: åˆ†æç»“æœ
        """
        try:
            logger.debug(f"æ‰¹é‡åˆ†ææ¶ˆæ¯: å‘é€è€…={sender_id}, ç¾¤ç»„={group_id}, æ¶ˆæ¯é•¿åº¦={len(message_text)}")
            
            # æ·»åŠ æ‰¹é‡åˆ†æè®¡æ•°å™¨å’Œé™åˆ¶
            if not hasattr(self, '_batch_analysis_count'):
                self._batch_analysis_count = {}
                self._batch_analysis_start_time = time.time()
            
            # é‡ç½®è®¡æ•°å™¨ï¼ˆæ¯å°æ—¶é‡ç½®ä¸€æ¬¡ï¼‰
            if time.time() - self._batch_analysis_start_time > 3600:
                self._batch_analysis_count = {}
                self._batch_analysis_start_time = time.time()
            
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æ‰¹é‡åˆ†æé™åˆ¶ï¼ˆæ¯å°æ—¶æœ€å¤š100æ¬¡LLMè°ƒç”¨ï¼‰
            current_time = time.time()
            hour_key = int(current_time // 3600)
            
            if hour_key not in self._batch_analysis_count:
                self._batch_analysis_count[hour_key] = 0
            
            if self._batch_analysis_count[hour_key] >= 100:
                logger.warning("æ‰¹é‡åˆ†æè¾¾åˆ°é™åˆ¶ï¼Œä½¿ç”¨ç®€åŒ–åˆ†æ")
                return await self._analyze_message_context_without_event(message_text)
            
            # å¢åŠ åˆ†æè®¡æ•°
            self._batch_analysis_count[hour_key] += 1
            
            # æ›´æ–°ç”¨æˆ·ç”»åƒï¼ˆå¦‚æœæœ‰è¶³å¤Ÿä¿¡æ¯ï¼‰
            if sender_id and group_id:
                await self._update_user_profile_batch(group_id, sender_id, sender_name, message_text, timestamp)
            
            # åˆ†æè¯é¢˜åå¥½
            topic_context = await self._analyze_topic_context(message_text)
            
            # åˆ†ææƒ…æ„Ÿå€¾å‘ï¼ˆå·²æœ‰ç¼“å­˜æœºåˆ¶ï¼‰
            emotional_context = await self._analyze_emotional_context(message_text)
            
            # åˆ†ææ²Ÿé€šé£æ ¼ï¼ˆæ·»åŠ é™åˆ¶ï¼‰
            style_context = {}
            if self._batch_analysis_count[hour_key] <= 50:  # é™åˆ¶é£æ ¼åˆ†æçš„è°ƒç”¨æ¬¡æ•°
                style_context = await self._analyze_communication_style(message_text)
            else:
                # ä½¿ç”¨ç®€åŒ–çš„é£æ ¼åˆ†æ
                style_context = {
                    'length_preference': len(message_text),
                    'emoji_usage': self._calculate_emoji_usage(message_text),
                    'punctuation_style': self._calculate_punctuation_style(message_text)
                }
            
            # è®¡ç®—ç›¸å…³æ€§å¾—åˆ†
            contextual_relevance = await self._calculate_enhanced_relevance(
                message_text, sender_id, group_id, timestamp
            )
            
            # æ„å»ºç®€åŒ–çš„ç¤¾äº¤ä¸Šä¸‹æ–‡
            social_context = {}
            if sender_id and group_id:
                social_context = await self._get_user_social_context(group_id, sender_id)
            
            return {
                'user_profile': self.user_profiles.get(f"{group_id}:{sender_id}", {}) if sender_id and group_id else {},
                'social_context': social_context,
                'topic_context': topic_context,
                'emotional_context': emotional_context,
                'temporal_context': {'timestamp': timestamp or time.time()},
                'style_context': style_context,
                'contextual_relevance': contextual_relevance
            }
            
        except Exception as e:
            logger.error(f"æ‰¹é‡æ¶ˆæ¯åˆ†æå¤±è´¥: {e}")
            # è¿”å›åŸºç¡€åˆ†æç»“æœ
            return await self._analyze_message_context_without_event(message_text)

    async def _update_user_profile_batch(self, group_id: str, sender_id: str, sender_name: str, 
                                       message_text: str, timestamp: float = None):
        """æ‰¹é‡æ›´æ–°ç”¨æˆ·ç”»åƒï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        try:
            user_key = f"{group_id}:{sender_id}"
            current_time = timestamp or time.time()
            
            if user_key not in self.user_profiles:
                # åˆ›å»ºUserProfileå¯¹è±¡è€Œä¸æ˜¯å­—å…¸
                profile = UserProfile(qq_id=sender_id, qq_name=sender_name)
                self.user_profiles[user_key] = profile
            else:
                profile = self.user_profiles[user_key]
                
            # æ›´æ–°åŸºæœ¬ä¿¡æ¯ - ä½¿ç”¨å±æ€§è®¿é—®è€Œä¸æ˜¯å­—å…¸è®¿é—®
            if hasattr(profile, 'message_count'):
                if isinstance(profile.communication_style, dict) and 'message_count' in profile.communication_style:
                    profile.communication_style['message_count'] += 1
                else:
                    profile.communication_style['message_count'] = 1
            
            # æ›´æ–°æ²Ÿé€šé£æ ¼
            style = await self._analyze_communication_style(message_text)
            if style:
                if not isinstance(profile.communication_style, dict):
                    profile.communication_style = {}
                profile.communication_style.update(style)
                
        except Exception as e:
            logger.error(f"æ‰¹é‡æ›´æ–°ç”¨æˆ·ç”»åƒå¤±è´¥: {e}")

    def _safe_get_profile_attr(self, profile, attr_name: str, default=None):
        """å®‰å…¨è·å–profileå±æ€§ï¼Œå…¼å®¹UserProfileå¯¹è±¡å’Œdict"""
        try:
            if isinstance(profile, dict):
                return profile.get(attr_name, default)
            else:
                return getattr(profile, attr_name, default)
        except Exception:
            return default

    def _get_user_profile_activity(self, group_id: str, sender_id: str) -> Dict[str, Any]:
        """å®‰å…¨è·å–ç”¨æˆ·æ¡£æ¡ˆæ´»åŠ¨æ¨¡å¼"""
        try:
            user_key = f"{group_id}:{sender_id}"
            if user_key in self.user_profiles:
                profile = self.user_profiles[user_key]
                if isinstance(profile, dict):
                    return profile.get('activity_pattern', {})
                else:
                    # UserProfileå¯¹è±¡
                    return getattr(profile, 'activity_pattern', {})
            return {}
        except Exception as e:
            logger.error(f"è·å–ç”¨æˆ·æ¡£æ¡ˆæ´»åŠ¨æ¨¡å¼å¤±è´¥: {e}")
            return {}

    async def _calculate_enhanced_relevance(self, message_text: str, sender_id: str = '', 
                                          group_id: str = '', timestamp: float = None) -> float:
        """è®¡ç®—å¢å¼ºçš„ç›¸å…³æ€§å¾—åˆ†"""
        try:
            # åŸºç¡€ç›¸å…³æ€§
            base_relevance = await self._calculate_basic_relevance(message_text)
            
            # ç”¨æˆ·æ´»è·ƒåº¦åŠ æˆ
            user_bonus = 0.0
            if sender_id and group_id:
                user_key = f"{group_id}:{sender_id}"
                if user_key in self.user_profiles:
                    user_profile = self.user_profiles[user_key]
                    # æ´»è·ƒç”¨æˆ·çš„æ¶ˆæ¯è·å¾—æ›´é«˜æƒé‡
                    message_count = user_profile.get('message_count', 0) if isinstance(user_profile, dict) else getattr(user_profile, 'message_count', 0)
                    if message_count > 10:
                        user_bonus = 0.1
                    
            return min(1.0, base_relevance + user_bonus)
            
        except Exception as e:
            logger.error(f"è®¡ç®—å¢å¼ºç›¸å…³æ€§å¤±è´¥: {e}")
            return await self._calculate_basic_relevance(message_text)

    async def _get_user_social_context(self, group_id: str, sender_id: str) -> Dict[str, Any]:
        """è·å–ç”¨æˆ·ç¤¾äº¤ä¸Šä¸‹æ–‡"""
        try:
            user_key = f"{group_id}:{sender_id}"
            if user_key in self.user_profiles:
                profile = self.user_profiles[user_key]
                message_count = self._safe_get_profile_attr(profile, 'message_count', 0)
                last_activity = self._safe_get_profile_attr(profile, 'last_activity', 0)
                return {
                    'message_count': message_count,
                    'activity_level': 'high' if message_count > 50 else 'low',
                    'last_activity': last_activity
                }
            return {}
            
        except Exception as e:
            logger.error(f"è·å–ç”¨æˆ·ç¤¾äº¤ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return {}

    async def _analyze_message_context_without_event(self, message_text: str) -> Dict[str, Any]:
        """åœ¨æ²¡æœ‰eventå¯¹è±¡æ—¶åˆ†ææ¶ˆæ¯ä¸Šä¸‹æ–‡ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        try:
            # åˆ†æè¯é¢˜åå¥½
            topic_context = await self._analyze_topic_context(message_text)
            
            # åˆ†ææƒ…æ„Ÿå€¾å‘
            emotional_context = await self._analyze_emotional_context(message_text)
            
            # åˆ†ææ²Ÿé€šé£æ ¼
            style_context = await self._analyze_communication_style(message_text)
            
            # è®¡ç®—åŸºç¡€ç›¸å…³æ€§å¾—åˆ†
            contextual_relevance = await self._calculate_basic_relevance(message_text)
            
            return {
                'user_profile': {},
                'social_context': {},
                'topic_context': topic_context,
                'emotional_context': emotional_context,
                'temporal_context': {},
                'style_context': style_context,
                'contextual_relevance': contextual_relevance
            }
            
        except Exception as e:
            logger.error(f"ç®€åŒ–ä¸Šä¸‹æ–‡åˆ†æå¤±è´¥: {e}")
            return {
                'user_profile': {},
                'social_context': {},
                'topic_context': {},
                'emotional_context': {},
                'temporal_context': {},
                'style_context': {},
                'contextual_relevance': 0.5
            }

    async def _calculate_basic_relevance(self, message_text: str) -> float:
        """è®¡ç®—åŸºç¡€ç›¸å…³æ€§å¾—åˆ†"""
        try:
            # åŸºäºæ¶ˆæ¯é•¿åº¦å’Œå†…å®¹è´¨é‡çš„ç®€å•è¯„åˆ†
            message_length = len(message_text.strip())
            if message_length < 5:
                return 0.2
            elif message_length < 20:
                return 0.4
            elif message_length < 50:
                return 0.6
            else:
                return 0.8
        except Exception:
            return 0.5

    async def _update_user_profile(self, group_id: str, qq_id: str, qq_name: str, message_text: str, event: AstrMessageEvent):
        """æ›´æ–°ç”¨æˆ·ç”»åƒå¹¶æŒä¹…åŒ–"""
        try:
            profile_data = await self.db_manager.load_user_profile(group_id, qq_id)
            if profile_data:
                profile = UserProfile(**profile_data)
            else:
                profile = UserProfile(qq_id=qq_id, qq_name=qq_name)
            
            # æ›´æ–°æ´»åŠ¨æ¨¡å¼
            current_hour = datetime.now().hour
            if 'activity_hours' not in profile.activity_pattern:
                profile.activity_pattern['activity_hours'] = Counter()
            elif not isinstance(profile.activity_pattern['activity_hours'], Counter):
                # å¦‚æœä»æ•°æ®åº“åŠ è½½çš„æ˜¯æ™®é€šå­—å…¸ï¼Œè½¬æ¢ä¸ºCounter
                profile.activity_pattern['activity_hours'] = Counter(profile.activity_pattern['activity_hours'])
            
            profile.activity_pattern['activity_hours'][current_hour] += 1
            
            # æ›´æ–°æ¶ˆæ¯é•¿åº¦åå¥½
            msg_length = len(message_text)
            if 'message_lengths' not in profile.activity_pattern:
                profile.activity_pattern['message_lengths'] = []
            profile.activity_pattern['message_lengths'].append(msg_length)
            
            # ä¿æŒæœ€è¿‘100æ¡æ¶ˆæ¯çš„é•¿åº¦è®°å½•
            if len(profile.activity_pattern['message_lengths']) > 100:
                profile.activity_pattern['message_lengths'] = profile.activity_pattern['message_lengths'][-100:]
            
            # æ›´æ–°è¯é¢˜åå¥½
            topics = await self._extract_topics(message_text)
            for topic in topics:
                if topic not in profile.topic_preferences:
                    profile.topic_preferences[topic] = 0
                profile.topic_preferences[topic] += 1
            
            # æ›´æ–°æ²Ÿé€šé£æ ¼
            style_features = await self._extract_style_features(message_text)
            for feature, value in style_features.items():
                if feature not in profile.communication_style:
                    profile.communication_style[feature] = []
                profile.communication_style[feature].append(value)
                
                # ä¿æŒæœ€è¿‘50ä¸ªç‰¹å¾å€¼
                if len(profile.communication_style[feature]) > 50:
                    profile.communication_style[feature] = profile.communication_style[feature][-50:]
            
            # ä½¿ç”¨ä¸€è‡´çš„ç”¨æˆ·é”®æ ¼å¼
            user_key = f"{group_id}:{qq_id}"
            self.user_profiles[user_key] = profile # æ›´æ–°å†…å­˜ä¸­çš„ç”»åƒ
            
            # æ¸…ç†profileæ•°æ®ä»¥ç¡®ä¿JSONåºåˆ—åŒ–å®‰å…¨
            profile_dict = asdict(profile)
            profile_dict = self._clean_profile_for_serialization(profile_dict)
            await self.db_manager.save_user_profile(group_id, profile_dict) # æŒä¹…åŒ–åˆ°æ•°æ®åº“
            
        except Exception as e:
            logger.error(f"æ›´æ–°ç”¨æˆ·ç”»åƒå¤±è´¥ (ç¾¤:{group_id}, ç”¨æˆ·:{qq_id}): {e}", exc_info=True)
            raise

    async def _analyze_social_context(self, event: AstrMessageEvent, message_text: str) -> Dict[str, Any]:
        """åˆ†æç¤¾äº¤å…³ç³»ä¸Šä¸‹æ–‡"""
        try:
            sender_id = event.get_sender_id()
            group_id = event.get_group_id() or event.get_sender_id()  # ç§èŠæ—¶ä½¿ç”¨ sender_id ä½œä¸ºä¼šè¯ ID
            
            social_context = {
                'mentions': [],
                'replies': [],
                'interaction_strength': {},
                'group_role': 'member'
            }
            
            # æå–@æ¶ˆæ¯
            mentions = self._extract_mentions(message_text)
            social_context['mentions'] = mentions

            logger.debug(f"[ç¤¾äº¤å…³ç³»] ç¾¤ç»„ {group_id} ç”¨æˆ· {sender_id} æåŠäº† {len(mentions)} ä¸ªç”¨æˆ·: {mentions}")

            # æ›´æ–°ç¤¾äº¤å…³ç³»
            for mentioned_user in mentions:
                await self._update_social_relation(
                    sender_id, mentioned_user, 'mention', group_id
                )
                logger.debug(f"[ç¤¾äº¤å…³ç³»] ä¿å­˜æåŠå…³ç³»: {sender_id} -> {mentioned_user}")

            # åˆ†æå›å¤å…³ç³»ï¼ˆå¦‚æœæ¡†æ¶æ”¯æŒï¼‰
            if hasattr(event, 'get_reply_info') and event.get_reply_info():
                reply_info = event.get_reply_info()
                replied_user = reply_info.get('user_id')
                if replied_user:
                    social_context['replies'].append(replied_user)
                    await self._update_social_relation(
                        sender_id, replied_user, 'reply', group_id
                    )
                    logger.debug(f"[ç¤¾äº¤å…³ç³»] ä¿å­˜å›å¤å…³ç³»: {sender_id} -> {replied_user}")
            else:
                logger.debug(f"[ç¤¾äº¤å…³ç³»] æ¶ˆæ¯äº‹ä»¶ä¸æ”¯æŒget_reply_infoæˆ–æ²¡æœ‰å›å¤ä¿¡æ¯")

            # === æ–°å¢ï¼šåŸºäºæ—¶é—´çª—å£çš„å¯¹è¯å…³ç³»åˆ†æ(å»é™¤@é™åˆ¶) ===
            await self._analyze_conversation_interactions(sender_id, group_id, message_text)

            # è®¡ç®—ä¸ç¾¤å†…æˆå‘˜çš„äº¤äº’å¼ºåº¦
            if sender_id in self.social_graph:
                for relation in self.social_graph[sender_id]:
                    social_context['interaction_strength'][relation.to_user] = relation.strength
            
            # åˆ†æç¾¤å†…è§’è‰²ï¼ˆåŸºäºå‘è¨€é¢‘ç‡å’Œ@æ¬¡æ•°ï¼‰
            group_role = await self._analyze_group_role(sender_id, group_id)
            social_context['group_role'] = group_role
            
            return social_context
        
        except Exception as e:
            logger.warning(f"ç¤¾äº¤ä¸Šä¸‹æ–‡åˆ†æå¤±è´¥: {e}")
            return {}

    async def _analyze_topic_context(self, message_text: str) -> Dict[str, float]:
        """åˆ†æè¯é¢˜ä¸Šä¸‹æ–‡"""
        topic_scores = {}
        
        for topic, keywords in self.topic_keywords.items():
            score = 0
            for keyword in keywords:
                if keyword in message_text:
                    score += 1
            
            if score > 0:
                topic_scores[topic] = score / len(keywords)
        
        return topic_scores

    async def _analyze_emotional_context(self, message_text: str) -> Dict[str, float]:
        """ä½¿ç”¨LLMåˆ†ææƒ…æ„Ÿä¸Šä¸‹æ–‡"""
        # æ£€æŸ¥ç¼“å­˜ï¼Œé¿å…é‡å¤åˆ†æç›¸åŒå†…å®¹
        cache_key = f"emotion_cache_{hash(message_text)}"
        if hasattr(self, '_analysis_cache') and cache_key in self._analysis_cache:
            cached_result = self._analysis_cache[cache_key]
            if time.time() - cached_result.get('timestamp', 0) < 300:  # 5åˆ†é’Ÿç¼“å­˜
                logger.debug(f"ä½¿ç”¨ç¼“å­˜çš„æƒ…æ„Ÿåˆ†æç»“æœ")
                return cached_result.get('result', self._simple_emotional_analysis(message_text))
        
        # ä¼˜å…ˆä½¿ç”¨æ¡†æ¶é€‚é…å™¨
        if self.llm_adapter and self.llm_adapter.has_refine_provider() and self.llm_adapter.providers_configured >= 2:
            prompt = self.prompts.JSON_ONLY_SYSTEM_PROMPT + "\n\n" + self.prompts.MULTIDIMENSIONAL_ANALYZER_EMOTIONAL_CONTEXT_PROMPT.format(
                message_text=message_text
            )
            try:
                response = await self.llm_adapter.refine_chat_completion(
                    prompt=prompt,
                    temperature=0.2
                )
                
                if response:
                    # ä½¿ç”¨å®‰å…¨çš„JSONè§£ææ–¹æ³•
                    emotion_scores = safe_parse_llm_json(
                        response.strip(), 
                        fallback_result=self._simple_emotional_analysis(message_text)
                    )
                    
                    if emotion_scores and isinstance(emotion_scores, dict):
                        # ç¡®ä¿æ‰€æœ‰åˆ†æ•°éƒ½åœ¨0-1ä¹‹é—´
                        for key, value in emotion_scores.items():
                            emotion_scores[key] = max(0.0, min(float(value), 1.0))
                        
                        # ç¼“å­˜ç»“æœ
                        if not hasattr(self, '_analysis_cache'):
                            self._analysis_cache = {}
                        self._analysis_cache[cache_key] = {
                            'result': emotion_scores,
                            'timestamp': time.time()
                        }
                        
                        logger.debug(f"æƒ…æ„Ÿä¸Šä¸‹æ–‡åˆ†æç»“æœ: {emotion_scores}")
                        return emotion_scores
                    else:
                        logger.warning(f"LLMæƒ…æ„Ÿåˆ†æè¿”å›æ ¼å¼ä¸æ­£ç¡®ï¼Œä½¿ç”¨ç®€åŒ–ç®—æ³•")
                        return self._simple_emotional_analysis(message_text)
                else:
                    logger.warning(f"LLMæƒ…æ„Ÿåˆ†ææœªè¿”å›æœ‰æ•ˆç»“æœï¼Œä½¿ç”¨ç®€åŒ–ç®—æ³•")
                    return self._simple_emotional_analysis(message_text)
                    
            except Exception as e:
                logger.error(f"LLMæƒ…æ„Ÿåˆ†æå¤±è´¥: {e}")
                return self._simple_emotional_analysis(message_text)
        else:
            logger.warning("æç‚¼æ¨¡å‹æœªé…ç½®ï¼Œæ— æ³•è¿›è¡ŒLLMæƒ…æ„Ÿåˆ†æï¼Œä½¿ç”¨ç®€åŒ–ç®—æ³•")
            return self._simple_emotional_analysis(message_text)
        
    def _simple_emotional_analysis(self, message_text: str) -> Dict[str, float]:
        """ç®€åŒ–çš„æƒ…æ„Ÿåˆ†æï¼ˆå¤‡ç”¨ï¼‰"""
        emotions = {
            'ç§¯æ': ['å¼€å¿ƒ', 'é«˜å…´', 'å…´å¥‹', 'æ»¡æ„', 'å–œæ¬¢', 'çˆ±', 'å¥½æ£’', 'å¤ªå¥½äº†', 'å“ˆå“ˆ', 'ğŸ˜„', 'ğŸ˜Š', 'ğŸ‘'],
            'æ¶ˆæ': ['éš¾è¿‡', 'ç”Ÿæ°”', 'å¤±æœ›', 'æ— èŠ', 'çƒ¦', 'è®¨åŒ', 'ç³Ÿç³•', 'ä¸å¥½', 'ğŸ˜­', 'ğŸ˜¢', 'ğŸ˜¡'],
            'ä¸­æ€§': ['çŸ¥é“', 'æ˜ç™½', 'å¯ä»¥', 'å¥½çš„', 'å—¯', 'å“¦', 'è¿™æ ·', 'ç„¶å'],
            'ç–‘é—®': ['å—', 'å‘¢', 'ï¼Ÿ', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å“ªé‡Œ', 'ğŸ¤”'],
            'æƒŠè®¶': ['å“‡', 'å¤©å“ª', 'çœŸçš„', 'ä¸ä¼šå§', 'å¤ª', 'ç«Ÿç„¶', 'å±…ç„¶', 'ğŸ˜±', 'ğŸ˜¯']
        }
        
        emotion_scores = {}
        # å°†æ¶ˆæ¯æ–‡æœ¬æŒ‰ç©ºæ ¼æˆ–æ ‡ç‚¹ç¬¦å·åˆ†å‰²æˆå•è¯ï¼Œå¹¶è¿‡æ»¤æ‰ç©ºå­—ç¬¦ä¸²
        words = [word for word in re.split(r'\s+|[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š]', message_text) if word]
        total_words = len(words) # å·²ç»ä¿®æ”¹ä¸ºå•è¯æ€»æ•°
        
        for emotion, keywords in emotions.items():
            count = 0
            for keyword in keywords:
                # æ£€æŸ¥å…³é”®è¯æ˜¯å¦åœ¨å•è¯åˆ—è¡¨ä¸­
                count += words.count(keyword)
            
            emotion_scores[emotion] = count / max(total_words, 1)
        
        return emotion_scores

    async def _analyze_temporal_context(self, event: AstrMessageEvent) -> Dict[str, Any]:
        """åˆ†ææ—¶é—´ä¸Šä¸‹æ–‡"""
        now = datetime.now()
        
        time_context = {
            'hour': now.hour,
            'weekday': now.weekday(),
            'time_period': self._get_time_period(now.hour),
            'is_weekend': now.weekday() >= 5,
            'season': self._get_season(now.month)
        }
        
        return time_context

    async def _analyze_communication_style(self, message_text: str) -> Dict[str, float]:
        """åˆ†ææ²Ÿé€šé£æ ¼ï¼ˆä¼˜åŒ–ç‰ˆï¼Œå‡å°‘LLMè°ƒç”¨ï¼‰"""
        try:
            # æ£€æŸ¥ç¼“å­˜
            cache_key = f"style_cache_{hash(message_text)}"
            if hasattr(self, '_analysis_cache') and cache_key in self._analysis_cache:
                cached_result = self._analysis_cache[cache_key]
                if time.time() - cached_result.get('timestamp', 0) < 600:  # 10åˆ†é’Ÿç¼“å­˜
                    logger.debug(f"ä½¿ç”¨ç¼“å­˜çš„é£æ ¼åˆ†æç»“æœ")
                    return cached_result.get('result', {})
            
            # ä¼˜åŒ–ï¼šä¼˜å…ˆä½¿ç”¨ç®€åŒ–è®¡ç®—ï¼Œå‡å°‘LLMè°ƒç”¨
            style_features = {
                'formal_level': self._simple_formal_level(message_text),
                'enthusiasm_level': self._simple_enthusiasm_level(message_text),
                'question_tendency': self._simple_question_tendency(message_text),
                'emoji_usage': self._calculate_emoji_usage(message_text),
                'length_preference': len(message_text),
                'punctuation_style': self._calculate_punctuation_style(message_text)
            }
            
            # åªæœ‰åœ¨æ‰¹é‡åˆ†æè®¡æ•°è¾ƒä½æ—¶æ‰ä½¿ç”¨LLMå¢å¼ºåˆ†æ
            if (hasattr(self, '_batch_analysis_count') and 
                any(count <= 20 for count in self._batch_analysis_count.values())):
                # é€‰æ‹©æ€§åœ°ä½¿ç”¨LLMå¢å¼ºæŸäº›ç‰¹å¾
                try:
                    style_features['formal_level'] = await self._calculate_formal_level(message_text)
                except Exception as e:
                    logger.debug(f"LLMæ­£å¼ç¨‹åº¦åˆ†æå¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬: {e}")
            
            # ç¼“å­˜ç»“æœ
            if not hasattr(self, '_analysis_cache'):
                self._analysis_cache = {}
            self._analysis_cache[cache_key] = {
                'result': style_features,
                'timestamp': time.time()
            }
            
            return style_features
        except Exception as e:
            logger.error(f'åˆ†ææ²Ÿé€šé£æ ¼å¤±è´¥ï¼š{e}')
            # è¿”å›æœ€åŸºæœ¬çš„é£æ ¼ç‰¹å¾
            return {
                'formal_level': 0.5,
                'enthusiasm_level': self._simple_enthusiasm_level(message_text),
                'question_tendency': self._simple_question_tendency(message_text),
                'emoji_usage': self._calculate_emoji_usage(message_text),
                'length_preference': len(message_text),
                'punctuation_style': self._calculate_punctuation_style(message_text)
            }

    async def _extract_topics(self, message_text: str) -> List[str]:
        """æå–æ¶ˆæ¯è¯é¢˜"""
        detected_topics = []
        
        for topic, keywords in self.topic_keywords.items():
            for keyword in keywords:
                if keyword in message_text:
                    detected_topics.append(topic)
                    break
        
        return detected_topics

    async def _extract_style_features(self, message_text: str) -> Dict[str, float]:
        """æå–é£æ ¼ç‰¹å¾"""
        return {
            'length': len(message_text),
            'punctuation_ratio': len([c for c in message_text if c in 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š']) / max(len(message_text), 1),
            'emoji_count': emoji.emoji_count(message_text),
            'question_count': message_text.count('ï¼Ÿ') + message_text.count('?'),
            'exclamation_count': message_text.count('ï¼') + message_text.count('!')
        }

    def _extract_mentions(self, message_text: str) -> List[str]:
        """æå–@æ¶ˆæ¯"""
        # åŒ¹é…@ç”¨æˆ·æ¨¡å¼
        at_pattern = r'@(\w+|\d+)'
        matches = re.findall(at_pattern, message_text)
        
        # å°è¯•è§£ææ˜µç§°åˆ°QQå·çš„æ˜ å°„
        mentioned_users = []
        for match in matches:
            if match.isdigit():
                # ç›´æ¥@çš„QQå·
                mentioned_users.append(match)
            else:
                # @çš„æ˜µç§°ï¼Œå°è¯•æ‰¾åˆ°å¯¹åº”çš„QQå·
                if match in self.nickname_mapping:
                    mentioned_users.append(self.nickname_mapping[match])
                else:
                    # è®°å½•æœªçŸ¥æ˜µç§°
                    mentioned_users.append(f"nickname:{match}")
        
        return mentioned_users

    async def _update_social_relation(self, from_user: str, to_user: str, relation_type: str, group_id: str):
        """æ›´æ–°ç¤¾äº¤å…³ç³»"""
        logger.debug(f"[ç¤¾äº¤å…³ç³»æ›´æ–°] å¼€å§‹æ›´æ–°: {from_user} -> {to_user}, ç±»å‹: {relation_type}, ç¾¤ç»„: {group_id}")

        # æŸ¥æ‰¾ç°æœ‰å…³ç³»
        existing_relation = None
        for relation in self.social_graph[from_user]:
            if relation.to_user == to_user and relation.relation_type == relation_type:
                existing_relation = relation
                break

        if existing_relation:
            # æ›´æ–°ç°æœ‰å…³ç³»
            old_frequency = existing_relation.frequency
            old_strength = existing_relation.strength
            existing_relation.frequency += 1
            existing_relation.last_interaction = datetime.now().isoformat()
            existing_relation.strength = min(existing_relation.strength + 0.1, 1.0)
            logger.info(f"[ç¤¾äº¤å…³ç³»æ›´æ–°] æ›´æ–°å·²å­˜åœ¨çš„å…³ç³»: {from_user} -> {to_user} ({relation_type}), "
                       f"é¢‘ç‡: {old_frequency} -> {existing_relation.frequency}, "
                       f"å¼ºåº¦: {old_strength:.2f} -> {existing_relation.strength:.2f}")
        else:
            # åˆ›å»ºæ–°å…³ç³»
            new_relation = SocialRelation(
                from_user=from_user,
                to_user=to_user,
                relation_type=relation_type,
                strength=0.1,
                frequency=1,
                last_interaction=datetime.now().isoformat()
            )
            self.social_graph[from_user].append(new_relation)
            logger.info(f"[ç¤¾äº¤å…³ç³»æ›´æ–°] åˆ›å»ºæ–°å…³ç³»: {from_user} -> {to_user} ({relation_type}), "
                       f"åˆå§‹å¼ºåº¦: 0.1, é¢‘ç‡: 1")

        # æŒä¹…åŒ–ç¤¾äº¤å…³ç³»
        relation_data = asdict(existing_relation if existing_relation else new_relation)
        relation_data = self._debug_dict_keys(relation_data, 'social_relation')

        try:
            await self.db_manager.save_social_relation(group_id, relation_data)
            logger.debug(f"[ç¤¾äº¤å…³ç³»æ›´æ–°] æˆåŠŸä¿å­˜åˆ°æ•°æ®åº“: {from_user} -> {to_user}")
        except Exception as e:
            logger.error(f"[ç¤¾äº¤å…³ç³»æ›´æ–°] ä¿å­˜åˆ°æ•°æ®åº“å¤±è´¥: {e}", exc_info=True)

    async def _analyze_conversation_interactions(self, sender_id: str, group_id: str, message_text: str):
        """
        åŸºäºæ—¶é—´çª—å£åˆ†æå¯¹è¯äº’åŠ¨å…³ç³»(ä¸éœ€è¦@)

        åˆ†æé€»è¾‘:
        1. è·å–æœ€è¿‘ä¸€å®šæ—¶é—´å†…çš„æ¶ˆæ¯
        2. åˆ¤æ–­ç”¨æˆ·ä¹‹é—´çš„å¯¹è¯è¿ç»­æ€§
        3. å»ºç«‹conversationç±»å‹çš„ç¤¾äº¤å…³ç³»
        """
        try:
            # è·å–æœ€è¿‘5åˆ†é’Ÿå†…çš„æ¶ˆæ¯
            recent_messages = await self.db_manager.get_messages_by_group_and_timerange(
                group_id=group_id,
                start_time=time.time() - 300,  # 5åˆ†é’Ÿ
                limit=20
            )

            if len(recent_messages) < 2:
                return

            # æ‰¾åˆ°å½“å‰ç”¨æˆ·ä¹‹å‰çš„æœ€è¿‘ä¸€æ¡å…¶ä»–äººçš„æ¶ˆæ¯
            previous_sender = None
            for msg in reversed(recent_messages):  # æŒ‰æ—¶é—´å€’åº
                if msg['sender_id'] != sender_id and msg['sender_id'] != 'bot':
                    previous_sender = msg['sender_id']
                    previous_message = msg['message']
                    time_diff = time.time() - msg['timestamp']
                    break

            if not previous_sender:
                return

            # å¦‚æœæ—¶é—´é—´éš”å°äº60ç§’,è®¤ä¸ºæ˜¯åœ¨å¯¹è¯
            if time_diff <= 60:
                # ä½¿ç”¨LLMåˆ¤æ–­æ˜¯å¦åœ¨å›åº”
                is_responding, reason = await self._is_likely_responding_llm(previous_message, message_text)

                if is_responding:
                    await self._update_social_relation(
                        sender_id, previous_sender, 'conversation', group_id
                    )
                    logger.info(f"[ç¤¾äº¤å…³ç³»-å¯¹è¯] æ£€æµ‹åˆ°å¯¹è¯å…³ç³»: {sender_id} -> {previous_sender}, "
                               f"æ—¶é—´é—´éš”: {time_diff:.1f}ç§’, åˆ¤æ–­ç†ç”±: {reason}")

        except Exception as e:
            logger.debug(f"[ç¤¾äº¤å…³ç³»-å¯¹è¯] åˆ†æå¯¹è¯äº’åŠ¨å¤±è´¥: {e}")

    async def _is_likely_responding_llm(self, previous_message: str, current_message: str) -> tuple[bool, str]:
        """
        ä½¿ç”¨LLMåˆ¤æ–­æ˜¯å¦åœ¨å›åº”ä¸Šä¸€æ¡æ¶ˆæ¯

        Returns:
            (æ˜¯å¦åœ¨å›åº”, åˆ¤æ–­ç†ç”±)
        """
        if not self.llm_adapter:
            logger.debug("[ç¤¾äº¤å…³ç³»-LLM] LLMé€‚é…å™¨æœªåˆå§‹åŒ–,ä½¿ç”¨ç®€å•è§„åˆ™åˆ¤æ–­")
            is_responding = self._is_likely_responding_simple(previous_message, current_message)
            return is_responding, "è§„åˆ™åˆ¤æ–­"

        try:
            prompt = f"""åˆ†æä»¥ä¸‹ä¸¤æ¡æ¶ˆæ¯çš„å…³ç³»,åˆ¤æ–­"å½“å‰æ¶ˆæ¯"æ˜¯å¦åœ¨å›åº”"ä¸Šä¸€æ¡æ¶ˆæ¯"ã€‚

ä¸Šä¸€æ¡æ¶ˆæ¯: {previous_message}
å½“å‰æ¶ˆæ¯: {current_message}

è¯·æ ¹æ®ä»¥ä¸‹æ ‡å‡†åˆ¤æ–­:
1. è¯­ä¹‰ç›¸å…³æ€§: ä¸¤æ¡æ¶ˆæ¯æ˜¯å¦åœ¨è®¨è®ºåŒä¸€ä¸ªè¯é¢˜
2. å›åº”æ€§: å½“å‰æ¶ˆæ¯æ˜¯å¦åŒ…å«å›åº”ã€å›ç­”ã€è¯„è®ºä¸Šä¸€æ¡æ¶ˆæ¯çš„å†…å®¹
3. å¯¹è¯è¿è´¯æ€§: ä¸¤æ¡æ¶ˆæ¯æ˜¯å¦æ„æˆè¿è´¯çš„å¯¹è¯

è¯·ä»¥JSONæ ¼å¼è¿”å›:
{{
  "is_responding": true/false,
  "reason": "åˆ¤æ–­ç†ç”±",
  "confidence": 0.0-1.0 (ç½®ä¿¡åº¦)
}}"""

            response = await self.llm_adapter.call_llm(
                prompt=prompt,
                system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¯¹è¯åˆ†æä¸“å®¶,æ“…é•¿åˆ¤æ–­æ¶ˆæ¯ä¹‹é—´çš„å…³ç³»ã€‚",
                temperature=0.3
            )

            # è§£æJSONå“åº”
            result = safe_parse_llm_json(response)

            if result and 'is_responding' in result:
                is_responding = result.get('is_responding', False)
                reason = result.get('reason', 'æœªçŸ¥')
                confidence = result.get('confidence', 0.5)

                logger.debug(f"[ç¤¾äº¤å…³ç³»-LLM] LLMåˆ¤æ–­ç»“æœ: {is_responding}, ç½®ä¿¡åº¦: {confidence}, ç†ç”±: {reason}")

                # åªæœ‰å½“ç½®ä¿¡åº¦è¶³å¤Ÿé«˜æ—¶æ‰è¿”å›True
                return is_responding and confidence >= 0.6, reason

            else:
                logger.warning(f"[ç¤¾äº¤å…³ç³»-LLM] LLMè¿”å›æ ¼å¼é”™è¯¯,ä½¿ç”¨ç®€å•è§„åˆ™: {response[:100]}")
                is_responding = self._is_likely_responding_simple(previous_message, current_message)
                return is_responding, "LLMè§£æå¤±è´¥,ä½¿ç”¨è§„åˆ™åˆ¤æ–­"

        except Exception as e:
            logger.warning(f"[ç¤¾äº¤å…³ç³»-LLM] LLMåˆ¤æ–­å¤±è´¥: {e}, ä½¿ç”¨ç®€å•è§„åˆ™")
            is_responding = self._is_likely_responding_simple(previous_message, current_message)
            return is_responding, f"LLMå¼‚å¸¸: {str(e)}"

    def _is_likely_responding_simple(self, previous_message: str, current_message: str) -> bool:
        """
        ç®€å•åˆ¤æ–­æ˜¯å¦åœ¨å›åº”ä¸Šä¸€æ¡æ¶ˆæ¯

        åˆ¤æ–­è§„åˆ™:
        1. åŒ…å«å›åº”æ€§è¯æ±‡
        2. è¯é¢˜ç›¸å…³æ€§(å…³é”®è¯é‡åˆ)
        3. ä¸æ˜¯çº¯è¡¨æƒ…/ç¬¦å·
        """
        # å›åº”æ€§è¯æ±‡
        response_keywords = [
            'æ˜¯çš„', 'ä¸æ˜¯', 'å¯¹', 'æ²¡é”™', 'ç¡®å®', 'åŒæ„', 'èµåŒ',
            'å¥½çš„', 'è¡Œ', 'å¯ä»¥', 'ä¸è¡Œ', 'ä¸å¯ä»¥',
            'å“ˆå“ˆ', 'ç¬‘æ­»', 'ï¼Ÿ', '?', 'ï¼', '!',
            'å—¯', 'å“¦', 'é¢', 'å‘ƒ', 'å•Š',
            'ä¸ºä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä»€ä¹ˆ', 'å“ªé‡Œ', 'å“ªä¸ª'
        ]

        # æ£€æŸ¥æ˜¯å¦åŒ…å«å›åº”æ€§è¯æ±‡
        for keyword in response_keywords:
            if keyword in current_message:
                return True

        # æ£€æŸ¥å…³é”®è¯é‡åˆ(ç®€å•çš„è¯é¢˜ç›¸å…³æ€§)
        import re
        prev_words = set(re.findall(r'[\u4e00-\u9fa5]+|[a-zA-Z]+', previous_message))
        curr_words = set(re.findall(r'[\u4e00-\u9fa5]+|[a-zA-Z]+', current_message))

        # ç§»é™¤å¸¸è§åœç”¨è¯
        stopwords = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸ª'}
        prev_words -= stopwords
        curr_words -= stopwords

        # å¦‚æœæœ‰å…±åŒå…³é”®è¯,è®¤ä¸ºå¯èƒ½åœ¨è®¨è®ºåŒä¸€è¯é¢˜
        if prev_words and curr_words:
            overlap = len(prev_words & curr_words)
            if overlap > 0:
                return True

        return False

    async def _analyze_group_role(self, user_id: str, group_id: str) -> str:
        """åˆ†æç”¨æˆ·åœ¨ç¾¤å†…çš„è§’è‰²"""
        # è¿™é‡Œå¯ä»¥åŸºäºå‘è¨€é¢‘ç‡ã€è¢«@æ¬¡æ•°ç­‰åˆ¤æ–­ç”¨æˆ·è§’è‰²
        # ç®€åŒ–å®ç°
        user_key = f"{group_id}:{user_id}"
        if user_key in self.user_profiles:
            profile = self.user_profiles[user_key]
            mention_count = sum(1 for relations in self.social_graph.values() 
                              for relation in relations 
                              if relation.to_user == user_id and relation.relation_type == 'mention')
            
            if mention_count > 10:
                return 'active_member'
            elif mention_count > 5:
                return 'regular_member'
            else:
                return 'member'
        
        return 'member'

    async def _calculate_contextual_relevance(self, sender_id: str, message_text: str, event: AstrMessageEvent) -> float:
        """è®¡ç®—ä¸Šä¸‹æ–‡ç›¸å…³æ€§å¾—åˆ†"""

        try:

            relevance_score = 0.0
            group_id = event.get_group_id() if event else ''
            
            # åŸºäºç”¨æˆ·å†å²è¡Œä¸ºçš„ç›¸å…³æ€§
            user_key = f"{group_id}:{sender_id}"
            if user_key in self.user_profiles:
                profile = self.user_profiles[user_key]
                
                # å…¼å®¹å¤„ç†UserProfileå¯¹è±¡å’Œdict
                if hasattr(profile, 'topic_preferences'):
                    topic_preferences = profile.topic_preferences
                else:
                    topic_preferences = profile.get('topic_preferences', {})
                
                # è¯é¢˜ä¸€è‡´æ€§
                current_topics = await self._extract_topics(message_text)
                for topic in current_topics:
                    if topic in topic_preferences:
                        relevance_score += 0.2
                
                # é£æ ¼ä¸€è‡´æ€§
                current_style = await self._extract_style_features(message_text)
                communication_style = profile.communication_style if hasattr(profile, 'communication_style') else profile.get('communication_style', {})
                
                if 'length' in communication_style:
                    avg_length = sum(communication_style['length'][-10:]) / min(10, len(communication_style['length']))
                    length_similarity = 1.0 - abs(current_style['length'] - avg_length) / max(avg_length, 1)
                    relevance_score += length_similarity * 0.1
            
                # æ—¶é—´ä¸Šä¸‹æ–‡ç›¸å…³æ€§
                current_hour = datetime.now().hour
                if user_key in self.user_profiles:
                    profile = self.user_profiles[user_key]
                    activity_pattern = profile.activity_pattern if hasattr(profile, 'activity_pattern') else profile.get('activity_pattern', {})
                    if 'activity_hours' in activity_pattern:
                        hour_frequency = activity_pattern['activity_hours'].get(current_hour, 0)
                        total_messages = sum(activity_pattern['activity_hours'].values())
                        if total_messages > 0:
                            time_relevance = hour_frequency / total_messages
                            relevance_score += time_relevance * 0.2
            
            return min(relevance_score, 1.0)
        except Exception as e:
            logger.warning(f"è®¡ç®—ä¸Šä¸‹æ–‡ç›¸å…³æ€§å¾—åˆ†-è®¡ç®—å¤±è´¥: {e}")
            return None

    def _get_time_period(self, hour: int) -> str:
        """è·å–æ—¶é—´æ®µ"""
        if 6 <= hour < 12:
            return 'ä¸Šåˆ'
        elif 12 <= hour < 18:
            return 'ä¸‹åˆ'
        elif 18 <= hour < 22:
            return 'æ™šä¸Š'
        else:
            return 'æ·±å¤œ'

    def _get_season(self, month: int) -> str:
        """è·å–å­£èŠ‚"""
        if month in [1, 2, 12]:
            return 'å†¬å­£'
        elif month in [3, 4, 5]:
            return 'æ˜¥å­£'
        elif month in [6, 7, 8]:
            return 'å¤å­£'
        else:
            return 'ç§‹å­£'

    async def _call_llm_for_style_analysis(self, text: str, prompt_template: str, fallback_function: callable, analysis_name: str) -> float:
        """
        é€šç”¨çš„LLMé£æ ¼åˆ†æè¾…åŠ©å‡½æ•°ã€‚
        Args:
            text: å¾…åˆ†æçš„æ–‡æœ¬ã€‚
            prompt_template: LLMæç¤ºæ¨¡æ¿ã€‚
            fallback_function: LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–æˆ–è°ƒç”¨å¤±è´¥æ—¶ä½¿ç”¨çš„å¤‡ç”¨å‡½æ•°ã€‚
            analysis_name: åˆ†æåç§°ï¼Œç”¨äºæ—¥å¿—è®°å½•ã€‚
        Returns:
            0-1ä¹‹é—´çš„è¯„åˆ†ã€‚
        """
        # æ£€æŸ¥é€‚é…å™¨å’Œrefine provideræ˜¯å¦å¯ç”¨
        if not self.llm_adapter or not self.llm_adapter.has_refine_provider() or self.llm_adapter.providers_configured < 2:
            logger.warning(f"æç‚¼æ¨¡å‹LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä½¿ç”¨LLMè®¡ç®—{analysis_name}ï¼Œä½¿ç”¨ç®€åŒ–ç®—æ³•ã€‚")
            return fallback_function(text)

        try:
            prompt = prompt_template.format(text=text)
            response = await self.llm_adapter.refine_chat_completion(
                prompt=prompt,
                temperature=0.1
            )
            
            if response:
                # response æ˜¯å­—ç¬¦ä¸²
                numbers = re.findall(r'0\.\d+|1\.0|0', response.strip())
                if numbers:
                    return min(float(numbers[0]), 1.0)
            
            return 0.5 # é»˜è®¤å€¼
            
        except Exception as e:
            logger.warning(f"LLM{analysis_name}è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–ç®—æ³•: {e}")
            return fallback_function(text)

    async def _calculate_formal_level(self, text: str) -> float:
        """ä½¿ç”¨LLMè®¡ç®—æ­£å¼ç¨‹åº¦"""
        prompt_template = self.prompts.MULTIDIMENSIONAL_ANALYZER_FORMAL_LEVEL_PROMPT
        return await self._call_llm_for_style_analysis(text, prompt_template, self._simple_formal_level, "æ­£å¼ç¨‹åº¦")

    def _simple_formal_level(self, text: str) -> float:
        """ç®€åŒ–çš„æ­£å¼ç¨‹åº¦è®¡ç®—ï¼ˆå¤‡ç”¨ï¼‰"""
        formal_indicators = ['æ‚¨', 'è¯·', 'è°¢è°¢æ‚¨', 'ä¸å¥½æ„æ€', 'æ‰“æ‰°äº†', 'æ•æˆ‘ç›´è¨€', 'è¯·é—®']
        informal_indicators = ['å“ˆå“ˆ', 'å˜¿', 'å•Š', 'å‘€', 'å“¦', 'å—¯å—¯', 'å“‡']
        
        formal_count = sum(text.count(word) for word in formal_indicators)
        informal_count = sum(text.count(word) for word in informal_indicators)
        
        total = formal_count + informal_count
        return formal_count / max(total, 1) if total > 0 else 0.5

    async def _calculate_enthusiasm_level(self, text: str) -> float:
        """ä½¿ç”¨LLMè®¡ç®—çƒ­æƒ…ç¨‹åº¦"""
        prompt_template = self.prompts.MULTIDIMENSIONAL_ANALYZER_ENTHUSIASM_LEVEL_PROMPT
        return await self._call_llm_for_style_analysis(text, prompt_template, self._simple_enthusiasm_level, "çƒ­æƒ…ç¨‹åº¦")

    def _simple_enthusiasm_level(self, text: str) -> float:
        """ç®€åŒ–çš„çƒ­æƒ…ç¨‹åº¦è®¡ç®—ï¼ˆå¤‡ç”¨ï¼‰"""
        enthusiasm_indicators = ['ï¼', '!', 'å“ˆå“ˆ', 'å¤ªå¥½äº†', 'æ£’', 'èµ', 'ğŸ˜„', 'ğŸ˜Š', 'ğŸ‰', 'å‰å®³', 'awesome']
        count = sum(text.count(indicator) for indicator in enthusiasm_indicators)
        return min(count / max(len(text), 1) * 20, 1.0)

    async def _calculate_question_tendency(self, text: str) -> float:
        """ä½¿ç”¨LLMè®¡ç®—æé—®å€¾å‘"""
        prompt_template = self.prompts.MULTIDIMENSIONAL_ANALYZER_QUESTION_TENDENCY_PROMPT
        return await self._call_llm_for_style_analysis(text, prompt_template, self._simple_question_tendency, "æé—®å€¾å‘")

    def _simple_question_tendency(self, text: str) -> float:
        """ç®€åŒ–çš„æé—®å€¾å‘è®¡ç®—ï¼ˆå¤‡ç”¨ï¼‰"""
        question_indicators = ['ï¼Ÿ', '?', 'å—', 'å‘¢', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å“ªé‡Œ', 'å¦‚ä½•']
        count = sum(text.count(indicator) for indicator in question_indicators)
        return min(count / max(len(text), 1) * 10, 1.0)

    def _calculate_emoji_usage(self, text: str) -> float:
        """è®¡ç®—è¡¨æƒ…ç¬¦å·ä½¿ç”¨ç¨‹åº¦"""
        emoji_count = emoji.emoji_count(text)
        return min(emoji_count / max(len(text), 1) * 10, 1.0)

    def _calculate_punctuation_style(self, text: str) -> float:
        """è®¡ç®—æ ‡ç‚¹ç¬¦å·é£æ ¼"""
        punctuation_count = len([c for c in text if c in 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''()ï¼ˆï¼‰'])
        return punctuation_count / max(len(text), 1)

    async def get_user_insights(self, group_id: str, qq_id: str) -> Dict[str, Any]:
        """ä½¿ç”¨LLMç”Ÿæˆæ·±åº¦ç”¨æˆ·æ´å¯Ÿ"""
        user_key = f"{group_id}:{qq_id}"
        if user_key not in self.user_profiles:
            return {"error": "ç”¨æˆ·ä¸å­˜åœ¨"}
        
        profile = self.user_profiles[user_key]
        
        # è®¡ç®—æ´»è·ƒæ—¶æ®µ
        active_hours = []
        activity_pattern = profile.activity_pattern if hasattr(profile, 'activity_pattern') else profile.get('activity_pattern', {})
        if 'activity_hours' in activity_pattern:
            sorted_hours = sorted(activity_pattern['activity_hours'].items(), 
                                key=lambda x: x[1], reverse=True) # ä¿®æ­£æ’åºé”®
            active_hours = [hour for hour, count in sorted_hours[:3]]
        
        # è®¡ç®—ä¸»è¦è¯é¢˜
        topic_preferences = profile.topic_preferences if hasattr(profile, 'topic_preferences') else profile.get('topic_preferences', {})
        main_topics = sorted(topic_preferences.items(), 
                           key=lambda x: x[1], reverse=True)[:3] # ä¿®æ­£æ’åºé”®
        
        # è®¡ç®—ç¤¾äº¤æ´»è·ƒåº¦
        social_activity = len(self.social_graph.get(qq_id, []))
        
        # ä½¿ç”¨LLMç”Ÿæˆæ·±åº¦æ´å¯Ÿ
        deep_insights = await self._generate_deep_insights(profile)
        
        return {
            'user_id': qq_id,
            'user_name': profile.qq_name if hasattr(profile, 'qq_name') else profile.get('name', f'ç”¨æˆ·{qq_id}'),
            'nicknames': profile.nicknames if hasattr(profile, 'nicknames') else profile.get('nicknames', []),
            'active_hours': active_hours,
            'main_topics': [topic for topic, count in main_topics],
            'social_activity': social_activity,
            'communication_style_summary': self._summarize_communication_style(profile),
            'activity_summary': self._summarize_activity_pattern(profile),
            'deep_insights': deep_insights,
            'personality_analysis': await self._analyze_personality_traits(profile),
            'social_behavior': await self._analyze_social_behavior(qq_id)
        }

    async def _generate_deep_insights(self, profile) -> Dict[str, Any]:
        """ä½¿ç”¨LLMç”Ÿæˆæ·±åº¦ç”¨æˆ·æ´å¯Ÿ"""
        # æ£€æŸ¥é€‚é…å™¨å’Œrefine provideræ˜¯å¦å¯ç”¨
        if not self.llm_adapter or not self.llm_adapter.has_refine_provider():
            logger.warning("æç‚¼æ¨¡å‹LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä½¿ç”¨LLMç”Ÿæˆæ·±åº¦ç”¨æˆ·æ´å¯Ÿã€‚")
            return {"error": "LLMæœåŠ¡ä¸å¯ç”¨"}

        try:
            # å…¼å®¹å¤„ç†UserProfileå¯¹è±¡å’Œdict
            def get_attr(obj, attr, default=None):
                if hasattr(obj, attr):
                    return getattr(obj, attr)
                elif isinstance(obj, dict):
                    return obj.get(attr, default)
                return default
                
            # å‡†å¤‡ç”¨æˆ·æ•°æ®æ‘˜è¦
            qq_name = get_attr(profile, 'qq_name', get_attr(profile, 'name', 'æœªçŸ¥ç”¨æˆ·'))
            nicknames = get_attr(profile, 'nicknames', [])
            topic_preferences = get_attr(profile, 'topic_preferences', {})
            activity_pattern = get_attr(profile, 'activity_pattern', {})
            social_connections = get_attr(profile, 'social_connections', [])
            
            user_data_summary = {
                'qq_name': qq_name,
                'nicknames': nicknames,
                'topic_preferences': dict(list(topic_preferences.items())[:5]) if topic_preferences else {},
                'activity_pattern': {
                    'peak_hours': [k for k, v in sorted(
                        activity_pattern.get('activity_hours', {}).items(),
                        key=lambda item: item[1], reverse=True
                    )[:3]],
                    'avg_message_length': sum(activity_pattern.get('message_lengths', [])) / 
                                        max(len(activity_pattern.get('message_lengths', [])), 1)
                },
                'social_connections': len(social_connections)
            }
            
            prompt = self.prompts.JSON_ONLY_SYSTEM_PROMPT + "\n\n" + self.prompts.MULTIDIMENSIONAL_ANALYZER_DEEP_INSIGHTS_PROMPT.format(
                user_data_summary=json.dumps(user_data_summary, ensure_ascii=False, indent=2)
            )
            
            if self.llm_adapter and self.llm_adapter.has_refine_provider():
                response = await self.llm_adapter.refine_chat_completion(
                    prompt=prompt,
                    temperature=0.1
                )
            else:
                response = None
            
            if response:
                # response æ˜¯å­—ç¬¦ä¸²
                try:
                    insights = safe_parse_llm_json(response.strip())
                    return insights
                except json.JSONDecodeError:
                    logger.warning(f"LLMå“åº”JSONè§£æå¤±è´¥ï¼Œè¿”å›ç®€åŒ–åˆ†æã€‚å“åº”å†…å®¹: {response.strip()}")
                    return {
                        "personality_type": "åˆ†æä¸­",
                        "communication_preference": "å¾…æ·±å…¥åˆ†æ",
                        "social_role": "ç¾¤ä½“æˆå‘˜",
                        "learning_potential": 0.7
                    }
            return {"error": "LLMæœªè¿”å›æœ‰æ•ˆå“åº”"}
                
        except Exception as e:
            logger.warning(f"æ·±åº¦æ´å¯Ÿç”Ÿæˆå¤±è´¥: {e}")
            return {"error": "æ´å¯Ÿç”Ÿæˆå¤±è´¥"}

    async def _analyze_personality_traits(self, profile) -> Dict[str, float]:
        """åˆ†æç”¨æˆ·äººæ ¼ç‰¹è´¨"""
        # æ£€æŸ¥é€‚é…å™¨å’Œrefine provideræ˜¯å¦å¯ç”¨
        if not self.llm_adapter or not self.llm_adapter.has_refine_provider():
            logger.warning("æç‚¼æ¨¡å‹LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä½¿ç”¨LLMåˆ†æäººæ ¼ç‰¹è´¨ï¼Œä½¿ç”¨ç®€åŒ–ç®—æ³•ã€‚")
            return self._simple_personality_analysis(profile)

        try:
            # å…¼å®¹å¤„ç†UserProfileå¯¹è±¡å’Œdict
            communication_style = profile.communication_style if hasattr(profile, 'communication_style') else profile.get('communication_style', {})
            
            # è·å–æœ€è¿‘çš„æ²Ÿé€šé£æ ¼æ•°æ®
            recent_styles = {}
            for feature, values in communication_style.items():
                if values:
                    recent_styles[feature] = sum(values[-10:]) / min(len(values), 10)
            
            prompt = self.prompts.JSON_ONLY_SYSTEM_PROMPT + "\n\n" + self.prompts.MULTIDIMENSIONAL_ANALYZER_PERSONALITY_TRAITS_PROMPT.format(
                communication_style_data=json.dumps(recent_styles, ensure_ascii=False, indent=2)
            )
            
            if self.llm_adapter and self.llm_adapter.has_refine_provider():
                response = await self.llm_adapter.refine_chat_completion(
                    prompt=prompt,
                    temperature=0.1
                )
            else:
                response = None
            
            if response:
                # response æ˜¯å­—ç¬¦ä¸²
                try:
                    traits = safe_parse_llm_json(response.strip())
                    return traits
                except json.JSONDecodeError:
                    logger.warning(f"LLMå“åº”JSONè§£æå¤±è´¥ï¼Œè¿”å›ç®€åŒ–äººæ ¼åˆ†æã€‚å“åº”å†…å®¹: {response.strip()}")
                    return self._simple_personality_analysis(profile)
            return self._simple_personality_analysis(profile)
                
        except Exception as e:
            logger.warning(f"äººæ ¼ç‰¹è´¨åˆ†æå¤±è´¥: {e}")
            return self._simple_personality_analysis(profile)

    def _simple_personality_analysis(self, profile) -> Dict[str, float]:
        """ç®€åŒ–çš„äººæ ¼åˆ†æï¼ˆå¤‡ç”¨ï¼‰"""
        # å…¼å®¹å¤„ç†UserProfileå¯¹è±¡å’Œdict
        communication_style = profile.communication_style if hasattr(profile, 'communication_style') else profile.get('communication_style', {})
        topic_preferences = profile.topic_preferences if hasattr(profile, 'topic_preferences') else profile.get('topic_preferences', {})
        
        # åŸºäºåŸºç¡€æ•°æ®çš„ç®€å•åˆ†æ
        style_data = communication_style
        
        # å¤–å‘æ€§ï¼šåŸºäºæ¶ˆæ¯é¢‘ç‡å’Œé•¿åº¦
        extraversion = 0.5
        if 'length' in style_data and style_data['length']:
            avg_length = sum(style_data['length'][-20:]) / min(len(style_data['length']), 20)
            extraversion = min(avg_length / 100, 1.0)
        
        # å¼€æ”¾æ€§ï¼šåŸºäºè¯é¢˜å¤šæ ·æ€§
        openness = len(topic_preferences) / 10 if topic_preferences else 0.5
        
        return {
            "openness": min(openness, 1.0),
            "conscientiousness": 0.6,  # é»˜è®¤å€¼
            "extraversion": extraversion,
            "agreeableness": 0.7,  # é»˜è®¤å€¼
            "neuroticism": 0.3   # é»˜è®¤å€¼
        }

    async def _analyze_social_behavior(self, qq_id: str) -> Dict[str, Any]:
        """åˆ†æç¤¾äº¤è¡Œä¸ºæ¨¡å¼"""
        if qq_id not in self.social_graph:
            return {"interaction_count": 0, "relationship_strength": {}}
        
        relations = self.social_graph[qq_id]
        
        # ç»Ÿè®¡ä¸åŒç±»å‹çš„ç¤¾äº¤è¡Œä¸º
        behavior_stats = {
            "mention_frequency": len([r for r in relations if r.relation_type == 'mention']),
            "reply_frequency": len([r for r in relations if r.relation_type == 'reply']),
            "total_interactions": len(relations),
            "avg_relationship_strength": sum(r.strength for r in relations) / max(len(relations), 1),
            "top_connections": [
                {"user": r.to_user, "strength": r.strength, "frequency": r.frequency}
                for r in sorted(relations, key=lambda x: x.strength, reverse=True)[:5]
            ]
        }
        
        return behavior_stats

    def _summarize_communication_style(self, profile) -> Dict[str, str]:
        """æ€»ç»“æ²Ÿé€šé£æ ¼"""
        style_summary = {}
        
        # å…¼å®¹å¤„ç†UserProfileå¯¹è±¡å’Œdict
        communication_style = profile.communication_style if hasattr(profile, 'communication_style') else profile.get('communication_style', {})
        
        if 'length' in communication_style and communication_style['length']:
            avg_length = sum(communication_style['length']) / len(communication_style['length'])
            if avg_length > 50:
                style_summary['length_style'] = 'è¯¦ç»†å‹'
            elif avg_length > 20:
                style_summary['length_style'] = 'é€‚ä¸­å‹'
            else:
                style_summary['length_style'] = 'ç®€æ´å‹'
        
        return style_summary

    def _summarize_activity_pattern(self, profile) -> Dict[str, Any]:
        """æ€»ç»“æ´»åŠ¨æ¨¡å¼"""
        activity_summary = {}
        
        # å…¼å®¹å¤„ç†UserProfileå¯¹è±¡å’Œdict
        activity_pattern = profile.activity_pattern if hasattr(profile, 'activity_pattern') else profile.get('activity_pattern', {})
        
        if 'activity_hours' in activity_pattern:
            hours = activity_pattern['activity_hours']
            if hours:
                peak_hour = max(hours.items(), key=lambda x: x[1])[0] # ä¿®æ­£ä¸ºè·å–é”®
                activity_summary['peak_hour'] = peak_hour
                activity_summary['peak_period'] = self._get_time_period(peak_hour)
        
        return activity_summary

    async def export_social_graph(self) -> Dict[str, Any]:
        """å¯¼å‡ºç¤¾äº¤å…³ç³»å›¾è°±"""
        graph_data = {
            'nodes': [],
            'edges': [],
            'statistics': {}
        }
        
        # å¯¼å‡ºèŠ‚ç‚¹ï¼ˆç”¨æˆ·ï¼‰
        # ä»æ•°æ®åº“åŠ è½½æ‰€æœ‰ç”¨æˆ·ç”»åƒï¼Œè€Œä¸æ˜¯åªï¿½ï¿½ï¿½å†…å­˜ä¸­è·å–
        # ä¸ºäº†ç®€åŒ–ï¼Œè¿™é‡Œä»ç„¶ä½¿ç”¨å†…å­˜ä¸­çš„ user_profilesï¼Œä½†å®é™…åº”è¯¥ä»æ•°æ®åº“åŠ è½½
        for user_key, profile in self.user_profiles.items():
            # ä»user_keyä¸­æå–ç”¨æˆ·IDç”¨äºæ˜¾ç¤º
            display_id = user_key.split(':')[-1] if ':' in user_key else user_key
            
            # å…¼å®¹å¤„ç†UserProfileå¯¹è±¡å’Œdict
            if hasattr(profile, 'qq_name'):
                name = profile.qq_name
                nicknames = profile.nicknames
                activity_level = len(profile.activity_pattern.get('activity_hours', {}))
            else:
                name = profile.get('name', f'ç”¨æˆ·{display_id}')
                nicknames = profile.get('nicknames', [])
                activity_level = len(profile.get('activity_pattern', {}).get('activity_hours', {}))
            
            graph_data['nodes'].append({
                'id': display_id,
                'name': name,
                'nicknames': nicknames,
                'user_key': user_key,
                'activity_level': activity_level
            })
        
        # å¯¼å‡ºè¾¹ï¼ˆå…³ç³»ï¼‰
        # ä»æ•°æ®åº“åŠ è½½æ‰€æœ‰ç¤¾äº¤å…³ç³»ï¼Œè€Œä¸æ˜¯åªä»å†…å­˜ä¸­è·å–
        # ä¸ºäº†ç®€åŒ–ï¼Œè¿™é‡Œä»ç„¶ä½¿ç”¨å†…å­˜ä¸­çš„ social_graphï¼Œä½†å®é™…åº”è¯¥ä»æ•°æ®åº“åŠ è½½
        for from_user, relations in self.social_graph.items():
            for relation in relations:
                graph_data['edges'].append({
                    'from': from_user,
                    'to': relation.to_user,
                    'type': relation.relation_type,
                    'strength': relation.strength,
                    'frequency': relation.frequency
                })
        
        # ç»Ÿè®¡ä¿¡æ¯
        graph_data['statistics'] = {
            'total_users': len(self.user_profiles),
            'total_relations': sum(len(relations) for relations in self.social_graph.values()),
            'nickname_mappings': len(self.nickname_mapping)
        }
        
        return graph_data
    
    async def stop(self):
        """åœæ­¢å¤šç»´åº¦åˆ†æå™¨æœåŠ¡"""
        try:
            # å–æ¶ˆå®šæœŸæ¸…ç†ä»»åŠ¡
            if hasattr(self, '_cleanup_task') and self._cleanup_task:
                self._cleanup_task.cancel()
                try:
                    await self._cleanup_task
                except asyncio.CancelledError:
                    pass
                self._cleanup_task = None
            
            # ä¿å­˜é‡è¦çš„ç”¨æˆ·ç”»åƒæ•°æ®åˆ°æ•°æ®åº“ï¼ˆå¦‚æœéœ€è¦æŒä¹…åŒ–ï¼‰
            try:
                await self._save_user_profiles_to_db()
            except Exception as e:
                logger.warning(f"ä¿å­˜ç”¨æˆ·ç”»åƒåˆ°æ•°æ®åº“å¤±è´¥: {e}")
            
            # æ¸…ç†å†…å­˜æ•°æ®
            if hasattr(self, 'user_profiles'):
                self.user_profiles.clear()
            if hasattr(self, 'social_graph'):
                self.social_graph.clear()
            if hasattr(self, '_analysis_cache'):
                self._analysis_cache.clear()
            if hasattr(self, 'nickname_mapping'):
                self.nickname_mapping.clear()
            
            logger.info("å¤šç»´åº¦åˆ†æå™¨å·²åœæ­¢")
            return True
            
        except Exception as e:
            logger.error(f"åœæ­¢å¤šç»´åº¦åˆ†æå™¨å¤±è´¥: {e}")
            return False
    
    async def _save_user_profiles_to_db(self):
        """ä¿å­˜ç”¨æˆ·ç”»åƒæ•°æ®åˆ°æ•°æ®åº“"""
        try:
            if not self.user_profiles:
                return
                
            # è¿™é‡Œå¯ä»¥å®ç°å°†ç”¨æˆ·ç”»åƒæ•°æ®ä¿å­˜åˆ°ä¸“é—¨çš„ç”¨æˆ·ç”»åƒè¡¨
            # å½“å‰ç®€åŒ–å®ç°ï¼Œä»…è®°å½•ç»Ÿè®¡ä¿¡æ¯
            logger.info(f"éœ€è¦ä¿å­˜ {len(self.user_profiles)} ä¸ªç”¨æˆ·ç”»åƒåˆ°æ•°æ®åº“")
            
            # TODO: å®ç°å…·ä½“çš„æ•°æ®åº“ä¿å­˜é€»è¾‘
            # ä¾‹å¦‚ï¼šCREATE TABLE user_profiles (group_id, user_id, profile_data, updated_at)
            
        except Exception as e:
            logger.error(f"ä¿å­˜ç”¨æˆ·ç”»åƒåˆ°æ•°æ®åº“å¤±è´¥: {e}")
