"""
å­¦ä¹ è´¨é‡ç›‘æ§æœåŠ¡ - ç›‘æ§å­¦ä¹ æ•ˆæœï¼Œé˜²æ­¢äººæ ¼å´©å
"""
import json
import time
import re # ç§»åŠ¨åˆ°æ–‡ä»¶é¡¶éƒ¨
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
from dataclasses import dataclass

from astrbot.api import logger
from astrbot.api.star import Context

from ..core.framework_llm_adapter import FrameworkLLMAdapter  # å¯¼å…¥æ¡†æ¶é€‚é…å™¨

from ..config import PluginConfig

from ..exceptions import StyleAnalysisError

from ..utils.json_utils import safe_parse_llm_json


@dataclass
class PersonaMetrics:
    """äººæ ¼æŒ‡æ ‡"""
    consistency_score: float = 0.0      # ä¸€è‡´æ€§å¾—åˆ†
    style_stability: float = 0.0        # é£æ ¼ç¨³å®šæ€§
    vocabulary_diversity: float = 0.0   # è¯æ±‡å¤šæ ·æ€§
    emotional_balance: float = 0.0      # æƒ…æ„Ÿå¹³è¡¡æ€§
    coherence_score: float = 0.0        # é€»è¾‘è¿è´¯æ€§


@dataclass
class LearningAlert:
    """å­¦ä¹ è­¦æŠ¥"""
    alert_type: str
    severity: str  # low, medium, high, critical
    message: str
    timestamp: str
    metrics: Dict[str, float]
    suggestions: List[str]


class LearningQualityMonitor:
    """å­¦ä¹ è´¨é‡ç›‘æ§å™¨"""
    
    def __init__(self, config: PluginConfig, context: Context, 
                 llm_adapter: Optional[FrameworkLLMAdapter] = None,
                 prompts: Any = None):
        self.config = config
        self.context = context
        self.prompts = prompts
        
        # ä½¿ç”¨æ¡†æ¶é€‚é…å™¨
        self.llm_adapter = llm_adapter
        
        # ç›‘æ§é˜ˆå€¼ - è°ƒæ•´ä¸ºæ›´åˆç†çš„å€¼
        self.consistency_threshold = 0.5    # ä¸€è‡´æ€§é˜ˆå€¼ (ä»0.7é™ä½åˆ°0.5)
        self.stability_threshold = 0.4      # ç¨³å®šæ€§é˜ˆå€¼ (ä»0.6é™ä½åˆ°0.4)
        self.drift_threshold = 0.4          # é£æ ¼åç§»é˜ˆå€¼ (ä»0.3æé«˜åˆ°0.4)
        
        # å†å²æŒ‡æ ‡å­˜å‚¨
        self.historical_metrics: List[PersonaMetrics] = []
        self.alerts_history: List[LearningAlert] = []
        
        logger.info("å­¦ä¹ è´¨é‡ç›‘æ§æœåŠ¡åˆå§‹åŒ–å®Œæˆ")

    async def evaluate_learning_batch(self, 
                                    original_persona: Dict[str, Any],
                                    updated_persona: Dict[str, Any],
                                    learning_messages: List[Dict[str, Any]]) -> PersonaMetrics:
        """è¯„ä¼°å­¦ä¹ æ‰¹æ¬¡è´¨é‡"""
        try:
            # è®¡ç®—å„é¡¹æŒ‡æ ‡
            consistency_score = await self._calculate_consistency(
                original_persona, updated_persona
            )
            
            style_stability = await self._calculate_style_stability(
                learning_messages
            )
            
            vocabulary_diversity = await self._calculate_vocabulary_diversity(
                learning_messages
            )
            
            emotional_balance = await self._calculate_emotional_balance(
                learning_messages
            )
            
            coherence_score = await self._calculate_coherence(
                updated_persona
            )
            
            metrics = PersonaMetrics(
                consistency_score=consistency_score,
                style_stability=style_stability,
                vocabulary_diversity=vocabulary_diversity,
                emotional_balance=emotional_balance,
                coherence_score=coherence_score
            )
            
            # å­˜å‚¨å†å²æŒ‡æ ‡
            self.historical_metrics.append(metrics)
            
            # è°ƒæ•´é˜ˆå€¼
            await self.adjust_thresholds_based_on_history()
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å‘å‡ºè­¦æŠ¥
            await self._check_quality_alerts(metrics)
            
            return metrics
            
        except Exception as e:
            logger.error(f"å­¦ä¹ è´¨é‡è¯„ä¼°å¤±è´¥: {e}")
            raise StyleAnalysisError(f"å­¦ä¹ è´¨é‡è¯„ä¼°å¤±è´¥: {str(e)}")

    async def _calculate_consistency(self, 
                                   original_persona: Dict[str, Any],
                                   updated_persona: Dict[str, Any]) -> float:
        """è®¡ç®—äººæ ¼ä¸€è‡´æ€§å¾—åˆ† - ä½¿ç”¨å¼ºLLMè¿›è¡Œä¸“ä¸šè¯„ä¼°"""
        try:
            original_prompt = original_persona.get('prompt', '')
            updated_prompt = updated_persona.get('prompt', '')
            
            # å¢å¼ºçš„ç©ºå€¼æ£€æŸ¥å’Œé»˜è®¤å€¼å¤„ç†
            if not original_prompt and not updated_prompt:
                logger.debug("åŸå§‹å’Œæ›´æ–°äººæ ¼éƒ½ä¸ºç©ºï¼Œè¿”å›ä¸­ç­‰ä¸€è‡´æ€§")
                return 0.7  # æé«˜é»˜è®¤å€¼ï¼Œå› ä¸ºä¸¤è€…éƒ½ç©ºå¯ä»¥è®¤ä¸ºæ˜¯ä¸€è‡´çš„
            elif not original_prompt or not updated_prompt:
                logger.debug("å…¶ä¸­ä¸€ä¸ªäººæ ¼ä¸ºç©ºï¼Œè¿”å›è¾ƒä½ä¸€è‡´æ€§")
                return 0.6  # æé«˜é»˜è®¤å€¼ï¼Œé¿å…å› æ•°æ®é—®é¢˜å¯¼è‡´çš„ä½åˆ†
            
            # å¦‚æœä¸¤ä¸ªpromptå®Œå…¨ç›¸åŒï¼Œç›´æ¥è¿”å›é«˜ä¸€è‡´æ€§
            if original_prompt.strip() == updated_prompt.strip():
                logger.debug("äººæ ¼å®Œå…¨ç›¸åŒï¼Œè¿”å›é«˜ä¸€è‡´æ€§")
                return 0.95
            
            # ä¼˜å…ˆä½¿ç”¨æ¡†æ¶é€‚é…å™¨
            if self.llm_adapter and self.llm_adapter.has_filter_provider():
                try:
                    prompt = self.prompts.LEARNING_QUALITY_MONITOR_CONSISTENCY_PROMPT.format(
                        original_persona_prompt=original_prompt,
                        updated_persona_prompt=updated_prompt
                    )
                    
                    response = await self.llm_adapter.filter_chat_completion(prompt=prompt)
                    
                    if response:
                        try:
                            # è§£æLLMè¿”å›çš„ä¸€è‡´æ€§å¾—åˆ†
                            consistency_text = response.strip()
                            logger.debug(f"LLMä¸€è‡´æ€§è¯„ä¼°åŸå§‹å“åº”: {consistency_text}")
                            
                            # å¢å¼ºçš„æ•°å€¼æå–é€»è¾‘
                            import re
                            # å…ˆå°è¯•æå–æ˜ç¡®çš„åˆ†æ•°æ ¼å¼
                            score_patterns = [
                                r'ä¸€è‡´æ€§[ï¼š:]\s*([0-9]*\.?[0-9]+)',
                                r'å¾—åˆ†[ï¼š:]\s*([0-9]*\.?[0-9]+)',
                                r'åˆ†æ•°[ï¼š:]\s*([0-9]*\.?[0-9]+)',
                                r'([0-9]*\.?[0-9]+)',  # ä»»ä½•æ•°å­—
                            ]
                            
                            for pattern in score_patterns:
                                numbers = re.findall(pattern, consistency_text)
                                if numbers:
                                    try:
                                        score = float(numbers[0])
                                        # å¦‚æœåˆ†æ•°å¤§äº1ï¼Œå¯èƒ½æ˜¯ç™¾åˆ†åˆ¶ï¼Œè½¬æ¢ä¸ºå°æ•°
                                        if score > 1.0:
                                            score = score / 100.0
                                        # ç¡®ä¿åˆ†æ•°åœ¨åˆç†èŒƒå›´å†…
                                        consistency_score = max(0.1, min(score, 1.0))  # æœ€ä½0.1ï¼Œé¿å…0.0
                                        logger.debug(f"è§£æå¾—åˆ°ä¸€è‡´æ€§å¾—åˆ†: {consistency_score}")
                                        return consistency_score
                                    except ValueError:
                                        continue
                            
                            # å¦‚æœæ— æ³•è§£ææ•°å€¼ï¼Œæ ¹æ®å…³é”®è¯åˆ¤æ–­
                            consistency_text_lower = consistency_text.lower()
                            if any(word in consistency_text_lower for word in ['å¾ˆå¥½', 'é«˜', 'ä¼˜ç§€', 'good', 'high']):
                                logger.debug("åŸºäºå…³é”®è¯åˆ¤æ–­ä¸ºé«˜ä¸€è‡´æ€§")
                                return 0.8
                            elif any(word in consistency_text_lower for word in ['ä¸­ç­‰', 'ä¸€èˆ¬', 'medium', 'average']):
                                logger.debug("åŸºäºå…³é”®è¯åˆ¤æ–­ä¸ºä¸­ç­‰ä¸€è‡´æ€§")
                                return 0.6
                            elif any(word in consistency_text_lower for word in ['ä½', 'å·®', 'low', 'poor']):
                                logger.debug("åŸºäºå…³é”®è¯åˆ¤æ–­ä¸ºä½ä¸€è‡´æ€§")
                                return 0.4
                            else:
                                logger.debug("æ— æ³•è§£æä¸€è‡´æ€§è¯„ä¼°ï¼Œè¿”å›ä¸­ç­‰é»˜è®¤å€¼")
                                return 0.6  # æé«˜é»˜è®¤å€¼
                        except (ValueError, IndexError) as e:
                            logger.warning(f"è§£æä¸€è‡´æ€§å¾—åˆ†å¤±è´¥: {e}, å“åº”: {consistency_text}")
                            return 0.6  # æé«˜é»˜è®¤å€¼
                    else:
                        logger.warning("LLMä¸€è‡´æ€§è¯„ä¼°æ— å“åº”")
                        return 0.6  # æé«˜é»˜è®¤å€¼
                except Exception as e:
                    logger.error(f"æ¡†æ¶é€‚é…å™¨è®¡ç®—äººæ ¼ä¸€è‡´æ€§å¤±è´¥: {e}")
                    return 0.6  # æé«˜é»˜è®¤å€¼
            else:
                logger.warning("æ²¡æœ‰å¯ç”¨çš„Filter Providerï¼Œä½¿ç”¨ç®€å•æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—")
                # ç®€å•çš„æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—ä½œä¸ºåå¤‡æ–¹æ¡ˆ
                return self._calculate_text_similarity(original_prompt, updated_prompt)
            
        except Exception as e:
            logger.error(f"è®¡ç®—äººæ ¼ä¸€è‡´æ€§å¤±è´¥: {e}")
            return 0.6  # æé«˜é»˜è®¤å€¼ï¼Œé¿å…é˜»å¡å­¦ä¹ 

    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦ä½œä¸ºåå¤‡æ–¹æ¡ˆ"""
        try:
            if not text1 or not text2:
                return 0.6
            
            # ç®€å•çš„å­—ç¬¦çº§ç›¸ä¼¼åº¦è®¡ç®—
            text1_clean = text1.strip().lower()
            text2_clean = text2.strip().lower()
            
            if text1_clean == text2_clean:
                return 0.95
            
            # è®¡ç®—æœ€é•¿å…¬å…±å­åºåˆ—
            def lcs_length(s1, s2):
                m, n = len(s1), len(s2)
                dp = [[0] * (n + 1) for _ in range(m + 1)]
                
                for i in range(1, m + 1):
                    for j in range(1, n + 1):
                        if s1[i-1] == s2[j-1]:
                            dp[i][j] = dp[i-1][j-1] + 1
                        else:
                            dp[i][j] = max(dp[i-1][j], dp[i][j-1])
                
                return dp[m][n]
            
            lcs_len = lcs_length(text1_clean, text2_clean)
            max_len = max(len(text1_clean), len(text2_clean))
            
            if max_len == 0:
                return 0.6
            
            similarity = lcs_len / max_len
            
            # ç¡®ä¿ç›¸ä¼¼åº¦åœ¨åˆç†èŒƒå›´å†…ï¼Œé¿å…è¿‡ä½å¯¼è‡´å­¦ä¹ åœæ»
            return max(0.4, min(similarity, 1.0))
            
        except Exception as e:
            logger.warning(f"è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            return 0.6

    async def _calculate_style_stability(self, messages: List[Dict[str, Any]]) -> float:
        """è®¡ç®—é£æ ¼ç¨³å®šæ€§"""
        if len(messages) < 2:
            return 1.0
            
        try:
            # åˆ†ææ¶ˆæ¯çš„é£æ ¼ç‰¹å¾
            style_features = []
            for msg in messages:
                features = {
                    'length': len(msg['message']),
                    'punctuation_ratio': self._get_punctuation_ratio(msg['message']),
                    'emoji_count': self._count_emoji(msg['message']),
                    'question_count': msg['message'].count('?') + msg['message'].count('ï¼Ÿ'),
                    'exclamation_count': msg['message'].count('!') + msg['message'].count('ï¼')
                }
                style_features.append(features)
            
            # è®¡ç®—ç‰¹å¾æ–¹å·®ï¼ˆç¨³å®šæ€§ä¸æ–¹å·®æˆåæ¯”ï¼‰
            variance_scores = []
            for feature in ['length', 'punctuation_ratio', 'emoji_count']:
                values = [f[feature] for f in style_features]
                if len(values) > 1:
                    mean_val = sum(values) / len(values)
                    variance = sum((x - mean_val) ** 2 for x in values) / len(values)
                    # å½’ä¸€åŒ–æ–¹å·®å¾—åˆ†ï¼ˆè¶Šå°è¶Šç¨³å®šï¼‰
                    normalized_variance = min(variance / (mean_val + 1), 1.0)
                    variance_scores.append(1.0 - normalized_variance)
            
            return sum(variance_scores) / len(variance_scores) if variance_scores else 0.5
            
        except Exception as e:
            logger.warning(f"é£æ ¼ç¨³å®šæ€§è®¡ç®—å¤±è´¥: {e}")
            return 0.5

    async def _calculate_vocabulary_diversity(self, messages: List[Dict[str, Any]]) -> float:
        """è®¡ç®—è¯æ±‡å¤šæ ·æ€§"""
        try:
            all_text = ' '.join([msg['message'] for msg in messages])
            words = all_text.split()
            
            if len(words) == 0:
                return 0.0
            
            unique_words = set(words)
            diversity_ratio = len(unique_words) / len(words)
            
            # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
            return min(diversity_ratio * 2, 1.0)
            
        except Exception as e:
            logger.warning(f"è¯æ±‡å¤šæ ·æ€§è®¡ç®—å¤±è´¥: {e}")
            return 0.5

    async def _calculate_emotional_balance(self, messages: List[Dict[str, Any]]) -> float:
        """ä½¿ç”¨LLMè®¡ç®—æƒ…æ„Ÿå¹³è¡¡æ€§"""
        if not self.llm_adapter or not self.llm_adapter.has_refine_provider():
            return self._simple_emotional_balance(messages)

        messages_text = "\n".join([msg['message'] for msg in messages])
        
        prompt = self.prompts.LEARNING_QUALITY_MONITOR_EMOTIONAL_BALANCE_PROMPT.format(
            batch_messages=messages_text
        )
        try:
            # ä½¿ç”¨æ¡†æ¶é€‚é…å™¨
            if self.llm_adapter and self.llm_adapter.has_refine_provider():
                response = await self.llm_adapter.refine_chat_completion(prompt=prompt)
            else:
                logger.warning("æ²¡æœ‰å¯ç”¨çš„LLMæœåŠ¡")
                return self._simple_emotional_balance(messages)
            
            # å¤„ç†å“åº”
            response_text = response if isinstance(response, str) else (response.text() if response and hasattr(response, 'text') else None)
            if response_text:
                # ä½¿ç”¨å®‰å…¨çš„JSONè§£æ
                default_scores = {
                    "ç§¯æ": 0.5,
                    "æ¶ˆæ": 0.5,
                    "ä¸­æ€§": 0.5
                }
                
                emotional_scores = safe_parse_llm_json(response_text, fallback_result=default_scores)
                
                if emotional_scores and isinstance(emotional_scores, dict):
                    # ç¡®ä¿æ‰€æœ‰åˆ†æ•°éƒ½åœ¨0-1ä¹‹é—´
                    for key, value in emotional_scores.items():
                        emotional_scores[key] = max(0.0, min(float(value), 1.0))
                    
                    # è®¡ç®—æƒ…æ„Ÿå¹³è¡¡æ€§ï¼šç§¯ææƒ…æ„Ÿå‡å»æ¶ˆææƒ…æ„Ÿï¼Œå†è°ƒæ•´åˆ°0-1èŒƒå›´
                    positive_score = emotional_scores.get("ç§¯æ", 0.5)
                    negative_score = emotional_scores.get("æ¶ˆæ", 0.5)
                    balance_score = (positive_score - negative_score + 1.0) / 2.0  # è½¬æ¢åˆ°0-1èŒƒå›´
                    return max(0.0, min(balance_score, 1.0))
                else:
                    return self._simple_emotional_balance(messages)
            return self._simple_emotional_balance(messages)
        except Exception as e:
            logger.warning(f"LLMæƒ…æ„Ÿå¹³è¡¡æ€§è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–ç®—æ³•: {e}")
            return self._simple_emotional_balance(messages)

    def _simple_emotional_balance(self, messages: List[Dict[str, Any]]) -> float:
        """ç®€åŒ–çš„æƒ…æ„Ÿå¹³è¡¡æ€§è®¡ç®—ï¼ˆå¤‡ç”¨ï¼‰"""
        positive_words = ['å¥½', 'æ£’', 'èµ', 'å–œæ¬¢', 'å¼€å¿ƒ', 'é«˜å…´', 'å“ˆå“ˆ']
        negative_words = ['ä¸', 'æ²¡', 'å', 'çƒ¦', 'è®¨åŒ', 'ç”Ÿæ°”', 'éš¾è¿‡']
        
        pos_count = 0
        neg_count = 0
        
        for msg in messages:
            text = msg['message']
            for word in positive_words:
                pos_count += text.count(word)
            for word in negative_words:
                neg_count += text.count(word)
        
        total_emotional = pos_count + neg_count
        if total_emotional == 0:
            return 0.8  # ä¸­æ€§æƒ…æ„Ÿ
        
        # è®¡ç®—å¹³è¡¡æ€§ï¼ˆè¶Šæ¥è¿‘0.5è¶Šå¹³è¡¡ï¼‰
        pos_ratio = pos_count / total_emotional
        balance_score = 1.0 - abs(pos_ratio - 0.5) * 2
        
        return balance_score

    async def _calculate_coherence(self, persona: Dict[str, Any]) -> float:
        """è®¡ç®—é€»è¾‘è¿è´¯æ€§"""
        try:
            prompt_text = persona.get('prompt', '')
            if not prompt_text:
                return 0.0
            
            # ç®€å•çš„è¿è´¯æ€§æ£€æŸ¥
            sentences = prompt_text.split('ã€‚')
            if len(sentences) < 2:
                return 0.8
            
            # æ£€æŸ¥å¥å­é•¿åº¦ä¸€è‡´æ€§å’Œç»“æ„å®Œæ•´æ€§
            sentence_lengths = [len(s.strip()) for s in sentences if s.strip()]
            if not sentence_lengths:
                return 0.0
            
            avg_length = sum(sentence_lengths) / len(sentence_lengths)
            length_variance = sum((l - avg_length) ** 2 for l in sentence_lengths) / len(sentence_lengths)
            
            # å½’ä¸€åŒ–è¿è´¯æ€§å¾—åˆ†
            coherence = max(0.0, 1.0 - length_variance / (avg_length + 1))
            
            return min(coherence, 1.0)
            
        except Exception as e:
            logger.warning(f"è¿è´¯æ€§è®¡ç®—å¤±è´¥: {e}")
            return 0.5

    async def _check_quality_alerts(self, metrics: PersonaMetrics):
        """æ£€æŸ¥è´¨é‡è­¦æŠ¥"""
        alerts = []
        
        # ä¸€è‡´æ€§æ£€æŸ¥
        if metrics.consistency_score < self.consistency_threshold:
            alerts.append(LearningAlert(
                alert_type="consistency",
                severity="high" if metrics.consistency_score < 0.5 else "medium",
                message=f"äººæ ¼ä¸€è‡´æ€§å¾—åˆ†è¿‡ä½: {metrics.consistency_score:.3f}",
                timestamp=datetime.now().isoformat(),
                metrics={'consistency_score': metrics.consistency_score},
                suggestions=["å»ºè®®äººå·¥å®¡æ ¸äººæ ¼å˜åŒ–", "è€ƒè™‘å›æ»šåˆ°ä¹‹å‰çš„äººæ ¼ç‰ˆæœ¬"]
            ))
        
        # ç¨³å®šæ€§æ£€æŸ¥
        if metrics.style_stability < self.stability_threshold:
            alerts.append(LearningAlert(
                alert_type="stability",
                severity="medium",
                message=f"é£æ ¼ç¨³å®šæ€§ä¸è¶³: {metrics.style_stability:.3f}",
                timestamp=datetime.now().isoformat(),
                metrics={'style_stability': metrics.style_stability},
                suggestions=["å¢åŠ è®­ç»ƒæ•°æ®ä¸€è‡´æ€§", "è°ƒæ•´å­¦ä¹ ç‡"]
            ))
        
        # é£æ ¼åç§»æ£€æŸ¥
        if len(self.historical_metrics) >= 2:
            current_drift = self._calculate_style_drift(
                self.historical_metrics[-2], metrics
            )
            if current_drift > self.drift_threshold:
                alerts.append(LearningAlert(
                    alert_type="drift",
                    severity="critical" if current_drift > 0.5 else "high",
                    message=f"æ£€æµ‹åˆ°æ˜¾è‘—é£æ ¼åç§»: {current_drift:.3f}",
                    timestamp=datetime.now().isoformat(),
                    metrics={'style_drift': current_drift},
                    suggestions=["ç«‹å³æš‚åœè‡ªåŠ¨å­¦ä¹ ", "äººå·¥å®¡æ ¸å­¦ä¹ æ•°æ®", "è€ƒè™‘é‡ç½®äººæ ¼"]
                ))
        
        # å­˜å‚¨è­¦æŠ¥
        self.alerts_history.extend(alerts)
        
        # è®°å½•è­¦æŠ¥
        for alert in alerts:
            logger.warning(f"å­¦ä¹ è´¨é‡è­¦æŠ¥ [{alert.severity}]: {alert.message}")

    def _calculate_style_drift(self, prev_metrics: PersonaMetrics, curr_metrics: PersonaMetrics) -> float:
        """è®¡ç®—é£æ ¼åç§»ç¨‹åº¦"""
        # è®¡ç®—å…³é”®æŒ‡æ ‡çš„å˜åŒ–å¹…åº¦
        consistency_drift = abs(curr_metrics.consistency_score - prev_metrics.consistency_score)
        stability_drift = abs(curr_metrics.style_stability - prev_metrics.style_stability)
        diversity_drift = abs(curr_metrics.vocabulary_diversity - prev_metrics.vocabulary_diversity)
        
        # åŠ æƒå¹³å‡åç§»
        weighted_drift = (
            consistency_drift * 0.4 +
            stability_drift * 0.3 +
            diversity_drift * 0.3
        )
        
        return weighted_drift

    def _get_punctuation_ratio(self, text: str) -> float:
        """è·å–æ ‡ç‚¹ç¬¦å·æ¯”ä¾‹"""
        punctuation = 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€""''()ï¼ˆï¼‰ã€ã€‘'
        punct_count = sum(1 for char in text if char in punctuation)
        return punct_count / len(text) if text else 0.0

    def _count_emoji(self, text: str) -> int:
        """ç»Ÿè®¡è¡¨æƒ…ç¬¦å·æ•°é‡"""
        # ç®€å•çš„è¡¨æƒ…ç¬¦å·æ£€æµ‹
        emoji_patterns = ['ğŸ˜€', 'ğŸ˜‚', 'ğŸ˜Š', 'ğŸ¤”', 'ğŸ‘', 'â¤ï¸', 'ğŸ‰']
        count = 0
        for emoji in emoji_patterns:
            count += text.count(emoji)
        return count

    async def get_quality_report(self) -> Dict[str, Any]:
        """è·å–è´¨é‡æŠ¥å‘Š"""
        if not self.historical_metrics:
            return {"error": "æš‚æ— å†å²æ•°æ®"}
        
        latest_metrics = self.historical_metrics[-1]
        
        # è®¡ç®—è¶‹åŠ¿
        trends = {}
        if len(self.historical_metrics) >= 2:
            prev_metrics = self.historical_metrics[-2]
            trends = {
                'consistency_trend': latest_metrics.consistency_score - prev_metrics.consistency_score,
                'stability_trend': latest_metrics.style_stability - prev_metrics.style_stability,
                'diversity_trend': latest_metrics.vocabulary_diversity - prev_metrics.vocabulary_diversity
            }
        
        # è·å–æœ€è¿‘çš„è­¦æŠ¥
        recent_alerts = [
            alert for alert in self.alerts_history
            if datetime.fromisoformat(alert.timestamp) > datetime.now() - timedelta(hours=24)
        ]
        
        return {
            'current_metrics': {
                'consistency_score': latest_metrics.consistency_score,
                'style_stability': latest_metrics.style_stability,
                'vocabulary_diversity': latest_metrics.vocabulary_diversity,
                'emotional_balance': latest_metrics.emotional_balance,
                'coherence_score': latest_metrics.coherence_score
            },
            'trends': trends,
            'recent_alerts': len(recent_alerts),
            'alert_summary': {
                'critical': len([a for a in recent_alerts if a.severity == 'critical']),
                'high': len([a for a in recent_alerts if a.severity == 'high']),
                'medium': len([a for a in recent_alerts if a.severity == 'medium'])
            },
            'recommendations': self._generate_recommendations(latest_metrics, recent_alerts)
        }

    def _generate_recommendations(self, metrics: PersonaMetrics, alerts: List[LearningAlert]) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if metrics.consistency_score < 0.7:
            recommendations.append("å»ºè®®å¢åŠ äººæ ¼ä¸€è‡´æ€§æ£€æŸ¥")
        
        if metrics.style_stability < 0.6:
            recommendations.append("å»ºè®®è°ƒæ•´å­¦ä¹ æ•°æ®ç­›é€‰æ ‡å‡†")
        
        if len(alerts) > 5:
            recommendations.append("è­¦æŠ¥é¢‘ç¹ï¼Œå»ºè®®äººå·¥ä»‹å…¥å®¡æ ¸")
        
        if not recommendations:
            recommendations.append("å­¦ä¹ è´¨é‡è‰¯å¥½ï¼Œå¯ç»§ç»­è‡ªåŠ¨å­¦ä¹ ")
        
        return recommendations

    async def should_pause_learning(self) -> tuple[bool, str]:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æš‚åœå­¦ä¹ """
        if not self.historical_metrics:
            return False, ""
        
        latest_metrics = self.historical_metrics[-1]
        
        # æ£€æŸ¥å…³é”®æŒ‡æ ‡
        if latest_metrics.consistency_score < 0.4:
            return True, "äººæ ¼ä¸€è‡´æ€§ä¸¥é‡ä¸‹é™"
        
        # æ£€æŸ¥æœ€è¿‘çš„ä¸¥é‡è­¦æŠ¥
        recent_critical_alerts = [
            alert for alert in self.alerts_history
            if (alert.severity in ['critical', 'high'] and 
                datetime.fromisoformat(alert.timestamp) > datetime.now() - timedelta(hours=1))
        ]
        
        if len(recent_critical_alerts) >= 2:
            return True, "æ£€æµ‹åˆ°å¤šä¸ªä¸¥é‡è´¨é‡é—®é¢˜"
        
        return False, ""

    async def adjust_thresholds_based_on_history(self):
        """åŸºäºå†å²æŒ‡æ ‡åŠ¨æ€è°ƒæ•´é˜ˆå€¼"""
        try:
            if len(self.historical_metrics) < 5:
                # å†å²æ•°æ®ä¸è¶³ï¼Œä½¿ç”¨é»˜è®¤é˜ˆå€¼
                return
            
            # è·å–æœ€è¿‘çš„æŒ‡æ ‡
            recent_metrics = self.historical_metrics[-5:]
            
            # è®¡ç®—æœ€è¿‘çš„å¹³å‡è¡¨ç°
            avg_consistency = sum(m.consistency_score for m in recent_metrics) / len(recent_metrics)
            avg_stability = sum(m.style_stability for m in recent_metrics) / len(recent_metrics)
            
            # åŠ¨æ€è°ƒæ•´é˜ˆå€¼ - å¦‚æœç³»ç»Ÿè¡¨ç°æŒç»­è‰¯å¥½ï¼Œå¯ä»¥é€‚å½“æé«˜é˜ˆå€¼
            if avg_consistency > 0.8:
                self.consistency_threshold = min(0.8, self.consistency_threshold + 0.05)
            elif avg_consistency < 0.5:
                self.consistency_threshold = max(0.6, self.consistency_threshold - 0.05)
            
            if avg_stability > 0.7:
                self.stability_threshold = min(0.7, self.stability_threshold + 0.05)
            elif avg_stability < 0.4:
                self.stability_threshold = max(0.5, self.stability_threshold - 0.05)
            
            # æ ¹æ®å˜åŒ–å¹…åº¦è°ƒæ•´åç§»é˜ˆå€¼
            if len(recent_metrics) >= 2:
                recent_changes = []
                for i in range(1, len(recent_metrics)):
                    change = abs(recent_metrics[i].consistency_score - recent_metrics[i-1].consistency_score)
                    recent_changes.append(change)
                
                avg_change = sum(recent_changes) / len(recent_changes)
                if avg_change < 0.1:
                    # å˜åŒ–å¾ˆå°ï¼Œå¯ä»¥æé«˜åç§»é˜ˆå€¼
                    self.drift_threshold = min(0.4, self.drift_threshold + 0.05)
                elif avg_change > 0.3:
                    # å˜åŒ–å¾ˆå¤§ï¼Œé™ä½åç§»é˜ˆå€¼
                    self.drift_threshold = max(0.2, self.drift_threshold - 0.05)
            
            logger.debug(f"åŠ¨æ€è°ƒæ•´é˜ˆå€¼ - ä¸€è‡´æ€§: {self.consistency_threshold:.2f}, "
                        f"ç¨³å®šæ€§: {self.stability_threshold:.2f}, "
                        f"åç§»: {self.drift_threshold:.2f}")
                        
        except Exception as e:
            logger.warning(f"åŠ¨æ€è°ƒæ•´é˜ˆå€¼å¤±è´¥: {e}")

    async def stop(self):
        """åœæ­¢æœåŠ¡"""
        try:
            logger.info("å­¦ä¹ è´¨é‡ç›‘æ§æœåŠ¡å·²åœæ­¢")
            return True
        except Exception as e:
            logger.error(f"åœæ­¢å­¦ä¹ è´¨é‡ç›‘æ§æœåŠ¡å¤±è´¥: {e}")
            return False
